2025-10-08 19:54:16,907 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-08 19:54:16,907 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-08 19:54:16,908 [INFO] werkzeug:  * Restarting with stat
2025-10-08 19:54:17,839 [WARNING] werkzeug:  * Debugger is active!
2025-10-08 19:54:17,847 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-08 19:55:14,134 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-08 19:55:14,135 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.002s
2025-10-08 19:55:14,137 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 19:55:14] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 19:55:14,139 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-08 19:55:14,140 [DEBUG] src.api.app: üìù Request data: {'job_description': 'About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'company_name': 'Ascendion', 'position_title': 'Python Developer'}
2025-10-08 19:55:14,365 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-7fa574e1-1fa0-4df8-9e13-70ee7705ab41', 'post_parser': <function Embeddings.create.<locals>.parser at 0x122d449a0>, 'json_data': {'input': 'Company: Ascendion. Position: Python Developer. Description: About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 19:55:14,386 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 19:55:14,386 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 19:55:14,483 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125800d90>
2025-10-08 19:55:14,484 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x11224f260> server_hostname='api.openai.com' timeout=5.0
2025-10-08 19:55:14,527 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125800d10>
2025-10-08 19:55:14,527 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:14,528 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:14,528 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:14,528 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:14,528 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:14,965 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'156'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-797b6d7f75-2wfcj'), (b'x-envoy-upstream-service-time', b'324'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999384'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'36ms'), (b'x-request-id', b'req_8874743cc20a4691bf954d3a9e8185fb'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nfw46w2BquFocVYX3O8u3TI.H_oEwYifrI7i5mTXfks-1759971315-1.0.1.1-EtgHyT_Bj5HAHhCUqgE6rtEXY2PjZs2yvpNfiHPIlpF0riI2lTK1LRrjuUAJUesGATn0ZzjbsfcOwfdiXIx7wqmta6m5JF5YqbfReAFa570; path=/; expires=Thu, 09-Oct-25 01:25:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=VeGn4TqErzuGaMMLH9jwTGW6dubpWWLpKVv2_RbHmys-1759971315043-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9e94c8aec89fb-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:14,969 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 19:55:14,971 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:14,972 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:14,972 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:14,972 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:14,972 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 00:55:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '156'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-797b6d7f75-2wfcj'), ('x-envoy-upstream-service-time', '324'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999384'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '36ms'), ('x-request-id', 'req_8874743cc20a4691bf954d3a9e8185fb'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=nfw46w2BquFocVYX3O8u3TI.H_oEwYifrI7i5mTXfks-1759971315-1.0.1.1-EtgHyT_Bj5HAHhCUqgE6rtEXY2PjZs2yvpNfiHPIlpF0riI2lTK1LRrjuUAJUesGATn0ZzjbsfcOwfdiXIx7wqmta6m5JF5YqbfReAFa570; path=/; expires=Thu, 09-Oct-25 01:25:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=VeGn4TqErzuGaMMLH9jwTGW6dubpWWLpKVv2_RbHmys-1759971315043-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98b9e94c8aec89fb-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 19:55:14,972 [DEBUG] openai._base_client: request_id: req_8874743cc20a4691bf954d3a9e8185fb
2025-10-08 19:55:15,002 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.862s
2025-10-08 19:55:15,002 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 19:55:15] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 19:55:20,577 [INFO] src.api.app: üåê OPTIONS /api/generate/cover-letter/ - 127.0.0.1
2025-10-08 19:55:20,578 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/cover-letter/ - 200 - 0.001s
2025-10-08 19:55:20,578 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 19:55:20] "OPTIONS /api/generate/cover-letter/ HTTP/1.1" 200 -
2025-10-08 19:55:20,581 [INFO] src.api.app: üåê POST /api/generate/cover-letter/ - 127.0.0.1
2025-10-08 19:55:20,581 [DEBUG] src.api.app: üìù Request data: {'jobDescription': 'About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'companyName': 'Ascendion', 'positionTitle': 'Python Developer', 'resumePdfFile': 'http://localhost:8080/api/resumes/generated/Ascendion/Python%20Developer.pdf'}
2025-10-08 19:55:20,625 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-47d4d515-2aca-45b9-a976-a74486da797a', 'post_parser': <function Embeddings.create.<locals>.parser at 0x111096de0>, 'json_data': {'input': 'Company: Ascendion. Position: Python Developer. Description: About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 19:55:20,625 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 19:55:20,626 [DEBUG] httpcore.connection: close.started
2025-10-08 19:55:20,626 [DEBUG] httpcore.connection: close.complete
2025-10-08 19:55:20,626 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 19:55:20,661 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125828310>
2025-10-08 19:55:20,662 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x11224f260> server_hostname='api.openai.com' timeout=5.0
2025-10-08 19:55:20,695 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x122d517d0>
2025-10-08 19:55:20,695 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:20,696 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:20,696 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:20,696 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:20,696 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:20,983 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'78'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-6c4c9cd988-x9659'), (b'x-envoy-upstream-service-time', b'174'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999384'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'36ms'), (b'x-request-id', b'req_16d584d2f04f4aa385767a485a6d590c'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9e97329fe4848-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:20,985 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 19:55:20,985 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:20,986 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:20,986 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:20,986 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:20,987 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 00:55:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '78', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-6c4c9cd988-x9659', 'x-envoy-upstream-service-time': '174', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999384', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '36ms', 'x-request-id': 'req_16d584d2f04f4aa385767a485a6d590c', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98b9e97329fe4848-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 19:55:20,987 [DEBUG] openai._base_client: request_id: req_16d584d2f04f4aa385767a485a6d590c
2025-10-08 19:55:21,117 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 19:55:21,143 [DEBUG] chromadb.config: Starting component System
2025-10-08 19:55:21,143 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 19:55:21,143 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 19:55:21,143 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 19:55:21,143 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 19:55:21,144 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12581be90>
2025-10-08 19:55:21,144 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,144 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,144 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,145 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,145 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,156 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 00:55:21 GMT')])
2025-10-08 19:55:21,157 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 19:55:21,157 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,157 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,157 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,157 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,157 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 19:55:21,163 [DEBUG] chromadb.config: Starting component System
2025-10-08 19:55:21,164 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 19:55:21,164 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 19:55:21,164 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 19:55:21,164 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 19:55:21,164 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125e307d0>
2025-10-08 19:55:21,165 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,165 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,165 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,165 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,165 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,173 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 00:55:21 GMT')])
2025-10-08 19:55:21,173 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 19:55:21,173 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,173 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,173 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,173 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,174 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,174 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,174 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,174 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,174 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:21,175 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 00:55:21 GMT')])
2025-10-08 19:55:21,175 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 19:55:21,175 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:21,175 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,175 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,175 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,177 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,177 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,177 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,177 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,177 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,185 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 00:55:21 GMT')])
2025-10-08 19:55:21,186 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 19:55:21,186 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,186 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,186 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,186 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,285 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-10baa437-47ac-42d6-9dce-e384a60b5c03', 'post_parser': <function Embeddings.create.<locals>.parser at 0x125e02de0>, 'json_data': {'input': [[10714, 279, 2683, 198, 10714, 40660, 408, 290, 198, 41203, 408, 290, 374, 264, 2539, 24358, 7528, 15009, 10105, 2883, 13, 1226, 1304, 323, 10299, 3241, 15771, 323, 3956, 430, 2410, 6650, 323, 6493, 86282, 11704, 311, 13723, 323, 8420, 13, 5751, 15009, 11, 9624, 11, 828, 11, 3217, 2955, 11, 323, 11005, 6425, 17357, 43880, 18475, 323, 5536, 369, 20790, 8403, 13, 11452, 68720, 304, 1561, 16228, 11, 1057, 32027, 315, 220, 21, 11, 931, 10, 40660, 14846, 28421, 10105, 505, 2212, 279, 24867, 13, 40660, 408, 290, 374, 5918, 1422, 2883, 25, 40660, 408, 290, 2361, 25, 13325, 25922]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 19:55:21,286 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 19:55:21,286 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 19:55:21,317 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125e33d50>
2025-10-08 19:55:21,318 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x124266a80> server_hostname='api.openai.com' timeout=None
2025-10-08 19:55:21,358 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125e33dd0>
2025-10-08 19:55:21,358 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,358 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,358 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,358 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,358 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,682 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): us.i.posthog.com:443
2025-10-08 19:55:21,752 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'90'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5c84678cfc-5s675'), (b'x-envoy-upstream-service-time', b'289'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999899'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_835e7f0716124d5bbcdc0545d0a09457'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=hR6mthbAGO5jMXE864VeMi89O52H8XQ2J5ZzYaf8rF8-1759971321-1.0.1.1-b_eGYpB_n.kKlrqzhVzaTGeSdB2u.ryPf75ujllLggaisS.otpJ1n9vz12Bfgzr8mcX6FE9Edc0Q_9pg7JuDD..1EwYOG1XqWCXQKRi.D5I; path=/; expires=Thu, 09-Oct-25 01:25:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=FqRzgv_hytH0PGhAEHj3TXcBJRVlVUYqpyGYYKvamXc-1759971321834-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9e9774da286ff-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:21,752 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 19:55:21,752 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,753 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,753 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,753 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,753 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 00:55:21 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '90'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5c84678cfc-5s675'), ('x-envoy-upstream-service-time', '289'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999899'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '6ms'), ('x-request-id', 'req_835e7f0716124d5bbcdc0545d0a09457'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=hR6mthbAGO5jMXE864VeMi89O52H8XQ2J5ZzYaf8rF8-1759971321-1.0.1.1-b_eGYpB_n.kKlrqzhVzaTGeSdB2u.ryPf75ujllLggaisS.otpJ1n9vz12Bfgzr8mcX6FE9Edc0Q_9pg7JuDD..1EwYOG1XqWCXQKRi.D5I; path=/; expires=Thu, 09-Oct-25 01:25:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=FqRzgv_hytH0PGhAEHj3TXcBJRVlVUYqpyGYYKvamXc-1759971321834-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98b9e9774da286ff-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 19:55:21,753 [DEBUG] openai._base_client: request_id: req_835e7f0716124d5bbcdc0545d0a09457
2025-10-08 19:55:21,755 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,755 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,755 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,755 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,755 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,768 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'6581'), (b'date', b'Thu, 09 Oct 2025 00:55:21 GMT')])
2025-10-08 19:55:21,768 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-08 19:55:21,768 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,768 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:21,768 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:21,768 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:21,772 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-89b840c9-a7cf-484e-b934-6465bc51589f', 'json_data': {'messages': [{'content': '\n    You are an expert cover letter writer specializing in creating compelling, personalized cover letters that effectively connect a candidate\'s background to specific job opportunities.\n\n    Your task is to analyze the job posting, personalized resume, and retrieved context to generate ONLY the cover letter content paragraphs in plain LaTeX format.\n\n    POSITION: Python Developer\n    COMPANY: Ascendion\n    JOB POSTING:\n    About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!\n\n    PERSONALIZED RESUME CONTENT:\n    RESUME: Amirreza Sokhankhosh\n‚ôÇphone431-293-6515 /envel‚å¢peamirreza.skhn@gmail.com /linkedinLinkedIn /githubGitHub ·ΩãCPortfolio\nHighlight of Qualifications\n‚Ä¢ Python Development: Extensive experience in Python development using Django, Flask, and FastAPI with\ndemonstrated ability in developing modular and scalable APIs that improve backend functionality and enhance user\nengagement by 25%.\n‚Ä¢ Distributed Systems & Blockchain: Engineered advanced systems using Flask and Hyperledger Fabric, show-\ncasing a strong ability to innovate in blockchain and distributed architecture, demonstrated by significant improve-\nments in fault tolerance (62.7%) and efficiency.\n‚Ä¢ AI & Machine Learning: Strong background in implementing advanced ML models with TensorFlow and Py-\nTorch, improving system performance and achieving accuracy enhancements in predictive systems by 15%.\n‚Ä¢ Cloud & DevOps: Proficient in Docker and Kubernetes for containerized deployment, reducing setup time by\n30% and enabling scalable solutions for diverse cloud environments.\n‚Ä¢ Full-stack Development: Adept at designing full-stack solutions with React and Django, ensuring seamless API\nintegration and improved data management using PostgreSQL.\nExperience\nUniversity of Manitoba Sep 2023 ‚Äì Jul 2025\nGraduate Research Assistant Winnipeg, Manitoba, Canada\n‚Ä¢ Developed modular APIs using Flask and Express.js, supporting novel distributed AI architectures, including\nsystems using blockchain consensus mechanisms.\n‚Ä¢ Reduced communication overhead by 85.2%, improved fault tolerance by 62.7%, and enhanced energy efficiency\nthrough innovative distributed machine learning architectures.\n‚Ä¢ Implemented advanced models using TensorFlow and PyTorch, optimizing system performance and resource allo-\ncation in a cloud environment.\n‚Ä¢ Published ground-breaking research in top-tier IEEE venues, showcasing expertise in AI, blockchain, and dis-\ntributed systems.\n‚Ä¢ Led initiatives that bridged digitalengineering gaps, pioneering efficiency improvements that aligned with enterprise-\nlevel needs.\nBobo May 2024 ‚Äì Aug 2024\nFull-stack Developer Intern Winnipeg, Manitoba, Canada\n‚Ä¢ Accelerated product development by designing RESTful APIs utilizing Supabase and PostgreSQL, vastly im-\nproving backend functionality and data management.\n‚Ä¢ Automated data integration processes withPython, converting CSV data to SQL scripts, which significantly stream-\nlined database population efforts.\n‚Ä¢ Collaborated closely with front-end teams to ensure seamless full-stack integration, aligning API specifications\nwith user interface requirements efficiently.\n‚Ä¢ Contributed to Agile workflows using Atlassian tools, fostering enhanced task management, documentation, and\nteam coordination.\nUniversity of Manitoba Sep 2024 ‚Äì Jun 2025\nTeaching Assistant Winnipeg, Manitoba, Canada\n‚Ä¢ Provided academic support to over 400 students in foundational courses including Introduction to Programming\n(Python) and Data Structures Algorithms .\n‚Ä¢ Guided students in solving complex coding and theoretical challenges, emphasizingproblem-solving and algorithm\ndesign.\n‚Ä¢ Resolved programming issues in departmental Help Centre, advising on best coding practices and encouraging a\nstrong grasp of Python and other core technologies.Projects\nMini Task Manager | Django, Django REST Framework, Python, React, SQLite| Code\n‚Ä¢ Developed a secure token-based authentication system with Django REST Framework to ensure user data\nprotection and streamlined access management.\n‚Ä¢ Implemented full CRUD operations for task management using React and Django, enabling seamless client-\nserver interactions and enhancing user experience.\n‚Ä¢ Engineered a responsive Material-UI interface, improving usability across devices, which led to a 25% increase\nin user engagement.\nExplanation: 1. **Technology Selection**: Emphasized ‚ÄòDjango‚Äò, ‚ÄòDjango REST Framework‚Äò, and ‚ÄòPython‚Äò as\nthey are directly related to the job requirement. Included ‚ÄòReact‚Äò to show full-stack capabilities and ‚ÄòSQLite‚Äò as a\nrelevant technology for data persistence. 2. **Achievements and Technical Details**: Focused on authentication,\nCRUD operations, and responsive design‚Äîall essential for a Python Developer role. 3. **Impact and Metrics**:\nHighlighted user engagement improvement to provide a tangible outcome of the project work.\nFederated Learning enabled Digital Twin | Python, Flask, Hyperledger Fabric, Docker, PyTorch| Code\n‚Ä¢ Developed a privacy-preserving Digital Twin system using Flask and Hyperledger Fabric, ensuring secure and\ndecentralized management of IoT sensor data across 76 smart building rooms.\n‚Ä¢ Implemented Temporal Fusion Transformer (TFT) models in PyTorch for accurate multi-variate time-series\nforecasting of temperature, CO2, and humidity levels with a median accuracy improvement of 15%.\n‚Ä¢ Integrated Docker for containerized deployment, reducing setup time by 30% and enabling seamless scaling across\nvarious real-world IoT environments.\nProof of Collaborative Learning (PoCL) | Python, Flask, TensorFlow, Docker, Hyperledger Fabric| Code\n‚Ä¢ Developed an innovativeblockchain consensus mechanismusing Python and TensorFlowto replace traditional\nmining with federated learning, increasing energy efficiency by reducing computational load.\n‚Ä¢ Integrated Flask as an API gateway to facilitate seamless communication and coordination between miners and\nthe blockchain network, ensuring robust and scalable architecture.\n‚Ä¢ Leveraged Hyperledger Fabric and Docker to deploy a decentralised, immutable ledger that streamlined trans-\naction processing and enhanced system reliability and availability.\nPaper Summarizer | Python, Flask, Detectron2, EasyOCR, PyTorch| Code\n‚Ä¢ Implemented an end-to-end academic paper summarization system utilizing Python and Flask, enabling efficient\nand automated content comprehension.\n‚Ä¢ Utilized Detectron2 with Faster R-CNNfor precise object detection, achieving accurate identification of document\nelements, including text and figures.\n‚Ä¢ Enhanced text and figure extraction accuracy with EasyOCR and PyTorch, resulting in comprehensive and co-\nherent summaries with structured PDF output.\nEducation\nUniversity of Manitoba Sep 2023 ‚Äì Aug 2025\nMaster of Science in Computer Science (GPA: 4.4 / 4.5) Winnipeg, Canada\n‚Ä¢ Relevant Coursework: Security & Privacy, Deep Generative Modeling, Blockchain & Distributed Systems: A+\nK.N. Toosi University of Technology Sep 2018 ‚Äì Feb 2023\nBachelor of Science in Computer Engineering\nTechnical Skills\nLanguages: Python, JavaScript, TypeScript, Java, C++\nWeb Frameworks: Back-end: Django, Flask, FastAPI. Front-end: React, Express.JS\nDatabases: PostgreSQL, MongoDB, MySQL, Redis\nCloud & DevOps: Docker, Git, GitHub, Linux, CI/CD, Kubernetes\nTools & Methodologies: Jira, Confluence, Agile, Scrum\nAI / Machine Learning : TensorFlow, Keras, PyTorch, Numpy, Pandas, Scikit-learn\n\n    RETRIEVED CONTEXT FROM KNOWLEDGE BASE:\n    [Document(id=\'3325f8b5-cac8-47aa-ab0d-018c50ca37e5\', metadata={\'start_index\': 0, \'source\': \'cover letter\', \'position\': \'Python Developer\', \'company\': \'Ascendion\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'15649aed-4073-49f5-807e-04fc96505ce0\', metadata={\'start_index\': 0, \'position\': \'Python Developer\', \'company\': \'Ascendion\', \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'c9163008-0ebe-447e-9477-80ed0da48ef6\', metadata={\'position\': \'Python Developer\', \'company\': \'Ascendion\', \'start_index\': 0, \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'9f633635-33d1-40f1-a506-22d8d27d73ba\', metadata={\'company\': \'Ascendion\', \'source\': \'cover letter\', \'position\': \'Python Developer\', \'start_index\': 0}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'39fc2647-7830-4dac-9b5d-b1aba7e35b2a\', metadata={\'position\': \'Python Developer\', \'company\': \'Ascendion\', \'source\': \'cover letter\', \'start_index\': 0}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'b4447976-35c8-4e28-9835-d87a11efa906\', metadata={\'start_index\': 0, \'company\': \'Ascendion\', \'source\': \'cover letter\', \'position\': \'Python Developer\'}, page_content=\'I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and extensive experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences.\\n\\nIn my recent projects, I have built scalable and secure applications using \\\\textbf{Python} and its frameworks, aligning with your requirements. For example, in my \\\\textbf{Mini Task Manager} project, I developed a token-based authentication system using \\\\textbf{Django REST Framework}, which resulted in a 25\\\\% increase in user engagement. Furthermore, my proficiency with \\\\textbf{Docker} and \\\\textbf{Kubernetes} has enabled me to reduce setup times by 30\\\\%, preparing me for the dynamic challenges outlined in your job posting.\'), Document(id=\'498860ac-ce95-4adc-8db4-d854f58ef15e\', metadata={\'position\': \'Python Developer\', \'start_index\': 885, \'source\': \'cover letter\', \'company\': \'Ascendion\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s commitment to creating opportunities and fostering inclusion. My projects, such as the \\\\textbf{Federated Learning enabled Digital Twin}, demonstrate my ability to innovate within \\\\textbf{blockchain} and IoT, supporting \\\\textbf{Ascendion}\'s mission to engineer solutions for Fortune 500 clients. The culture of high performance and endless innovation at Ascendion resonates deeply with my professional values and aspirations.\\n\\nI am enthusiastic about the opportunity to contribute to \\\\textbf{Ascendion}\'s mission by bringing my skills to build advanced digital solutions. My background in \\\\textbf{distributed systems} and passion for \\\\textbf{digital engineering} uniquely position me to make significant contributions to your team. I am excited to bring my perspective and technical expertise to Ascendion, collaborating to create technology that elevates life."), Document(id=\'d2f30633-a110-4546-9653-bcce5f4685d0\', metadata={\'start_index\': 1025, \'company\': \'Ascendion\', \'source\': \'cover letter\', \'position\': \'Python Developer\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s dedication to creating opportunities and fostering inclusion. My projects, such as the \\\\textbf{Federated Learning enabled Digital Twin}, demonstrate my ability to innovate within \\\\textbf{blockchain} and IoT, supporting \\\\textbf{Ascendion}\'s mission to engineer solutions for Fortune 500 clients. The culture of high performance and endless innovation at Ascendion resonates deeply with my professional values and aspirations.\\n\\nI am enthusiastic about the opportunity to contribute to \\\\textbf{Ascendion}\'s mission by bringing my skills to build advanced digital solutions. My background in \\\\textbf{distributed systems} and passion for \\\\textbf{digital engineering} uniquely position me to make significant contributions to your team. I am excited to bring my perspective and technical expertise to Ascendion, collaborating to create technology that elevates life.")]\n                                                       \n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Company mission, values, and culture\n       - Key technical requirements and preferred technologies\n       - Required experience level and responsibilities\n       - Specific skills and qualifications sought\n\n    2. Use the personalized resume content to:\n       - Highlight the most relevant experiences and achievements\n       - Showcase technical skills that match job requirements\n       - Reference specific projects that demonstrate required capabilities\n       - Quantify accomplishments where possible\n\n    3. Leverage the retrieved context to:\n       - Add depth and authenticity to your statements\n       - Reference relevant research, projects, or experiences not in the resume\n       - Demonstrate domain knowledge and passion for the field\n       - Show progression and learning from past experiences\n\n    4. Follow the template structure but adapt content to:\n       - Match the company\'s tone and industry language\n       - Create a narrative that connects past experiences to future contributions\n       - Show genuine enthusiasm for the specific role and company\n       - Address any potential gaps or explain career transitions\n\n\n    CRITICAL OUTPUT REQUIREMENTS:\n    - Generate ONLY the cover letter content paragraphs\n    - DO NOT include any LaTeX document structure (\\documentclass, \\begin{document}, \\begin{letter}, \\end{letter}, \\end{document})\n    - DO NOT include any explanations, introductions, or meta-commentary\n    - DO NOT include salutations like "Dear Hiring Manager" or closings like "Sincerely"\n    - Start directly with the first paragraph content\n    - End with the last paragraph content\n    - Use clean paragraph breaks (double newlines) between paragraphs\n\n    CONTENT STRUCTURE:\n    Write exactly 3-4 paragraphs of cover letter content:\n\n    Paragraph 1: Express enthusiasm for the specific role and company, briefly state your relevant background and why you\'re applying.\n\n    Paragraph 2: Highlight your most relevant technical experiences and projects with specific examples that match the job requirements.\n\n    Paragraph 3: Demonstrate alignment with the company\'s mission and technology stack, referencing retrieved context to show deeper knowledge and genuine interest.\n\n    Paragraph 4 (optional): Reiterate enthusiasm and mention specific contributions you can make to the team and company.\n\n                                                       \n    COVER LETTER STRUCTURE:\n    1. Opening paragraph: Express enthusiasm for the specific role and company, briefly state your relevant background\n    2. Body paragraph 1: Highlight relevant technical experience and projects with specific examples\n    3. Body paragraph 2: Demonstrate alignment with company mission/technology stack, reference retrieved context for depth\n    4. Closing paragraph: Reiterate enthusiasm, mention specific contributions you can make, professional closing\n\n    TONE AND STYLE:\n    - Professional yet enthusiastic\n    - Confident but not boastful\n    - Specific and detailed, not generic\n    - Forward-looking and solution-oriented\n    - Authentic and genuine\n    LATEX FORMATTING REQUIREMENTS:\n    - Use proper LaTeX escaping: \\& instead of & for ampersands\n    - Use \\% for percentages in text  \n    - Use \\textbf{} for emphasizing key technologies, company names, and achievements\n    - Write in clean, readable paragraph format\n    - Use standard paragraph separation (double newlines)\n\n    CONTENT GUIDELINES:\n    - Reference specific technologies, frameworks, and methodologies from the job posting\n    - Use metrics and quantified achievements where available\n    - Show understanding of the company\'s challenges and how you can help\n    - Demonstrate continuous learning and adaptability\n    - Connect past experiences to future potential contributions\n    - Use retrieved context to add unique insights or relevant background\n    - Be professional yet enthusiastic, confident but not boastful\n    - Make it specific and detailed, not generic\n\n    OUTPUT FORMAT EXAMPLE:\n    I am excited to apply for the {position} position at \\textbf{{company name}}. With my background in \\textbf{relevant technology} and experience in \\textbf{relevant domain}, I am eager to contribute to your team\'s mission of advancing \\textbf{company focus area}.\n\n    In my recent projects, I have built scalable applications using \\textbf{specific technologies} that align with your requirements. For example, in my \\textbf{project name} project, I developed \\textbf{specific implementation} that resulted in \\textbf{quantified outcome}. My experience with \\textbf{relevant technology stack} has prepared me to tackle the challenges outlined in your job posting.\n\n    I am particularly drawn to \\textbf{{company name}}\'s commitment to \\textbf{company values/mission}. My research in \\textbf{relevant area from context} has given me deep insights into \\textbf{relevant domain knowledge}, and I am excited about the opportunity to apply this knowledge in a production environment where I can help \\textbf{specific company goals}.\n\n    Generate the cover letter content paragraphs following this exact format:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 19:55:21,773 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 19:55:21,773 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 19:55:21,811 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x125e1d0d0>
2025-10-08 19:55:21,811 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x1242669f0> server_hostname='api.openai.com' timeout=None
2025-10-08 19:55:21,847 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126c865d0>
2025-10-08 19:55:21,847 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,847 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:21,847 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:21,847 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:21,847 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:21,969 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 19:55:43,894 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'19202'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'21914'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'24538'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'10.924s'), (b'x-request-id', b'req_320b0423c8f34e8e92b4a0c7cdf2a368'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nF3XZNw09ncQUed1IToTadn28AhjrmDW5Laii9YmwEo-1759971343-1.0.1.1-2jft8bpc8xiSYk5r39Juv7CFjah9zB.n13tD2cyCaddWjDMbeU.TT19lvKyvRG8_eNrsyNFq33_FnydbtcW5EVLxTYPrLOTOoD2BxzCgX2M; path=/; expires=Thu, 09-Oct-25 01:25:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=w3IBkDlco9LGLjOI7o6A_4clQ6A35h64.U0tNvVZHoE-1759971343977-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9e97a4c6aa2a1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:43,897 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 19:55:43,897 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:43,905 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:43,905 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:43,905 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:43,905 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Thu, 09 Oct 2025 00:55:43 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '19202'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '21914'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '30000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '24538'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '10.924s'), ('x-request-id', 'req_320b0423c8f34e8e92b4a0c7cdf2a368'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=nF3XZNw09ncQUed1IToTadn28AhjrmDW5Laii9YmwEo-1759971343-1.0.1.1-2jft8bpc8xiSYk5r39Juv7CFjah9zB.n13tD2cyCaddWjDMbeU.TT19lvKyvRG8_eNrsyNFq33_FnydbtcW5EVLxTYPrLOTOoD2BxzCgX2M; path=/; expires=Thu, 09-Oct-25 01:25:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=w3IBkDlco9LGLjOI7o6A_4clQ6A35h64.U0tNvVZHoE-1759971343977-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98b9e97a4c6aa2a1-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 19:55:43,905 [DEBUG] openai._base_client: request_id: req_320b0423c8f34e8e92b4a0c7cdf2a368
2025-10-08 19:55:45,372 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 19:55:45,378 [DEBUG] chromadb.config: Starting component System
2025-10-08 19:55:45,378 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 19:55:45,378 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 19:55:45,378 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 19:55:45,379 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 19:55:45,383 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126521b50>
2025-10-08 19:55:45,384 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,384 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,384 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,384 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,384 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,385 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,385 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 19:55:45,385 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,385 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,385 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,385 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,385 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 19:55:45,392 [DEBUG] chromadb.config: Starting component System
2025-10-08 19:55:45,392 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 19:55:45,392 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 19:55:45,392 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 19:55:45,392 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 19:55:45,395 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12652c090>
2025-10-08 19:55:45,395 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,395 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,395 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,395 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,395 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,396 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,396 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,397 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,397 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,399 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,399 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,399 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,399 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,399 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,400 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,400 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 19:55:45,400 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,400 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,401 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,401 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,403 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-a463d2d3-8f21-4c21-8f40-adbff688d5ee', 'post_parser': <function Embeddings.create.<locals>.parser at 0x110e8dbc0>, 'json_data': {'input': [[40, 1097, 12304, 311, 3881, 369, 279, 13325, 25922, 2361, 520, 1144, 1342, 13536, 90, 41203, 408, 290, 7966, 3161, 856, 4092, 304, 1144, 1342, 13536, 90, 31380, 4500, 92, 323, 16781, 3217, 449, 49125, 1778, 439, 1144, 1342, 13536, 90, 35, 5970, 2186, 1144, 1342, 13536, 90, 3968, 1091, 2186, 323, 1144, 1342, 13536, 90, 33274, 7227, 2186, 358, 1097, 24450, 311, 17210, 311, 701, 2128, 596, 9131, 315, 46890, 7528, 15009, 323, 24944, 86282, 11704, 13, 40660, 408, 290, 596, 7829, 315, 19297, 323, 1202, 39955, 311, 15009, 430, 12231, 988, 2324, 16917, 89986, 449, 856, 6721, 2819, 323, 58522, 13], [644, 856, 3293, 7224, 11, 358, 617, 5918, 69311, 323, 9966, 8522, 1701, 1144, 1342, 13536, 90, 31380, 92, 323, 1202, 49125, 11, 5398, 287, 449, 701, 8670, 13, 1789, 3187, 11, 304, 856, 1144, 1342, 13536, 90, 55313, 5546, 10790, 92, 2447, 11, 358, 8040, 264, 4037, 6108, 17066, 1887, 1701, 1144, 1342, 13536, 90, 35, 5970, 26487, 24686, 2186, 13239, 304, 264, 220, 914, 59, 4, 5376, 304, 1217, 20392, 13, 24296, 11, 856, 63239, 449, 1144, 1342, 13536, 90, 35, 13973, 92, 323, 1144, 1342, 13536, 90, 42, 30927, 92, 706, 9147, 757, 311, 8108, 6642, 3115, 555, 220, 966, 59, 13689, 20646, 757, 369, 279, 8915, 11774, 33740, 304, 701, 2683, 17437, 382, 40, 1097, 8104, 15107, 311, 1144, 1342, 13536, 90, 41203, 408, 290, 11923, 82, 15507, 311, 6968, 10708, 323, 86644, 28286, 13, 3092, 7224, 11, 1778, 439, 279, 1144, 1342, 13536, 90, 37, 7442, 660, 21579, 9147, 14434, 36047, 2186, 20461, 856, 5845, 311, 92064, 2949, 1144, 1342, 13536, 90, 4677, 8995, 92, 323, 50180, 11, 12899, 40660, 408, 290, 596, 9131, 311, 24490, 10105, 369, 46331, 220, 2636, 8403, 13, 578, 7829, 315, 1579, 5178, 323, 26762, 19297, 520, 40660, 408, 290, 29280, 988, 17693, 449, 856, 6721, 2819, 323, 58522, 13], [40, 1097, 42702, 922, 279, 6776, 311, 17210, 311, 1144, 1342, 13536, 90, 41203, 408, 290, 11923, 82, 9131, 555, 12967, 856, 7512, 311, 1977, 11084, 7528, 10105, 13, 3092, 4092, 304, 1144, 1342, 13536, 90, 63475, 6067, 92, 323, 11939, 369, 1144, 1342, 13536, 90, 58369, 15009, 92, 42329, 2361, 757, 311, 1304, 5199, 19564, 311, 701, 2128, 13, 358, 1097, 12304, 311, 4546, 856, 13356, 323, 11156, 19248, 311, 40660, 408, 290, 11, 73301, 311, 1893, 5557, 430, 12231, 988, 2324, 13]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 19:55:45,403 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 19:55:45,403 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 19:55:45,436 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126530ed0>
2025-10-08 19:55:45,436 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x125a82ba0> server_hostname='api.openai.com' timeout=None
2025-10-08 19:55:45,478 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x126530f90>
2025-10-08 19:55:45,478 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,478 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,478 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,478 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,478 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,706 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 19:55:45,731 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'131'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-canary-794cf74f59-hcs9d'), (b'x-envoy-upstream-service-time', b'174'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999603'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'23ms'), (b'x-request-id', b'req_47f7a0b015cb48799b3c30424f875c76'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=XvQmt_jq6oR6WVY.DfC90paKMfnOKyS0dhd65F_csFk-1759971345-1.0.1.1-EXbgIrFoF41QCeYwJOA.5EymemlXwOfMJttSbjGNSY2AyclcmsPhtACpvxnEzUaJreBM9mxE6PkEr6bv34MLP2IQPJ.prDAG_o5vGM6Tf1c; path=/; expires=Thu, 09-Oct-25 01:25:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=hJGP6IZCwlcoOrZ17iJubE.rH476l0PdnN4rS0PrPxs-1759971345813-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9ea0dffb2618b-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:45,731 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 19:55:45,731 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,737 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,737 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,737 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,737 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 00:55:45 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '131'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-canary-794cf74f59-hcs9d'), ('x-envoy-upstream-service-time', '174'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999603'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '23ms'), ('x-request-id', 'req_47f7a0b015cb48799b3c30424f875c76'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=XvQmt_jq6oR6WVY.DfC90paKMfnOKyS0dhd65F_csFk-1759971345-1.0.1.1-EXbgIrFoF41QCeYwJOA.5EymemlXwOfMJttSbjGNSY2AyclcmsPhtACpvxnEzUaJreBM9mxE6PkEr6bv34MLP2IQPJ.prDAG_o5vGM6Tf1c; path=/; expires=Thu, 09-Oct-25 01:25:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=hJGP6IZCwlcoOrZ17iJubE.rH476l0PdnN4rS0PrPxs-1759971345813-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98b9ea0dffb2618b-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 19:55:45,737 [DEBUG] openai._base_client: request_id: req_47f7a0b015cb48799b3c30424f875c76
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 19:55:45,739 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'55'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,739 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-08 19:55:45,740 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 19:55:45,741 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,741 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,741 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,741 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,742 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,742 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,742 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,742 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,790 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'2'), (b'date', b'Thu, 09 Oct 2025 00:55:45 GMT')])
2025-10-08 19:55:45,790 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-08 19:55:45,790 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,790 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:45,790 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:45,790 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:45,824 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-6da6f913-9c85-4bc3-9607-84b53043f2e4', 'post_parser': <function Embeddings.create.<locals>.parser at 0x122d44c20>, 'json_data': {'input': 'Company: Ascendion. Position: Python Developer. Description: About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 19:55:45,824 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 19:55:45,825 [DEBUG] httpcore.connection: close.started
2025-10-08 19:55:45,825 [DEBUG] httpcore.connection: close.complete
2025-10-08 19:55:45,825 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 19:55:45,858 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12652d450>
2025-10-08 19:55:45,858 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x11224f260> server_hostname='api.openai.com' timeout=5.0
2025-10-08 19:55:45,895 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12652cfd0>
2025-10-08 19:55:45,896 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 19:55:45,896 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 19:55:45,896 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 19:55:45,896 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 19:55:45,896 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 19:55:46,186 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 00:55:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'62'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-76f88f4767-sj5mg'), (b'x-envoy-upstream-service-time', b'201'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999384'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'36ms'), (b'x-request-id', b'req_d56a30f41d164cdb89ab1cf89a38fc2c'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98b9ea109e784848-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 19:55:46,186 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 19:55:46,187 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 19:55:46,187 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 19:55:46,187 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 19:55:46,187 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 19:55:46,187 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 00:55:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '62', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-76f88f4767-sj5mg', 'x-envoy-upstream-service-time': '201', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999384', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '36ms', 'x-request-id': 'req_d56a30f41d164cdb89ab1cf89a38fc2c', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98b9ea109e784848-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 19:55:46,187 [DEBUG] openai._base_client: request_id: req_d56a30f41d164cdb89ab1cf89a38fc2c
2025-10-08 19:55:46,209 [INFO] src.api.app: ‚úÖ POST /api/generate/cover-letter/ - 200 - 25.629s
2025-10-08 19:55:46,209 [WARNING] src.api.app: üêå Slow request: POST /api/generate/cover-letter/ took 25.629s
2025-10-08 19:55:46,210 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 19:55:46] "POST /api/generate/cover-letter/ HTTP/1.1" 200 -
2025-10-08 19:58:38,873 [INFO] werkzeug:  * Detected change in '/Users/amirrezasokhankhosh/Documents/Workstation/Aria/agent/src/api/routes/context.py', reloading
2025-10-08 19:58:39,031 [DEBUG] httpcore.connection: close.started
2025-10-08 19:58:39,032 [DEBUG] httpcore.connection: close.complete
2025-10-08 19:58:39,032 [DEBUG] httpcore.connection: close.started
2025-10-08 19:58:39,032 [DEBUG] httpcore.connection: close.complete
2025-10-08 19:58:39,108 [DEBUG] httpcore.connection: close.started
2025-10-08 19:58:39,109 [DEBUG] httpcore.connection: close.complete
2025-10-08 19:58:39,376 [INFO] werkzeug:  * Restarting with stat
2025-10-08 19:58:41,057 [WARNING] werkzeug:  * Debugger is active!
2025-10-08 19:58:41,067 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-08 19:58:43,097 [INFO] werkzeug:  * Detected change in '/Users/amirrezasokhankhosh/Documents/Workstation/Aria/agent/server.py', reloading
2025-10-08 19:58:43,320 [INFO] werkzeug:  * Restarting with stat
2025-10-08 19:58:44,373 [WARNING] werkzeug:  * Debugger is active!
2025-10-08 19:58:44,383 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-08 19:59:02,579 [INFO] werkzeug:  * Detected change in '/Users/amirrezasokhankhosh/Documents/Workstation/Aria/agent/server.py', reloading
2025-10-08 19:59:02,749 [INFO] werkzeug:  * Restarting with stat
2025-10-08 20:21:40,897 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-08 20:21:40,897 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-08 20:21:40,898 [INFO] werkzeug:  * Restarting with stat
2025-10-08 20:21:41,841 [WARNING] werkzeug:  * Debugger is active!
2025-10-08 20:21:41,851 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-08 20:22:29,786 [INFO] src.api.app: üåê OPTIONS /api/generate/cover-letter/ - 127.0.0.1
2025-10-08 20:22:29,787 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/cover-letter/ - 200 - 0.001s
2025-10-08 20:22:29,788 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:22:29] "OPTIONS /api/generate/cover-letter/ HTTP/1.1" 200 -
2025-10-08 20:22:29,792 [INFO] src.api.app: üåê POST /api/generate/cover-letter/ - 127.0.0.1
2025-10-08 20:22:29,793 [DEBUG] src.api.app: üìù Request data: {'jobDescription': 'About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'companyName': 'Ascendion', 'positionTitle': 'Python Developer', 'resumePdfFile': 'http://localhost:8080/api/resumes/preview/ml-engineering'}
2025-10-08 20:22:30,080 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-a11f62cb-1778-4c5b-a4be-f4e4753a7dff', 'post_parser': <function Embeddings.create.<locals>.parser at 0x117bfc860>, 'json_data': {'input': 'Company: Ascendion. Position: Python Developer. Description: About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 20:22:30,101 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 20:22:30,102 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 20:22:30,208 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x121e98250>
2025-10-08 20:22:30,209 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x107dff2f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 20:22:30,247 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11736ec50>
2025-10-08 20:22:30,247 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:30,247 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:30,247 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:30,247 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:30,247 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:30,638 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 01:22:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'104'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-76f88f4767-sj5mg'), (b'x-envoy-upstream-service-time', b'245'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999384'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'36ms'), (b'x-request-id', b'req_56aaa63d7f6a4b67a14fd13db7563926'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=a9xQ1DTt3hoBSD1JnrhrojdJsUy025X5udt.97ybVJU-1759972950-1.0.1.1-736ivUunLC4ibUp7.mN1_3H5Tb8SNuBqV6z6guHWAYSFJEKbu6EiQkCZaRPOokwdutzUURAD4XZEmSPkqxgEmCkNjUVEulEprxIj9Avmk.k; path=/; expires=Thu, 09-Oct-25 01:52:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=OT3QznwgkWx8OSa3SMOVCGyWRpG2cENu7L69rIE0IhY-1759972950636-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98ba113bbc68eadb-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 20:22:30,642 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 20:22:30,643 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:30,646 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:30,646 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:30,646 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:30,647 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 01:22:30 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '104'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-76f88f4767-sj5mg'), ('x-envoy-upstream-service-time', '245'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999384'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '36ms'), ('x-request-id', 'req_56aaa63d7f6a4b67a14fd13db7563926'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=a9xQ1DTt3hoBSD1JnrhrojdJsUy025X5udt.97ybVJU-1759972950-1.0.1.1-736ivUunLC4ibUp7.mN1_3H5Tb8SNuBqV6z6guHWAYSFJEKbu6EiQkCZaRPOokwdutzUURAD4XZEmSPkqxgEmCkNjUVEulEprxIj9Avmk.k; path=/; expires=Thu, 09-Oct-25 01:52:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=OT3QznwgkWx8OSa3SMOVCGyWRpG2cENu7L69rIE0IhY-1759972950636-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98ba113bbc68eadb-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 20:22:30,647 [DEBUG] openai._base_client: request_id: req_56aaa63d7f6a4b67a14fd13db7563926
2025-10-08 20:22:30,779 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 20:22:30,805 [DEBUG] chromadb.config: Starting component System
2025-10-08 20:22:30,805 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 20:22:30,805 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 20:22:30,805 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 20:22:30,806 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 20:22:30,809 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x121f0ce90>
2025-10-08 20:22:30,809 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,809 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:30,809 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,809 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:30,809 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,822 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 01:22:30 GMT')])
2025-10-08 20:22:30,822 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 20:22:30,822 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,823 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:30,823 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:30,823 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:30,824 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 20:22:30,829 [DEBUG] chromadb.config: Starting component System
2025-10-08 20:22:30,830 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 20:22:30,830 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 20:22:30,830 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 20:22:30,830 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 20:22:30,834 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1236284d0>
2025-10-08 20:22:30,834 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,834 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:30,834 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,834 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:30,834 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 01:22:30 GMT')])
2025-10-08 20:22:30,844 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:30,844 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,845 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:30,845 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,845 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:30,845 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:30,846 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 01:22:30 GMT')])
2025-10-08 20:22:30,846 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 20:22:30,846 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:30,846 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:30,846 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:30,846 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:30,847 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:30,847 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:30,847 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:30,847 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:30,847 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:30,854 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 01:22:30 GMT')])
2025-10-08 20:22:30,854 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 20:22:30,854 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:30,854 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:30,854 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:30,854 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:30,936 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-3ca4e9e1-0a59-483d-b6bd-42f45acecc53', 'post_parser': <function Embeddings.create.<locals>.parser at 0x121ffb1a0>, 'json_data': {'input': [[10714, 279, 2683, 198, 10714, 40660, 408, 290, 198, 41203, 408, 290, 374, 264, 2539, 24358, 7528, 15009, 10105, 2883, 13, 1226, 1304, 323, 10299, 3241, 15771, 323, 3956, 430, 2410, 6650, 323, 6493, 86282, 11704, 311, 13723, 323, 8420, 13, 5751, 15009, 11, 9624, 11, 828, 11, 3217, 2955, 11, 323, 11005, 6425, 17357, 43880, 18475, 323, 5536, 369, 20790, 8403, 13, 11452, 68720, 304, 1561, 16228, 11, 1057, 32027, 315, 220, 21, 11, 931, 10, 40660, 14846, 28421, 10105, 505, 2212, 279, 24867, 13, 40660, 408, 290, 374, 5918, 1422, 2883, 25, 40660, 408, 290, 2361, 25, 13325, 25922]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 20:22:30,937 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 20:22:30,937 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 20:22:30,968 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12362bd10>
2025-10-08 20:22:30,968 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x121b62d50> server_hostname='api.openai.com' timeout=None
2025-10-08 20:22:31,008 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x121ecb510>
2025-10-08 20:22:31,008 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,008 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:31,008 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:31,008 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:31,008 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,304 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 01:22:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'65'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-797b6d7f75-b95mq'), (b'x-envoy-upstream-service-time', b'198'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999899'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_f6d5f92b404241058d16cab95ce4b45e'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=K0MuFD0CWLR1p4hXpvXa1UqYrEDjX6f2YZ3Wzgm6QOw-1759972951-1.0.1.1-d4r9oD578RLy_Xc.GApyWW30QIcxJ.wB9EoQqLyUMiqPwT3nk4paSFBWs6ZCTbNJmtBeGxs1ghfAuj7RmiamWz.mAkgn3mH4ZTiytOV..jc; path=/; expires=Thu, 09-Oct-25 01:52:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=8JAjylMJTus3jobYAVfR7NrwWOx4LAxLUgcU0Lof_gY-1759972951375-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98ba114078a2f4ae-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 20:22:31,305 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 20:22:31,306 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:31,309 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:31,309 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:31,309 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:31,309 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 01:22:31 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '65'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-797b6d7f75-b95mq'), ('x-envoy-upstream-service-time', '198'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999899'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '6ms'), ('x-request-id', 'req_f6d5f92b404241058d16cab95ce4b45e'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=K0MuFD0CWLR1p4hXpvXa1UqYrEDjX6f2YZ3Wzgm6QOw-1759972951-1.0.1.1-d4r9oD578RLy_Xc.GApyWW30QIcxJ.wB9EoQqLyUMiqPwT3nk4paSFBWs6ZCTbNJmtBeGxs1ghfAuj7RmiamWz.mAkgn3mH4ZTiytOV..jc; path=/; expires=Thu, 09-Oct-25 01:52:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=8JAjylMJTus3jobYAVfR7NrwWOx4LAxLUgcU0Lof_gY-1759972951375-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98ba114078a2f4ae-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 20:22:31,309 [DEBUG] openai._base_client: request_id: req_f6d5f92b404241058d16cab95ce4b45e
2025-10-08 20:22:31,310 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,310 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:31,311 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:31,311 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:31,311 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,322 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'5694'), (b'date', b'Thu, 09 Oct 2025 01:22:30 GMT')])
2025-10-08 20:22:31,322 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-08 20:22:31,322 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:31,322 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:31,323 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:31,323 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:31,326 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-79ca4e25-35ec-429c-928c-db2bb2663d28', 'json_data': {'messages': [{'content': '\n    You are an expert cover letter writer specializing in creating compelling, personalized cover letters that effectively connect a candidate\'s background to specific job opportunities.\n\n    Your task is to analyze the job posting, personalized resume, and retrieved context to generate ONLY the cover letter content paragraphs in plain LaTeX format.\n\n    POSITION: Python Developer\n    COMPANY: Ascendion\n    JOB POSTING:\n    About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!\n\n    PERSONALIZED RESUME CONTENT:\n    RESUME: Amirreza Sokhankhosh\n‚ôÇphone431-293-6515 /envel‚å¢peamirreza.skhn@gmail.com /linkedinLinkedIn /githubGitHub ·ΩãCPortfolio\nHighlight of Relevant Skills\n‚Ä¢ Generative AI & LLMs: Designed and deployed applications powered by LLMs using LangChain, OpenAI API,\nvector databases (FAISS), and Ollama, with expertise in retrieval-augmented generation (RAG), fine-tuning\n(LoRA), and advanced prompting strategies (CoT, ReAct, Zero/Few-Shot).\n‚Ä¢ Machine Learning Engineering: Skilled in developing, training, and scaling ML models with PyTorch, Ten-\nsorFlow, Hugging Face Transformers, applying best practices in distributed AI, model evaluation, and MLOps\n(Docker, Kubernetes, CI/CD) for production-grade pipelines.\n‚Ä¢ NLP & Computer Vision: Applied deep learning to text and image data through projects in natural language\nprocessing, OCR, and image detection/segmentation , demonstrating the ability to handle multimodal ML\nchallenges.\n‚Ä¢ Full-Stack Development: Proven track record in building and integrating end-to-end applications, combining\nReact (TypeScript) for intuitive frontends with scalable backends in Flask, Django, NestJS, and databases such\nas PostgreSQL.\n‚Ä¢ APIs & Systems Design: Strong background in architecting and scalinginference APIs and cloud-native solutions,\nbridging robust backend services with user-friendly interfaces.\n‚Ä¢ Cloud & DevOps: Hands-on with AWS, Docker, Kubernetes, GitHub Actions, and CI/CD pipelines to ensure\nreliable and scalable deployments.\n‚Ä¢ Collaboration & Agile Practices: Experienced in Agile/Scrum workflows, cross-functional collaboration, code\nreviews, and delivering high-quality, production-ready systems in both research and industry settings.\nExperience\nUniversity of Manitoba Sep 2023 ‚Äì Jul 2025\nGraduate Research Assistant Winnipeg, Canada\n‚Ä¢ Designed/developed four novel distributed AI architectures, emphasizing secure, scalable & efficient system design\nand modular API development (TensorFlow, PyTorch, Hyperledger Fabric, Express.js/Flask).\n‚Ä¢ Achieved significant system optimizations: reduced communication overhead by 85.2% & increased fault tolerance\nby 62.7%, demonstrating expertise in building reliable AI systems.\n‚Ä¢ Authored/contributed to multiple research publications in top-tier IEEE venues.\nBobo App Ltd. May 2024 ‚Äì Aug 2024\nFull Stack Developer Intern Winnipeg, Canada\n‚Ä¢ Collaborated with a team at MILA in developing an efficient chatbot responsible for customer service.\n‚Ä¢ Accelerated product development by designing and implementingRESTful APIs using Supabase and PostgreSQL,\nenhancing backend functionality and data management.\n‚Ä¢ Automated data integration by developing a Python script to convert CSV data into executable SQL, significantly\nstreamlining database population.\n‚Ä¢ Ensured seamless full-stack integration through close collaboration with the front-end team to align API specifi-\ncations with user interface requirements.\n‚Ä¢ Contributed to agile workflow efficiency by utilizing the Atlassian suite ( Jira, Confluence) for task management,\ndocumentation, and team coordination.\nNadin Soft (Sadr Group Company) Jul 2020 ‚Äì Dec 2020\nFull-stack Developer Intern\n‚Ä¢ Drove the successful completion of a critical internal project, addressing key deficiencies in API security, authen-\ntication, and data modeling for production deployment.\n‚Ä¢ Developed robust RESTful APIs with AdonisJS and Express.js, implementing JWT authentication to secure\nuser access and enable core application functionalities.‚Ä¢ Designed and implemented highly accurate MySQL database schemas for precise tracking and management of em-\nployee working and non-working hours, foundational for the project‚Äôs business logic.\n‚Ä¢ Deployed fully functional features to the live production server and created thorough Swagger API documenta-\ntion, facilitating seamless internal integration and knowledge transfer.\n‚Ä¢ Contributed to an Agile Scrum development team by actively tracking and managing personal tasks within ClickUp,\nensuring efficient progress and clear communication within sprints and CI/CD pipelines.\nProjects\nMarkMate |React, Django, PostgreSQL, OpenAI API, LangChain|Link\n‚Ä¢ Built a full-stack web app for automated assignment grading using React, Django, and PostgreSQL.\n‚Ä¢ Integrated OCR and RAG-based LLM agents with custom test-case pipelines to generate consistent feedback.\n‚Ä¢ Reduced grading time and instructor effort by ‚âà80%, improving reliability and scalability.\nPaper Summarizer |Hugging Face Transformers, Ollama, OCR, LLMs|Link\n‚Ä¢ Engineered an LLM-powered summarization tool for academic papers with OCR + NLP pipelines .\n‚Ä¢ Applied LLaVA and LLaMA models for segmentation, entity extraction, and multimodal summarization.\n‚Ä¢ Generated holistic one-page summaries, improving research efficiency for academic users.\nUnified Image Evaluation Metric |PyTorch, NumPy|Link\n‚Ä¢ Designed a novel evaluation metric for generative AI, integrating precision, recall, quality, and diversity.\n‚Ä¢ Quantified generalization with KNN-based scoring and unified results via harmonic sum.\n‚Ä¢ Applied as a regularization term to WGAN-GP and Diffusion models , improving generalization.\nDigital Twin ‚Äì FL |TensorFlow, PyTorch Forecasting, Flask, Express.js|Link\n‚Ä¢ Developed a smart-building Digital Twin predicting CO2, humidity, and temperature across 76 IoT rooms.\n‚Ä¢ Implemented Temporal Fusion Transformers (TFT) for multivariate time series forecasting.\n‚Ä¢ Integrated into a federated learning framework, ensuring scalability and privacy-preserving analytics.\nFacial Landmark & Boundary Detection |OpenCV, TensorFlow, Faster R-CNN|Link\n‚Ä¢ Developed a computer vision pipeline for multi-face detection and landmark localization .\n‚Ä¢ Trained dual Faster R-CNN models to detect boundaries and extract 68 facial landmarks.\n‚Ä¢ Delivered robust performance for real-time facial feature extraction.\nEducation\nUniversity of Manitoba Sep 2023 ‚Äì Aug 2025\nMaster of Science in Computer Science (GPA: 4.4 / 4.5) Winnipeg, Canada\n‚Ä¢ Relevant Coursework: Security & Privacy, Deep Generative Modeling, Blockchain & Distributed Systems: A+\nK.N. Toosi University of Technology Sep 2018 ‚Äì Feb 2023\nBachelor of Science in Computer Engineering\nTechnical Skills\nAI / Machine Learning : TensorFlow, PyTorch, Flower, Numpy, Pandas, OpenCV, Transformers, LangChain, Lang-\nGraph, Scikit-learn, Keras, Matplotlib, TensorBoard, CUDA.\nLanguages: Python, C++, Java, JavaScript, TypeScript, Node JS, Solidity.\nCloud & DevOps: Docker, Kubernetes, Hyperledger Fabric, Git, GitHub, Linux, CI/CD, AWS.\nDatabases: PostgreSQL, MongoDB, MySQL, Redis, Db2, Supabase.\nWeb Frameworks: Back-end: Nest.JS, Django, Express.JS, Flask, Adonis.JS. Front-end: React, Tailwind\nTools & Methodologies: Jira, Confluence, Swagger, ClickUp, Agile, Scrum.\nCertificates\nInferring Causal Effects from Observational Data University of Pennsylvania|Credential\nReinforcement Learning Specialization University of Alberta|Credential\n\n    RETRIEVED CONTEXT FROM KNOWLEDGE BASE:\n    [Document(id=\'3325f8b5-cac8-47aa-ab0d-018c50ca37e5\', metadata={\'company\': \'Ascendion\', \'position\': \'Python Developer\', \'start_index\': 0, \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'15649aed-4073-49f5-807e-04fc96505ce0\', metadata={\'company\': \'Ascendion\', \'source\': \'cover letter\', \'position\': \'Python Developer\', \'start_index\': 0}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'dbcc2e7c-246d-45e7-b5e7-41e0fd0b758a\', metadata={\'position\': \'Python Developer\', \'start_index\': 0, \'company\': \'Ascendion\', \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and extensive experience with frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team\'s mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its dedication to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'c9163008-0ebe-447e-9477-80ed0da48ef6\', metadata={\'start_index\': 0, \'position\': \'Python Developer\', \'company\': \'Ascendion\', \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'9f633635-33d1-40f1-a506-22d8d27d73ba\', metadata={\'position\': \'Python Developer\', \'start_index\': 0, \'company\': \'Ascendion\', \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'39fc2647-7830-4dac-9b5d-b1aba7e35b2a\', metadata={\'position\': \'Python Developer\', \'company\': \'Ascendion\', \'start_index\': 0, \'source\': \'cover letter\'}, page_content="I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my extensive experience in \\\\textbf{Python development} and proficiency in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences. Ascendion\'s culture of innovation and its commitment to engineering that elevates life strongly resonate with my professional values and aspirations."), Document(id=\'b4447976-35c8-4e28-9835-d87a11efa906\', metadata={\'start_index\': 0, \'source\': \'cover letter\', \'position\': \'Python Developer\', \'company\': \'Ascendion\'}, page_content=\'I am excited to apply for the Python Developer position at \\\\textbf{Ascendion}. With my background in \\\\textbf{Python development} and extensive experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of transforming digital engineering and delivering captivating experiences.\\n\\nIn my recent projects, I have built scalable and secure applications using \\\\textbf{Python} and its frameworks, aligning with your requirements. For example, in my \\\\textbf{Mini Task Manager} project, I developed a token-based authentication system using \\\\textbf{Django REST Framework}, which resulted in a 25\\\\% increase in user engagement. Furthermore, my proficiency with \\\\textbf{Docker} and \\\\textbf{Kubernetes} has enabled me to reduce setup times by 30\\\\%, preparing me for the dynamic challenges outlined in your job posting.\'), Document(id=\'b35b20df-30ed-4c58-ba25-4ad9f0949211\', metadata={\'start_index\': 1497, \'source\': \'cover letter\', \'company\': \'Ascendion\', \'position\': \'Python Developer\'}, page_content="I am enthusiastic about the opportunity to contribute to \\\\textbf{Ascendion}\'s mission by bringing my skills to build advanced digital solutions. My background in \\\\textbf{distributed systems} and passion for \\\\textbf{digital engineering} uniquely position me to make significant contributions to your team. I am excited to bring my perspective and technical expertise to Ascendion, collaborating to create technology that elevates life.")]\n                                                       \n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Company mission, values, and culture\n       - Key technical requirements and preferred technologies\n       - Required experience level and responsibilities\n       - Specific skills and qualifications sought\n\n    2. Use the personalized resume content to:\n       - Highlight the most relevant experiences and achievements\n       - Showcase technical skills that match job requirements\n       - Reference specific projects that demonstrate required capabilities\n       - Quantify accomplishments where possible\n\n    3. Leverage the retrieved context to:\n       - Add depth and authenticity to your statements\n       - Reference relevant research, projects, or experiences not in the resume\n       - Demonstrate domain knowledge and passion for the field\n       - Show progression and learning from past experiences\n\n    4. Follow the template structure but adapt content to:\n       - Match the company\'s tone and industry language\n       - Create a narrative that connects past experiences to future contributions\n       - Show genuine enthusiasm for the specific role and company\n       - Address any potential gaps or explain career transitions\n\n\n    CRITICAL OUTPUT REQUIREMENTS:\n    - Generate ONLY the cover letter content paragraphs\n    - DO NOT include any LaTeX document structure (\\documentclass, \\begin{document}, \\begin{letter}, \\end{letter}, \\end{document})\n    - DO NOT include any explanations, introductions, or meta-commentary\n    - DO NOT include salutations like "Dear Hiring Manager" or closings like "Sincerely"\n    - Start directly with the first paragraph content\n    - End with the last paragraph content\n    - Use clean paragraph breaks (double newlines) between paragraphs\n\n    CONTENT STRUCTURE:\n    Write exactly 3-4 paragraphs of cover letter content:\n\n    Paragraph 1: Express enthusiasm for the specific role and company, briefly state your relevant background and why you\'re applying.\n\n    Paragraph 2: Highlight your most relevant technical experiences and projects with specific examples that match the job requirements.\n\n    Paragraph 3: Demonstrate alignment with the company\'s mission and technology stack, referencing retrieved context to show deeper knowledge and genuine interest.\n\n    Paragraph 4 (optional): Reiterate enthusiasm and mention specific contributions you can make to the team and company.\n\n                                                       \n    COVER LETTER STRUCTURE:\n    1. Opening paragraph: Express enthusiasm for the specific role and company, briefly state your relevant background\n    2. Body paragraph 1: Highlight relevant technical experience and projects with specific examples\n    3. Body paragraph 2: Demonstrate alignment with company mission/technology stack, reference retrieved context for depth\n    4. Closing paragraph: Reiterate enthusiasm, mention specific contributions you can make, professional closing\n\n    TONE AND STYLE:\n    - Professional yet enthusiastic\n    - Confident but not boastful\n    - Specific and detailed, not generic\n    - Forward-looking and solution-oriented\n    - Authentic and genuine\n    LATEX FORMATTING REQUIREMENTS:\n    - Use proper LaTeX escaping: \\& instead of & for ampersands\n    - Use \\% for percentages in text  \n    - Use \\textbf{} for emphasizing key technologies, company names, and achievements\n    - Write in clean, readable paragraph format\n    - Use standard paragraph separation (double newlines)\n\n    CONTENT GUIDELINES:\n    - Reference specific technologies, frameworks, and methodologies from the job posting\n    - Use metrics and quantified achievements where available\n    - Show understanding of the company\'s challenges and how you can help\n    - Demonstrate continuous learning and adaptability\n    - Connect past experiences to future potential contributions\n    - Use retrieved context to add unique insights or relevant background\n    - Be professional yet enthusiastic, confident but not boastful\n    - Make it specific and detailed, not generic\n\n    OUTPUT FORMAT EXAMPLE:\n    I am excited to apply for the {position} position at \\textbf{{company name}}. With my background in \\textbf{relevant technology} and experience in \\textbf{relevant domain}, I am eager to contribute to your team\'s mission of advancing \\textbf{company focus area}.\n\n    In my recent projects, I have built scalable applications using \\textbf{specific technologies} that align with your requirements. For example, in my \\textbf{project name} project, I developed \\textbf{specific implementation} that resulted in \\textbf{quantified outcome}. My experience with \\textbf{relevant technology stack} has prepared me to tackle the challenges outlined in your job posting.\n\n    I am particularly drawn to \\textbf{{company name}}\'s commitment to \\textbf{company values/mission}. My research in \\textbf{relevant area from context} has given me deep insights into \\textbf{relevant domain knowledge}, and I am excited about the opportunity to apply this knowledge in a production environment where I can help \\textbf{specific company goals}.\n\n    Generate the cover letter content paragraphs following this exact format:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 20:22:31,327 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 20:22:31,327 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 20:22:31,350 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): us.i.posthog.com:443
2025-10-08 20:22:31,357 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c81690>
2025-10-08 20:22:31,357 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x121b63260> server_hostname='api.openai.com' timeout=None
2025-10-08 20:22:31,393 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c81ad0>
2025-10-08 20:22:31,393 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,393 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:31,393 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:31,393 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:31,393 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:31,613 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 20:22:37,017 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 01:22:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'5306'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'5473'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'24744'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'10.512s'), (b'x-request-id', b'req_35ed2a93264b48c3860122690bb1a8c4'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=BrPCYlZ08G431WUmR0xsk.4ccjnr2SzUeswZALxWrsA-1759972957-1.0.1.1-NZAe1DRnsaqsesPeGCsHL3yAWYqh_Vloo8NF0lo6CVYD6bpCXuBeG_jaSem5dQ0cx2qPqQ9n.E2ZdqFO.t_FyzfqKB0vihHhyauQr5iyTMA; path=/; expires=Thu, 09-Oct-25 01:52:37 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=e3JpE2VAWPOC5zF4nRzLeMkiMsvaDqJg9jnPDTIC3HE-1759972957087-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98ba1142eb13f8d7-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 20:22:37,019 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 20:22:37,019 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:37,020 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:37,020 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:37,020 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:37,020 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Thu, 09 Oct 2025 01:22:37 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '5306'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '5473'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '30000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '24744'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '10.512s'), ('x-request-id', 'req_35ed2a93264b48c3860122690bb1a8c4'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=BrPCYlZ08G431WUmR0xsk.4ccjnr2SzUeswZALxWrsA-1759972957-1.0.1.1-NZAe1DRnsaqsesPeGCsHL3yAWYqh_Vloo8NF0lo6CVYD6bpCXuBeG_jaSem5dQ0cx2qPqQ9n.E2ZdqFO.t_FyzfqKB0vihHhyauQr5iyTMA; path=/; expires=Thu, 09-Oct-25 01:52:37 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=e3JpE2VAWPOC5zF4nRzLeMkiMsvaDqJg9jnPDTIC3HE-1759972957087-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98ba1142eb13f8d7-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 20:22:37,020 [DEBUG] openai._base_client: request_id: req_35ed2a93264b48c3860122690bb1a8c4
2025-10-08 20:22:38,433 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 20:22:38,439 [DEBUG] chromadb.config: Starting component System
2025-10-08 20:22:38,439 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 20:22:38,439 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 20:22:38,439 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 20:22:38,439 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 20:22:38,441 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c22510>
2025-10-08 20:22:38,441 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,442 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,442 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,443 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 20:22:38,449 [DEBUG] chromadb.config: Starting component System
2025-10-08 20:22:38,449 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 20:22:38,449 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 20:22:38,449 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 20:22:38,449 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 20:22:38,451 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c28350>
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,452 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 20:22:38,452 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,453 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,453 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,454 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,455 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,455 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,455 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,455 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,455 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,458 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,458 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 20:22:38,458 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,458 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,458 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,458 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,462 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-f7b0840d-6f36-49dc-ba5c-92f994277d9b', 'post_parser': <function Embeddings.create.<locals>.parser at 0x121ff8220>, 'json_data': {'input': [[40, 1097, 12304, 311, 3881, 369, 279, 13325, 25922, 2361, 520, 1144, 1342, 13536, 90, 41203, 408, 290, 7966, 3161, 856, 4092, 304, 1144, 1342, 13536, 90, 31380, 4500, 92, 323, 3217, 304, 49125, 1778, 439, 1144, 1342, 13536, 90, 35, 5970, 2186, 1144, 1342, 13536, 90, 3968, 1091, 2186, 323, 1144, 1342, 13536, 90, 33274, 7227, 2186, 358, 1097, 24450, 311, 17210, 311, 701, 2128, 596, 9131, 315, 46890, 7528, 15009, 323, 24944, 86282, 11704, 13, 40660, 408, 290, 596, 7829, 315, 19297, 323, 1202, 15507, 311, 15009, 430, 12231, 988, 2324, 16917, 89986, 449, 856, 6721, 2819, 323, 58522, 13], [644, 856, 3293, 7224, 11, 358, 617, 5918, 69311, 323, 9966, 8522, 1701, 1144, 1342, 13536, 90, 31380, 92, 323, 1202, 49125, 11, 5398, 287, 449, 701, 8670, 13, 1789, 3187, 11, 304, 856, 1144, 1342, 13536, 90, 9126, 97742, 92, 2447, 11, 358, 8040, 459, 28598, 16720, 66288, 1887, 1701, 1144, 1342, 13536, 90, 15143, 2186, 1144, 1342, 13536, 90, 35, 5970, 2186, 323, 1144, 1342, 13536, 90, 4226, 60896, 2186, 18189, 66288, 892, 555, 13489, 220, 1490, 59, 14697, 24296, 11, 856, 3217, 449, 1144, 1342, 13536, 90, 35, 13973, 92, 323, 1144, 1342, 13536, 90, 42, 30927, 92, 9147, 757, 311, 4305, 11297, 24047, 10105, 11, 20646, 757, 369, 279, 8915, 11774, 520, 40660, 408, 290, 13], [40, 1097, 8104, 15107, 311, 1144, 1342, 13536, 90, 41203, 408, 290, 11923, 82, 15507, 311, 6968, 5557, 430, 12231, 988, 2324, 323, 701, 25679, 389, 7528, 15009, 369, 46331, 220, 2636, 8403, 13, 3092, 3217, 304, 1144, 1342, 13536, 90, 63475, 6067, 92, 323, 5780, 6975, 5825, 757, 449, 279, 7512, 4460, 311, 25555, 304, 420, 3560, 13, 358, 1097, 12304, 922, 279, 22199, 315, 19486, 14713, 48448, 15592, 323, 5780, 6975, 14645, 311, 4726, 40660, 408, 290, 596, 18699, 9021, 323, 11886, 6485, 5435, 369, 701, 8403, 382, 40, 1097, 42702, 922, 279, 6776, 311, 17210, 311, 1144, 1342, 13536, 90, 41203, 408, 290, 11923, 82, 9131, 555, 12967, 856, 7512, 311, 1977, 11084, 7528, 10105, 13, 3161, 856, 3831, 4092, 304, 13325, 323, 5552, 14645, 11, 358, 1097, 16913, 430, 358, 649, 1304, 14247, 323, 5199, 19564, 311, 701, 2128, 11, 10695, 40660, 408, 290, 6493, 86282, 323, 14713, 48448, 10105, 13]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 20:22:38,462 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 20:22:38,462 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 20:22:38,490 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c31190>
2025-10-08 20:22:38,490 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x121f2f020> server_hostname='api.openai.com' timeout=None
2025-10-08 20:22:38,527 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c2ab50>
2025-10-08 20:22:38,527 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,528 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,528 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,528 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,528 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,727 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 01:22:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'102'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-76f88f4767-b4zgv'), (b'x-envoy-upstream-service-time', b'123'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999623'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'22ms'), (b'x-request-id', b'req_7022852cbbee45b1a8ffeb78025d4cc9'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=a28QBEKlBKYVNVkpFjZPYgS5CudSYiaifNErHqVXN4M-1759972958-1.0.1.1-SRwuhbW_nSr_BoN4ZCBFldRZ7ihOb1Q10E.8XVvasHh9XnSa4HdWibyQTuinozzDtWHo9kUxuTk1TpjN9B1wvmc3aqw1Tq76N1ZSOtdy0cI; path=/; expires=Thu, 09-Oct-25 01:52:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=UFncv8jjZvOqE8d5DZpKiIpMCUyaQpCIDyP3LeI8q.Q-1759972958801-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98ba116f79a83ef3-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 20:22:38,728 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 20:22:38,729 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,733 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,733 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,733 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,734 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 01:22:38 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '102'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-76f88f4767-b4zgv'), ('x-envoy-upstream-service-time', '123'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999623'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '22ms'), ('x-request-id', 'req_7022852cbbee45b1a8ffeb78025d4cc9'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=a28QBEKlBKYVNVkpFjZPYgS5CudSYiaifNErHqVXN4M-1759972958-1.0.1.1-SRwuhbW_nSr_BoN4ZCBFldRZ7ihOb1Q10E.8XVvasHh9XnSa4HdWibyQTuinozzDtWHo9kUxuTk1TpjN9B1wvmc3aqw1Tq76N1ZSOtdy0cI; path=/; expires=Thu, 09-Oct-25 01:52:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=UFncv8jjZvOqE8d5DZpKiIpMCUyaQpCIDyP3LeI8q.Q-1759972958801-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98ba116f79a83ef3-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 20:22:38,734 [DEBUG] openai._base_client: request_id: req_7022852cbbee45b1a8ffeb78025d4cc9
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'55'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,735 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,735 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,736 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,736 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,736 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,736 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,736 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,757 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 20:22:38,769 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'2'), (b'date', b'Thu, 09 Oct 2025 01:22:38 GMT')])
2025-10-08 20:22:38,770 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-08 20:22:38,770 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,770 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:38,770 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:38,770 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:38,786 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-06c4cbac-7ce3-4ed3-be3b-ac240d846356', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10678dd00>, 'json_data': {'input': 'Company: Ascendion. Position: Python Developer. Description: About the job\nAbout Ascendion\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n¬∑ Build the coolest tech for world‚Äôs leading brands\n¬∑ Solve complex problems ‚Äì and learn new skills\n¬∑ Experience the power of transforming digital engineering for Fortune 500 clients\n¬∑ Master your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\n\n\nJob Title : Python Developer\n\nLocation: Hybrid in Toronto, Canada\n\n \n\nLooking for Python Developer with Django, Flask and FastAPI\n\n\n\nSalary Range: The salary for this position is between $130,000 ‚Äì $140,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\n\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\n\n\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let‚Äôs talk!', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 20:22:38,787 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 20:22:38,787 [DEBUG] httpcore.connection: close.started
2025-10-08 20:22:38,787 [DEBUG] httpcore.connection: close.complete
2025-10-08 20:22:38,787 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 20:22:38,826 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x123c22ed0>
2025-10-08 20:22:38,826 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x107dff2f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 20:22:38,862 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x121ea3910>
2025-10-08 20:22:38,862 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 20:22:38,862 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 20:22:38,862 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 20:22:38,862 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 20:22:38,862 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 20:22:39,121 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 01:22:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'83'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-797b6d7f75-htpvc'), (b'x-envoy-upstream-service-time', b'148'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999384'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'36ms'), (b'x-request-id', b'req_95a0dfe4adac42d48523040ff076a618'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98ba11719dfdae77-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 20:22:39,122 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 20:22:39,122 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 20:22:39,123 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 20:22:39,123 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 20:22:39,123 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 20:22:39,124 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 01:22:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '83', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-797b6d7f75-htpvc', 'x-envoy-upstream-service-time': '148', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999384', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '36ms', 'x-request-id': 'req_95a0dfe4adac42d48523040ff076a618', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98ba11719dfdae77-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 20:22:39,124 [DEBUG] openai._base_client: request_id: req_95a0dfe4adac42d48523040ff076a618
2025-10-08 20:22:39,147 [INFO] src.api.app: ‚úÖ POST /api/generate/cover-letter/ - 200 - 9.355s
2025-10-08 20:22:39,147 [WARNING] src.api.app: üêå Slow request: POST /api/generate/cover-letter/ took 9.355s
2025-10-08 20:22:39,148 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:22:39] "POST /api/generate/cover-letter/ HTTP/1.1" 200 -
2025-10-08 20:22:52,122 [INFO] src.api.app: üåê GET /api/resumes/generated/Python Developer.pdf - 127.0.0.1
2025-10-08 20:22:52,123 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Python Developer.pdf - 404 - 0.002s
2025-10-08 20:22:52,124 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:22:52] "[33mGET /api/resumes/generated/Python%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-08 20:22:54,445 [INFO] src.api.app: üåê GET /api/resumes/generated/Python Developer.pdf - 127.0.0.1
2025-10-08 20:22:54,446 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Python Developer.pdf - 404 - 0.001s
2025-10-08 20:22:54,447 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:22:54] "[33mGET /api/resumes/generated/Python%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-08 20:22:57,670 [INFO] src.api.app: üåê GET /api/resumes/preview/ml-engineering - 127.0.0.1
2025-10-08 20:22:57,673 [INFO] src.api.app: ‚úÖ GET /api/resumes/preview/ml-engineering - 304 - 0.003s
2025-10-08 20:22:57,674 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:22:57] "[36mGET /api/resumes/preview/ml-engineering HTTP/1.1[0m" 304 -
2025-10-08 20:23:03,732 [INFO] src.api.app: üåê GET /api/resumes/preview/ml-engineering - 127.0.0.1
2025-10-08 20:23:03,733 [INFO] src.api.app: ‚úÖ GET /api/resumes/preview/ml-engineering - 304 - 0.001s
2025-10-08 20:23:03,736 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 20:23:03] "[36mGET /api/resumes/preview/ml-engineering HTTP/1.1[0m" 304 -
2025-10-08 22:27:40,485 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-08 22:27:40,486 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-08 22:27:40,486 [INFO] werkzeug:  * Restarting with stat
2025-10-08 22:27:41,470 [WARNING] werkzeug:  * Debugger is active!
2025-10-08 22:27:41,480 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-08 23:26:00,788 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-08 23:26:00,791 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.010s
2025-10-08 23:26:00,796 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:26:00] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:26:00,800 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-08 23:26:00,802 [DEBUG] src.api.app: üìù Request data: {'job_description': 'About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'company_name': 'BMO', 'position_title': 'AI Developer'}
2025-10-08 23:26:01,309 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-eefc3c49-aaf5-44a8-91c4-d93c4f8e2ebd', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10c5fc860>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:26:01,336 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:26:01,337 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:26:01,394 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf8a590>
2025-10-08 23:26:01,396 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:26:01,454 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf94250>
2025-10-08 23:26:01,455 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:01,455 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:01,455 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:01,456 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:01,456 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:02,691 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'133'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-56fc795659-k4r7s'), (b'x-envoy-upstream-service-time', b'310'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_5d8934495335419ebbc7e712342d679b'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fxa.PuD4b4ccWmuQE0TEpHbyhW4eKsgUwm4lH3Khago-1759983962-1.0.1.1-6gVVRE_ZY3FRLQkJu5VNJQ.smuybEWZZWh8U60TDNdrlYoj.n6dM1IDfB1ckzyW.1UR1cIgW1jcrP5fYgIRHZ5CcFcEAHaiBu8f5lt5c9Ww; path=/; expires=Thu, 09-Oct-25 04:56:02 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=rPzkt2vDdBeKR6KqJxRFkvYT0JWGhx.5UuAmBiBiTU0-1759983962749-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1e0faa8d6205-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:02,692 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:26:02,692 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:02,693 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:02,694 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:02,694 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:02,694 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:26:02 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '133'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-56fc795659-k4r7s'), ('x-envoy-upstream-service-time', '310'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999342'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '39ms'), ('x-request-id', 'req_5d8934495335419ebbc7e712342d679b'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=fxa.PuD4b4ccWmuQE0TEpHbyhW4eKsgUwm4lH3Khago-1759983962-1.0.1.1-6gVVRE_ZY3FRLQkJu5VNJQ.smuybEWZZWh8U60TDNdrlYoj.n6dM1IDfB1ckzyW.1UR1cIgW1jcrP5fYgIRHZ5CcFcEAHaiBu8f5lt5c9Ww; path=/; expires=Thu, 09-Oct-25 04:56:02 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=rPzkt2vDdBeKR6KqJxRFkvYT0JWGhx.5UuAmBiBiTU0-1759983962749-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb1e0faa8d6205-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:26:02,694 [DEBUG] openai._base_client: request_id: req_5d8934495335419ebbc7e712342d679b
2025-10-08 23:26:02,723 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 1.922s
2025-10-08 23:26:02,723 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:26:02] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:26:02,728 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-08 23:26:02,729 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-08 23:26:02,729 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:26:02] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-08 23:26:02,731 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-08 23:26:02,732 [DEBUG] src.api.app: üìù Request data: {'jobDescription': 'About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'companyName': 'BMO', 'positionTitle': 'AI Developer', 'strategy': 'generate'}
2025-10-08 23:26:02,742 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-2a99da1c-e3b9-4c46-bd59-eb10a336e4ac', 'post_parser': <function Embeddings.create.<locals>.parser at 0x103a1aca0>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:26:02,742 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:26:02,742 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:02,743 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:02,743 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:02,743 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:02,743 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:05,724 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'84'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-7cc8d487b9-thqkl'), (b'x-envoy-upstream-service-time', b'413'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_3a9fbe7d9dbc495db81eae6103595931'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1e17bf456205-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:05,725 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:26:05,735 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:05,737 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:05,737 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:05,737 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:05,737 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:05 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '84', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-7cc8d487b9-thqkl', 'x-envoy-upstream-service-time': '413', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999342', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_3a9fbe7d9dbc495db81eae6103595931', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1e17bf456205-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:05,738 [DEBUG] openai._base_client: request_id: req_3a9fbe7d9dbc495db81eae6103595931
2025-10-08 23:26:05,796 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-e117a981-bf09-4b5e-97a4-c788528f5b3a', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring professional experiences to specific job requirements.\n    \n    Your task is to analyze the provided job posting and candidate experiences, then generate LaTeX-formatted resume entries that highlight the most relevant skills, achievements, and experiences for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE EXPERIENCES:\n    [\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Graduate Research Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2023-09",\n    "end_date": "2025-07",\n    "duration": "1 yr 11 mos",\n    "highlights": [\n      "Designed and developed four novel distributed AI architectures (PoCL, SSFL, BSFL, BPFL) addressing critical challenges in distributed learning.",\n      "PoCL: a blockchain consensus mechanism that repurposes energy-intensive mining for secure, incentive-aligned, and resource-efficient federated learning.",\n      "SSFL: An architecture enhancing scalability and efficiency for distributed deep learning.",\n      "BSFL: The first decentralized SplitFed Learning framework leveraging smart contracts for model integrity and decentralized coordination.",\n      "BPFL: A framework improving fairness and model ownership through contribution-based personalization and tokenized access.",\n      "Implemented and optimized these architectures using a robust tech stack, including TensorFlow, PyTorch for AI model development, Hyperledger Fabric for permissioned blockchain integration, and Express.js/Flask for modular API development.",\n      "Reduced communication overhead by 85.2%, improved fault tolerance by 62.7%, and enhanced energy efficiency through PoCL.",\n      "Authored research published/submitted to top-tier IEEE venues."\n    ],\n    "skills": [\n      "Distributed Machine Learning",\n      "TensorFlow",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Blockchain",\n      "System Optimization",\n      "Consensus Algorithms",\n      "Technical Communication",\n      "Problem Solving"\n    ],\n    "publications": [\n      "Proof-of-Collaborative-Learning: A Multi-winner Federated Learning Consensus Algorithm (Published)",\n      "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches (Accepted)",\n      "Towards Fair Model Ownership: Blockchain-Driven Personalization in Federated Learning (Under Review)"\n    ]\n  },\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Teaching Assistant",\n    "employment_type": "Contract",\n    "start_date": "2024-09",\n    "end_date": "2025-06",\n    "duration": "10 mos",\n    "highlights": [\n      "Delivered direct academic and technical support to over 400 undergraduate students across foundational Computer Science courses, including Introduction to Programming (Python), Object-Oriented Programming (Java), Data Structures & Algorithms, Computer Networks, and Distributed Systems.",\n      "Guided students through complex coding projects and theoretical challenges, focusing on problem-solving strategies, algorithm efficiency, and system design principles.",\n      "Offered specialized technical assistance at the university\'s Help Centre, adeptly diagnosing and resolving programming issues and advising on effective coding practices for diverse academic contexts."\n    ],\n    "skills": [\n      "Python",\n      "Java",\n      "Data Structures & Algorithms",\n      "Computer Networks",\n      "Distributed Systems",\n      "Technical Communication"\n    ]\n  },\n  {\n    "organization": "Bobo",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Full-stack Developer Intern",\n    "employment_type": "Internship",\n    "start_date": "2024-05",\n    "end_date": "2024-08",\n    "duration": "4 mos",\n    "highlights": [\n      "Accelerated product development by designing and implementing RESTful APIs using Supabase and PostgreSQL, enhancing backend functionality and data management.",\n      "Automated data integration by developing a Python script to convert CSV data into executable SQL, significantly streamlining database population.",\n      "Ensured seamless full-stack integration by collaborating closely with the front-end team to align API specifications with user interface requirements.",\n      "Contributed to agile workflow efficiency through active utilization of the Atlassian suite (Jira, Confluence) for task management, documentation, and team coordination."\n    ],\n    "skills": [\n      "Supabase",\n      "PostgreSQL",\n      "Jira",\n      "Confluence",\n      "Collaboration"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "location": "",\n    "title": "Lead Teaching Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2021-01",\n    "end_date": "2023-01",\n    "duration": "2 yrs 1 mo",\n    "highlights": [\n      "Supported and guided over 300 undergraduate students across complex technical subjects, including Linear Algebra, System Design and Analysis, Computer Networks, Discrete Mathematics, Operating Systems, and Algorithm Design.",\n      "Designed and developed engaging lectures, practical coding projects, and challenging assignments, significantly enhancing student comprehension and hands-on application of core computer science concepts.",\n      "Provided specialized support in areas like algorithm implementation, system architecture, and network protocols, bridging theoretical knowledge with practical problem-solving."\n    ],\n    "skills": [\n      "System Design and Analysis",\n      "Computer Networks",\n      "Algorithms",\n      "Operating Systems",\n      "Technical Teaching"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "title": "Research Assistant",\n    "location": "",\n    "employment_type": "Full-time",\n    "start_date": "2021-06",\n    "end_date": "2022-08",\n    "duration": "1 yr 3 mos",\n    "highlights": [\n      "Led an undergraduate research team, successfully initiating and guiding a study on solar panel funding fairness in Switzerland that culminated in key analytical findings.",\n      "Executed comprehensive data collection and cleaning of over 10GB of raw data from diverse government portals, ensuring high data integrity and preparing datasets for advanced analysis.",\n      "Leveraged advanced causal inference techniques in R to analyze complex datasets, producing detailed analytical reports that quantified and highlighted significant subsidy biases."\n    ],\n    "skills": [\n      "PostgreSQL",\n      "PostGIS",\n      "R",\n      "Machine Learning",\n      "Data Cleaning",\n      "Team Leadership"\n    ]\n  },\n  {\n    "organization": "Sadr Group Company",\n    "title": "Full-stack Developer Intern",\n    "location": "",\n    "employment_type": "Part-time",\n    "start_date": "2020-07",\n    "end_date": "2020-12",\n    "duration": "6 mos",\n    "highlights": [\n      "Drove the successful completion of a critical internal project, addressing key deficiencies in API security, authentication, and data modeling for production deployment.",\n      "Developed robust RESTful APIs with AdonisJS and Express.js, implementing JWT authentication to secure user access and enable core application functionalities.",\n      "Designed and implemented highly accurate MySQL database schemas for precise tracking and management of employee working and non-working hours, foundational for the project\'s business logic.",\n      "Deployed fully functional features to the live production server and created thorough Swagger API documentation, facilitating seamless internal integration and knowledge transfer.",\n      "Contributed to an Agile Scrum development team by actively tracking and managing personal tasks within ClickUp, ensuring efficient progress and clear communication within sprints and CI/CD pipelines."\n    ],\n    "skills": [\n      "Node.js",\n      "Express.js",\n      "Adonis.js",\n      "Vue.js",\n      "MySQL",\n      "Swagger",\n      "Technical Communication"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements, skills, technologies, and qualifications\n    2. Select the 3-4 most relevant experiences from the candidate\'s background\n    3. For each selected experience, generate a LaTeX resume entry following this EXACT format:\n\n    \\resumeSubheading\n        {Organization Name}{Start Date ‚Äì End Date}\n        {Job Title}{Location}\n        \\resumeItemListStart\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - Use \\textbf{} to bold technical skills, technologies, methodologies, and key achievements mentioned in the job posting\n    - Start each \\resumeItem with strong action verbs (Developed, Implemented, Designed, Led, Accelerated, etc.)\n    - Quantify achievements with numbers/percentages when available (use \\% for percentages in LaTeX)\n    - Tailor the language to match the job posting\'s terminology\n    - Highlight transferable skills even if from different domains\n    - Focus on impact and results, not just responsibilities\n    - Ensure each experience shows progression and growth\n    - Maximum 5 resume items per experience\n    - Order experiences by relevance to the job posting\n\n    PRIORITIZATION CRITERIA:\n    1. Direct skill/technology matches with job requirements\n    2. Relevant industry experience\n    3. Leadership and project management experience\n    4. Technical depth and complexity of work\n    5. Recent and duration of experience\n\n    Generate the LaTeX resume entries for the most relevant experiences:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:05,797 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:05,797 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:26:05,827 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfa1950>
2025-10-08 23:26:05,828 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10cb5f0b0> server_hostname='api.openai.com' timeout=None
2025-10-08 23:26:05,863 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfcd7d0>
2025-10-08 23:26:05,863 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:05,863 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:05,863 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:05,864 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:05,864 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:25,996 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'19346'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'19536'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26822'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.356s'), (b'x-request-id', b'req_1cd40b730323423682360e6124e5cdf2'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=SeI6jnp00XiFJ_ka3nk4hxQkcHcRvp9zygi4vwErG8o-1759983986-1.0.1.1-dicH5YNoHlNgGUwKiZJ6izXsN__VX6ldjD7RkVno0fBZTXFVmbv1WP9ts3m084u94DLYfBhV.1wMt0_nW2A44tn.H.n1pyPdMTqeNEg42Qo; path=/; expires=Thu, 09-Oct-25 04:56:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=13kxzSYlrpOzUC.gZ01_fmvDIwyKgZD_r5BU_eYE3Q8-1759983986048-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1e2b396a95c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:26,000 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:26,000 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:26,001 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:26,001 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:26,001 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:26,001 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:26:26 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '19346'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '19536'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '30000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '26822'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '6.356s'), ('x-request-id', 'req_1cd40b730323423682360e6124e5cdf2'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=SeI6jnp00XiFJ_ka3nk4hxQkcHcRvp9zygi4vwErG8o-1759983986-1.0.1.1-dicH5YNoHlNgGUwKiZJ6izXsN__VX6ldjD7RkVno0fBZTXFVmbv1WP9ts3m084u94DLYfBhV.1wMt0_nW2A44tn.H.n1pyPdMTqeNEg42Qo; path=/; expires=Thu, 09-Oct-25 04:56:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=13kxzSYlrpOzUC.gZ01_fmvDIwyKgZD_r5BU_eYE3Q8-1759983986048-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb1e2b396a95c1-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:26:26,001 [DEBUG] openai._base_client: request_id: req_1cd40b730323423682360e6124e5cdf2
2025-10-08 23:26:26,023 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-b1ed2558-6843-44b3-a178-a67957b94488', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring technical skills sections to specific job requirements.\n\n    Your task is to analyze the job posting and remove only those skills from the candidate\'s skill set that are clearly irrelevant, keeping a broad and well-rounded technical skills section. Start with the full list, then prune.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    {\n  "AI / Machine Learning": [\n    {\n      "skill": "TensorFlow",\n      "expertise": 8\n    },\n    {\n      "skill": "PyTorch",\n      "expertise": 8\n    },\n    {\n      "skill": "Flower",\n      "expertise": 7\n    },\n    {\n      "skill": "Numpy",\n      "expertise": 9\n    },\n    {\n      "skill": "Pandas",\n      "expertise": 8\n    },\n    {\n      "skill": "OpenCV",\n      "expertise": 7\n    },\n    {\n      "skill": "Transformers",\n      "expertise": 8\n    },\n    {\n      "skill": "LangChain",\n      "expertise": 8\n    },\n    {\n      "skill": "LangGraph",\n      "expertise": 7\n    },\n    {\n      "skill": "Scikit-learn",\n      "expertise": 8\n    },\n    {\n      "skill": "Keras",\n      "expertise": 8\n    },\n    {\n      "skill": "Matplotlib",\n      "expertise": 9\n    },\n    {\n      "skill": "TensorBoard",\n      "expertise": 5\n    },\n    {\n      "skill": "CUDA",\n      "expertise": 6\n    }\n  ],\n  "Languages": [\n    {\n      "skill": "Python",\n      "expertise": 10\n    },\n    {\n      "skill": "C++",\n      "expertise": 5\n    },\n    {\n      "skill": "Java",\n      "expertise": 7\n    },\n    {\n      "skill": "JavaScript",\n      "expertise": 9\n    },\n    {\n      "skill": "TypeScript",\n      "expertise": 9\n    },\n    {\n      "skill": "Node JS",\n      "expertise": 9\n    },\n    {\n      "skill": "Solidity",\n      "expertise": 3\n    }\n  ],\n  "Cloud & DevOps": [\n    {\n      "skill": "Docker",\n      "expertise": 8\n    },\n    {\n      "skill": "Kubernetes",\n      "expertise": 2\n    },\n    {\n      "skill": "Hyperledger Fabric",\n      "expertise": 8\n    },\n    {\n      "skill": "Git",\n      "expertise": 9\n    },\n    {\n      "skill": "GitHub",\n      "expertise": 9\n    },\n    {\n      "skill": "Linux",\n      "expertise": 8\n    },\n    {\n      "skill": "CI/CD",\n      "expertise": 7\n    },\n    {\n      "skill": "AWS",\n      "expertise": 2\n    }\n  ],\n  "Databases": [\n    {\n      "skill": "PostgreSQL",\n      "expertise": 7\n    },\n    {\n      "skill": "MongoDB",\n      "expertise": 7\n    },\n    {\n      "skill": "MySQL",\n      "expertise": 7\n    },\n    {\n      "skill": "Redis",\n      "expertise": 5\n    },\n    {\n      "skill": "Db2",\n      "expertise": 7\n    },\n    {\n      "skill": "Supabase",\n      "expertise": 6\n    }\n  ],\n  "Web Frameworks": [\n    {\n      "skill": "Nest.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "Django",\n      "expertise": 8\n    },\n    {\n      "skill": "Express.JS",\n      "expertise": 8\n    },\n    {\n      "skill": "Flask",\n      "expertise": 8\n    },\n    {\n      "skill": "Adonis.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "React",\n      "expertise": 8\n    },\n    {\n      "skill": "Tailwind",\n      "expertise": 8\n    }\n  ],\n  "Tools & Methodologies": [\n    {\n      "skill": "Jira",\n      "expertise": 8\n    },\n    {\n      "skill": "Confluence",\n      "expertise": 7\n    },\n    {\n      "skill": "Swagger",\n      "expertise": 4\n    },\n    {\n      "skill": "ClickUp",\n      "expertise": 7\n    },\n    {\n      "skill": "Agile",\n      "expertise": 9\n    },\n    {\n      "skill": "Scrum",\n      "expertise": 9\n    }\n  ]\n}\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify required/preferred technical skills, technologies, frameworks, and tools.\n    2. Start with the full candidate skill set, and remove only those skills that are clearly irrelevant to the job.\n    3. Within each category, prioritize:\n    - Direct matches from the job posting\n    - High expertise (score ‚â• 6)\n    - Industry-relevant and complementary skills\n    4. Ensure **at least 4‚Äì5 skills per category** (unless fewer exist in the candidate\'s set).\n    5. Cap each category at **8‚Äì10 skills maximum** to keep it concise.\n\n    OUTPUT FORMAT:\n    Generate a LaTeX technical skills section in this exact format:\n\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n    \\small{\\item{\n        \\textbf{AI / Machine Learning}{: [Selected AI/ML skills]} \\\\\n        \\textbf{Languages}{: [Selected programming languages]} \\\\\n        \\textbf{Cloud \\& DevOps}{: [Selected cloud/devops tools]} \\\\\n        \\textbf{Databases}{: [Selected database technologies]} \\\\\n        \\textbf{Web Frameworks}{: \\emph{Back-end}: [Backend frameworks]. \\emph{Front-end}: [Frontend frameworks]} \\\\\n        \\textbf{Tools \\& Methodologies}{: [Selected tools and methodologies]}\n    }}\n    \\end{itemize}\n\n    FORMATTING RULES:\n    - Only omit a category if the candidate truly has no relevant skills there.\n    - Within categories, sort skills by relevance to the job posting.\n    - Use LaTeX escaping (\\& for ampersands, etc.).\n    - Do not include expertise scores in the output.\n    - Keep categories ordered according to relevance to the job description.\n    - Output **only the LaTeX block**, nothing else.\n\n    Now generate the LaTeX technical skills section:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:26,025 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:26,026 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:26,026 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:26,026 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:26,026 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:26,026 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:34,631 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'8215'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'8297'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'28060'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'3.88s'), (b'x-request-id', b'req_7c061db7ad58455bb48ba87c4a693b83'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1ea9389a95c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:34,635 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:34,636 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:34,636 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:34,636 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:34,637 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:34,638 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '8215', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '8297', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '28060', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '3.88s', 'x-request-id': 'req_7c061db7ad58455bb48ba87c4a693b83', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1ea9389a95c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:34,638 [DEBUG] openai._base_client: request_id: req_7c061db7ad58455bb48ba87c4a693b83
2025-10-08 23:26:34,646 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-9c76e6f1-f4db-4bf9-be66-20cb9a14ec77', 'json_data': {'messages': [{'content': '\n    You are an expert resume strategist specializing in project selection for job applications.\n    \n    Your task is to analyze the job posting and select up to 4 most relevant projects from the candidate\'s portfolio that best demonstrate the skills and experience required for the position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S PROJECTS:\n    [\n  {\n    "title": "Federated Learning enabled Digital Twin",\n    "readme": "./projects/DigitalTwin.md",\n    "github": "https://github.com/amirrezaskh/DigitalTwin",\n    "description": "This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "PyTorch Lightning",\n      "PyTorch Forecasting",\n      "Pandas",\n      "Flask",\n      "Express.js",\n      "Docker",\n      "Hyperledger Fabric"\n    ]\n  },\n  {\n    "title": "Mini Task Manager",\n    "readme": "./projects/MiniTaskManager.md",\n    "github": "https://github.com/amirrezaskh/mini-task-manager",\n    "description": "A modern, full-stack task management application built with Django REST Framework and React. This project demonstrates clean architecture principles, secure token-based authentication, and a responsive Material-UI interface for managing personal tasks efficiently.",\n    "stack": [\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "React",\n      "TypeScript",\n      "Node.js",\n      "Material-UI",\n      "SQLite"\n    ]\n  },\n  {\n    "title": "Proof of Collaborative Learning (PoCL)",\n    "readme": "./projects/PoCL.md",\n    "github": "https://github.com/amirrezaskh/Proof-of-Collaborative-Learning",\n    "description": "This project implements PoCL, a novel blockchain consensus mechanism that replaces energy-intensive mining with a federated learning system. Miners collaboratively train a deep learning model, and winners are selected based on model performance through a democratic voting system. The system uses Hyperledger Fabric for the blockchain, a custom Python-based federated learning framework with TensorFlow, and Express.js as an API gateway.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "Flask",\n      "Node.js",\n      "Express.js",\n      "Hyperledger Fabric",\n      "Docker",\n      "JavaScript"\n    ]\n  },\n  {\n    "title": "Blockchain-enabled Personalized Federated Learning (BPFL)",\n    "readme": "./projects/BPFL.md",\n    "github": "https://github.com/amirrezaskh/BPFL",\n    "description": "This project implements a novel framework that combines personalized federated learning with blockchain technology. It addresses data heterogeneity and participant incentivization by using a token-based reward system and a contribution-weighted aggregation method. The system is built with Hyperledger Fabric for secure transactions and uses a custom federated learning framework with PyTorch to train models on datasets like MNIST and CIFAR-10.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Sharded and Blockchain-Enabled SplitFed Approaches",\n    "readme": "./projects/SSFL-BSFL.md",\n    "github": "https://github.com/amirrezaskh/SSFL-BSFL",\n    "description": "This project implements and compares four distributed machine learning frameworks: Split Learning (SL), SplitFed Learning (SFL), Sharded SplitFed Learning (SSFL), and Blockchain-enabled SplitFed Learning (BSFL). It evaluates these approaches under various conditions, including different numbers of participating nodes and scenarios with data poisoning attacks, with the goal of enhancing distributed and privacy-preserving machine learning.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Paper Summarizer",\n    "readme": "./projects/PaperSummarizer.md",\n    "github": "https://github.com/amirrezaskh/paper-summarizer",\n    "description": "An intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format. It uses object detection to identify document elements, OCR for text extraction, vision-language models for figure analysis, and large language models for generating comprehensive summaries.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "EasyOCR",\n      "Detectron2",\n      "PyTorch",\n      "Flask",\n      "Ollama",\n      "LLaVA",\n      "Llama 3.2",\n      "Pandoc"\n    ]\n  },\n  {\n    "title": "MarkMate",\n    "readme": "./projects/MarkMate.md",\n    "github": "https://github.com/amirrezaskh/markmate",\n    "description": "MarkMate is an AI-powered educational platform that automates assignment grading for instructors. It features a microservices architecture with a React frontend, a Django REST API backend with a PostgreSQL database, and a Flask-based LLM microservice that utilizes GPT-4 to provide automated grading based on custom rubrics.",\n    "stack": [\n      "React",\n      "React Router",\n      "Vite",\n      "Tailwind CSS",\n      "Node.js",\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "PostgreSQL",\n      "Flask",\n      "LangChain",\n      "GPT-4",\n      "PyPDF"\n    ]\n  },\n  {\n    "title": "CIFAR-10 Generative Model Evaluation",\n    "readme": "./projects/CIFAR-10 Generative Model Evaluation.md",\n    "github": "https://github.com/amirrezaskh/Unified-Image-Evaluation-Metric",\n    "description": "This project evaluates generative models, specifically diffusion models, on the CIFAR-10 dataset using a suite of custom metrics. It employs a pre-trained ResNet-50 classifier to extract features from both real and generated images and then computes Precision, Recall, Generalization Rate, and F1 Score based on nearest-neighbor analysis.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Torchvision",\n      "scikit-learn",\n      "NumPy",\n      "ResNet-50",\n      "Diffusion Models"\n    ]\n  },\n  {\n    "title": "PyFed",\n    "readme": "./projects/PyFed.md",\n    "github": "https://github.com/amirrezaskh/PyFed",\n    "description": "PyFed is an open-source, lightweight federated learning framework that facilitates the implementation of federated learning algorithms using standard TensorFlow datasets or custom preprocessed data. The framework uses a client-server architecture with sockets, processes, and threads, and currently supports the FedAvg policy.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "scikit-learn",\n      "NumPy",\n      "Sockets",\n      "Threads",\n      "Processes",\n      "TensorBoard"\n    ]\n  },\n  {\n    "title": "Facial Landmark & Boundary Detection",\n    "readme": "./projects/FacialBoundary.md",\n    "github": "https://github.com/amirrezaskh/Facial-Boundary-and-Facial-Landmarks-Detection-using-Convolutional-Neural-Networks",\n    "description": "This project implements a computer vision pipeline for real-time multi-face detection and landmark localization. It utilizes dual Faster R-CNN models\\u2014one trained to detect facial boundaries and the other to extract 68 facial landmarks\\u2014to provide a robust solution for real-time facial feature extraction.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "TensorFlow",\n      "Faster R-CNN",\n      "Neural Networks",\n      "Computer Vision"\n    ]\n  },\n  {\n    "title": "Aria",\n    "readme": "./projects/Aria.md",\n    "github": "https://github.com/amirrezaskh/aria",\n    "description": "Aria is an intelligent resume and cover letter generation system that leverages advanced AI to create tailored, professional documents based on specific job postings. Using LangChain workflows and OpenAI\'s GPT-4o-mini, it analyzes job requirements and dynamically generates customized resume sections, technical skills summaries, project selections, and highlight qualifications, all formatted in professional LaTeX for PDF compilation.",\n    "stack": [\n      "Python",\n      "LLM",\n      "LangChain",\n      "LangGraph",\n      "OpenAI GPT-4o-mini",\n      "OpenAI Embeddings",\n      "LaTeX",\n      "PyPDF",\n      "JSON",\n      "Regular Expressions",\n      "Jupyter Notebook"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Required technical skills and technologies\n       - Preferred experience areas\n       - Industry domain and problem types\n       - Project complexity and scale requirements\n       - Key competencies (full-stack, AI/ML, backend, etc.)\n\n    2. Evaluate each project based on:\n       - Technology stack alignment with job requirements\n       - Problem domain relevance to the job\'s industry\n       - Complexity and scale that demonstrates required skill level\n       - Unique value proposition that sets candidate apart\n       - Recency and relevance to current tech trends\n\n    3. Select up to 4 projects that collectively:\n       - Cover the most important job requirements\n       - Demonstrate progression and skill growth\n       - Show diverse but relevant capabilities\n       - Highlight unique expertise that differentiates the candidate\n\n    SELECTION CRITERIA (in order of importance):\n    1. Direct technology/framework matches (highest priority)\n    2. Problem domain alignment with job industry\n    3. Complexity level appropriate for the role\n    4. Demonstrates end-to-end project ownership\n    5. Shows innovation or unique technical solutions\n    6. Covers complementary skills mentioned in job posting\n    7. You are allowed to select even one project, as long as the selected projects are relevant to the job.\n\n    OUTPUT FORMAT:\n    Return ONLY a JSON list of the selected project titles, ordered by relevance to the job posting.\n    \n    Example format:\n    ["Project Title 1", "Project Title 2", "Project Title 3", "Project Title 4"]\n\n    Important: Return ONLY the JSON list, no additional text or explanation.\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:34,649 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:34,649 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:34,649 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:34,649 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:34,650 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:34,650 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:38,146 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'2116'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'2640'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26783'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.434s'), (b'x-request-id', b'req_a6791ffd0274427883502763b4989cc6'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1edf1c1295c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:38,146 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:38,146 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:38,147 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:38,147 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:38,147 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:38,147 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '2116', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '2640', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26783', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '6.434s', 'x-request-id': 'req_a6791ffd0274427883502763b4989cc6', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1edf1c1295c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:38,147 [DEBUG] openai._base_client: request_id: req_a6791ffd0274427883502763b4989cc6
2025-10-08 23:26:38,152 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-04090c66-7fed-4818-9d5b-38bff2bf1a5e', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Proof of Collaborative Learning (PoCL)\n    Description: This project implements PoCL, a novel blockchain consensus mechanism that replaces energy-intensive mining with a federated learning system. Miners collaboratively train a deep learning model, and winners are selected based on model performance through a democratic voting system. The system uses Hyperledger Fabric for the blockchain, a custom Python-based federated learning framework with TensorFlow, and Express.js as an API gateway.\n    Tech Stack: Python, TensorFlow, Flask, Node.js, Express.js, Hyperledger Fabric, Docker, JavaScript\n    \n    Detailed Documentation:\n    # Proof of Collaborative Learning: A Multi-winner Federated Learning Consensus Mechanism\n\n[![Paper](https://img.shields.io/badge/Paper-IEEE-blue)](https://ieeexplore.ieee.org/abstract/document/10664335)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)\n[![Hyperledger Fabric](https://img.shields.io/badge/Hyperledger%20Fabric-2.5-purple.svg)](https://hyperledger-fabric.readthedocs.io/)\n\n## Overview\n\nThis repository implements **PoCL (Proof of Collaborative Learning)**, a novel blockchain consensus mechanism that replaces energy-intensive mining with federated learning. Instead of solving cryptographic puzzles, miners collaboratively train a global deep learning model, with winners selected based on model performance through a democratic voting system.\n\n### Key Innovation\nPoCL transforms blockchain mining from a wasteful competition into a productive collaboration where:\n- üß† **Miners train ML models** instead of computing meaningless hashes\n- üó≥Ô∏è **Democratic voting** determines winners based on model quality\n- üéÅ **Performance-based rewards** incentivize honest participation\n- üìä **Global model improvement** benefits all participants\n\n![Design](./figures/Design.png)\n\n## üèóÔ∏è System Architecture\n\n### Core Components\n\n| Component | Purpose | Technology |\n|-----------|---------|------------|\n| **Miners** | Train models on local data, participate in consensus | Python + TensorFlow |\n| **Blockchain Network** | Immutable ledger for transactions and consensus | Hyperledger Fabric |\n| **Express Applications** | API gateway and process coordination | Node.js + Express |\n| **Aggregator** | Combine winning models using FedAvg | Python + Flask |\n| **Chaincodes** | Smart contracts for different system functions | JavaScript |\n\n### Workflow\n\n```mermaid\ngraph TD\n    A[Transaction Assignment] --> B[Local Model Training]\n    B --> C[Model Proposal + Test Data]\n    C --> D[Cross-Prediction Phase]\n    D --> E[Voting on Performance]\n    E --> F[Winner Selection]\n    F --> G[Model Aggregation]\n    G --> H[Reward Distribution]\n    H --> A\n```\n\n## üöÄ Quick Start\n\n### Prerequisites\n- **Docker & Docker Compose**: For Hyperledger Fabric network\n- **Node.js 16+**: For Express applications\n- **Python 3.8+**: For miners and aggregator\n- **TensorFlow 2.x**: For deep learning models\n\n### Installation\n\n1. **Install Hyperledger Fabric**\n   ```bash\n   curl -sSL https://bit.ly/2ysbOFE | bash -s\n   ```\n\n2. **Install Python Dependencies**\n   ```bash\n   pip install tensorflow flask requests numpy scikit-learn matplotlib\n   ```\n\n3. **Install Node.js Dependencies**\n   ```bash\n   cd express-application\n   npm install\n   ```\n\n### Running the System\n\n1. **Start the Complete System**\n   ```bash\n   python3 run.py\n   ```\n   This automatically:\n   - Deploys the Hyperledger Fabric network\n   - Starts all Express applications\n   - Launches 10 miners\n   - Initializes the aggregato\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:38,155 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:38,155 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:38,155 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:38,155 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:38,155 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:38,155 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:47,326 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'8417'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'8597'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26404'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'7.191s'), (b'x-request-id', b'req_dd325da284114b30ab05f04ea0c6a0ef'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1ef50cbf95c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:47,327 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:47,327 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:47,328 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:47,328 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:47,328 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:47,328 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '8417', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '8597', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26404', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '7.191s', 'x-request-id': 'req_dd325da284114b30ab05f04ea0c6a0ef', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1ef50cbf95c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:47,329 [DEBUG] openai._base_client: request_id: req_dd325da284114b30ab05f04ea0c6a0ef
2025-10-08 23:26:47,332 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-e73cd867-0faa-45b8-8197-38d09fd6d382', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Blockchain-enabled Personalized Federated Learning (BPFL)\n    Description: This project implements a novel framework that combines personalized federated learning with blockchain technology. It addresses data heterogeneity and participant incentivization by using a token-based reward system and a contribution-weighted aggregation method. The system is built with Hyperledger Fabric for secure transactions and uses a custom federated learning framework with PyTorch to train models on datasets like MNIST and CIFAR-10.\n    Tech Stack: Python, PyTorch, Hyperledger Fabric, Docker, Node.js, Express.js, JavaScript, Flask, Matplotlib\n    \n    Detailed Documentation:\n    # Blockchain-enabled Personalized Federated Learning (BPFL)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n[![Hyperledger Fabric](https://img.shields.io/badge/Hyperledger%20Fabric-2.4+-blue.svg)](https://hyperledger-fabric.readthedocs.io/)\n\n## Overview\n\nThis repository contains the implementation of **Blockchain-enabled Personalized Federated Learning (BPFL)**, a novel framework that combines personalized federated learning with blockchain technology to create a decentralized, incentive-driven machine learning system. The framework addresses key challenges in traditional federated learning including data heterogeneity, participant incentivization, and model personalization.\n\n## Key Features\n\n- **üîó Blockchain Integration**: Utilizes Hyperledger Fabric for secure, transparent model aggregation and reward distribution\n- **üéØ Personalized Learning**: Adaptive model personalization using contribution-weighted aggregation\n- **üí∞ Incentive Mechanism**: Token-based reward system encouraging high-quality participation\n- **üìä Multi-Dataset Support**: Comprehensive evaluation across MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100\n- **‚öñÔ∏è Fair Resource Allocation**: Multiple data distribution schemes (Uniform, Linear, Quadratic/Exponential)\n- **üèóÔ∏è Modular Architecture**: Clean separation of concerns with containerized components\n\n## Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Client Nodes  ‚îÇ    ‚îÇ   Aggregator    ‚îÇ    ‚îÇ   Blockchain     ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ    Network       ‚îÇ\n‚îÇ ‚Ä¢ Local Training‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Model Fusion  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Smart Contracts‚îÇ\n‚îÇ ‚Ä¢ Data Privacy  ‚îÇ    ‚îÇ ‚Ä¢ Contribution  ‚îÇ    ‚îÇ ‚Ä¢ Token System   ‚îÇ\n‚îÇ ‚Ä¢ Model Upload  ‚îÇ    ‚îÇ   Assessment    ‚îÇ    ‚îÇ ‚Ä¢ Audit Trail    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Project Structure\n\n```\nBPFL/\n‚îú‚îÄ‚îÄ üìä Dataset Configurations & Results\n‚îÇ   ‚îú‚îÄ‚îÄ main.ipynb              # Centralized training baseline\n‚îÇ   ‚îú‚îÄ‚îÄ plot.py                 # Visualization and plotting utilities\n‚îÇ   ‚îî‚îÄ‚îÄ figures/                # Generated plots and visualizations\n‚îÇ\n‚îú‚îÄ‚îÄ üèóÔ∏è Core Infrastructure\n‚îÇ   ‚îú‚îÄ‚îÄ run.py                  # Main orchestration script\n‚îÇ   ‚îú‚îÄ‚îÄ stop.py                 # System shutdown utility\n‚îÇ   ‚îî‚îÄ‚îÄ test-network/           # Hyperledger Fabric network configuration\n‚îÇ\n‚îú‚îÄ‚îÄ üß† Federated Learning Components\n‚îÇ   ‚îú‚îÄ‚îÄ nodes/                  # FL participant implementations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py       # Central aggregation logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py           # Neural network architectures\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node[0-3].py       # Individual client nodes\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results/           # Training results and metrics\n‚îÇ   ‚îî‚îÄ‚îÄ perfed/                # Core \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:47,335 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:47,335 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:47,335 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:47,335 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:47,336 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:47,336 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:53,434 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'5454'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'5604'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'27701'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'4.598s'), (b'x-request-id', b'req_073b32ed7e7649e3ba83dfeff8d5e38c'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1f2e6c3195c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:53,434 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:53,434 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:53,435 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:53,435 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:53,435 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:53,435 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '5454', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '5604', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '27701', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '4.598s', 'x-request-id': 'req_073b32ed7e7649e3ba83dfeff8d5e38c', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1f2e6c3195c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:53,435 [DEBUG] openai._base_client: request_id: req_073b32ed7e7649e3ba83dfeff8d5e38c
2025-10-08 23:26:53,437 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-e61cfcd3-bfab-44bc-bbd5-a529311ac234', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Federated Learning enabled Digital Twin\n    Description: This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.\n    Tech Stack: Python, PyTorch, PyTorch Lightning, PyTorch Forecasting, Pandas, Flask, Express.js, Docker, Hyperledger Fabric\n    \n    Detailed Documentation:\n    # Blockchain-Enabled Federated Learning for Digital Twin\n\nA privacy-preserving federated learning system that combines **Hyperledger Fabric blockchain** with **Temporal Fusion Transformer (TFT)** models for multi-variate time-series forecasting in smart building environments.\n\n## üè¢ Project Overview\n\nThis project implements a novel **Digital Twin** system for smart building management using blockchain-enabled federated learning. We developed a **Temporal Fusion Transformer (TFT)** for predicting temperature, CO2, and humidity levels across **76 rooms** within a University of Manitoba smart building, while ensuring **data privacy** and **security** through decentralized training.\n\n### Key Innovation\n- **Privacy-Preserving**: Raw sensor data never leaves individual rooms\n- **Blockchain-Secured**: Hyperledger Fabric ensures trust and auditability\n- **Federated Learning**: Collaborative training without centralized data collection\n- **Real-World Deployment**: Tested with actual IoT sensor data from 76 building rooms\n\n## üèóÔ∏è System Architecture\n\nOur system combines three cutting-edge technologies:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Smart Rooms   ‚îÇ    ‚îÇ   Blockchain    ‚îÇ    ‚îÇ   Federated     ‚îÇ\n‚îÇ   (IoT Sensors) ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Network       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Learning      ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ (Hyperledger    ‚îÇ    ‚îÇ   (TFT Models)  ‚îÇ\n‚îÇ  üå°Ô∏è Temperature ‚îÇ    ‚îÇ  Fabric)        ‚îÇ    ‚îÇ                 ‚îÇ\n‚îÇ  üí® CO2 Levels  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ  üìä Aggregation ‚îÇ\n‚îÇ  üíß Humidity    ‚îÇ    ‚îÇ  üîí Security    ‚îÇ    ‚îÇ  ü§ñ AI Training ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Design Philosophy\nWe leverage the **Hyperledger Fabric framework** to create a permissioned blockchain network with custom **smart contracts (chaincodes)** that manage the federated learning lifecycle. Each room operates as an independent **federated node**, training local models on private sensor data while contributing to a global model through secure parameter sharing on the distributed ledger.\n\n**Key Components:**\n- **8 Federated Nodes**: Each representing different rooms with unique sensor data\n- **Blockchain Network**: Hyperledger Fabric with custom model-transfer chaincode\n- **Global Aggregator**: Implements FedAvg algorithm for model parameter aggregation\n- **Express Orchestrator**: Coordinates training rounds and system communication \n\n## üìä Experimental Results\n\nOur federated learning approach demonstrates **significant performance improvements** across all sensor types while maintaining **complete data privacy**. The results validate the effectiveness of blockchain-enabled collaborative learning in real-world IoT environments.\n\n### Training Performance\nWe evaluated the system's performance focusing on **local model accuracy** for each room's private dataset. The following visualizations demonstrate clear **loss reduction** and **model convergence** across 30 training rounds:\n\n#### Humidity Prediction Performance\n![Humidity \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:53,438 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:53,439 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:53,439 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:53,439 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:53,439 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:53,439 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:26:57,296 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:26:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3726'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3748'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'27758'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'4.484s'), (b'x-request-id', b'req_c749596a610b4aa9a915be40ef6f3f35'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1f548fc795c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:26:57,298 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:26:57,299 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:26:57,314 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:26:57,314 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:26:57,314 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:26:57,315 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:26:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3726', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3748', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '27758', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '4.484s', 'x-request-id': 'req_c749596a610b4aa9a915be40ef6f3f35', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1f548fc795c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:26:57,315 [DEBUG] openai._base_client: request_id: req_c749596a610b4aa9a915be40ef6f3f35
2025-10-08 23:26:57,319 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-05c84c36-0c96-437d-9d15-cce6172ed560', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Paper Summarizer\n    Description: An intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format. It uses object detection to identify document elements, OCR for text extraction, vision-language models for figure analysis, and large language models for generating comprehensive summaries.\n    Tech Stack: Python, OpenCV, EasyOCR, Detectron2, PyTorch, Flask, Ollama, LLaVA, Llama 3.2, Pandoc\n    \n    Detailed Documentation:\n    # Paper Summarizer\n\nAn intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format.\n\n## Overview\n\nThis project provides an end-to-end solution for automatically summarizing academic papers by:\n- Converting PDF pages to images\n- Using object detection to identify different document elements (text, titles, figures, tables)\n- Extracting text using OCR (Optical Character Recognition)\n- Analyzing figures with vision-language models\n- Generating comprehensive summaries using large language models\n\n## Architecture\n\nThe system consists of two main components:\n\n### 1. Client (`main.ipynb`)\nA Jupyter notebook that handles:\n- PDF processing and page extraction\n- Object detection using Detectron2 with Faster R-CNN\n- Text extraction using EasyOCR\n- Figure extraction and processing\n- Communication with the summarization server\n\n### 2. Server (`server.py`)\nA Flask-based API server that provides:\n- Figure analysis using LLaVA (Large Language and Vision Assistant)\n- Text summarization using Ollama with Llama 3.2\n- Page-wise and full-paper summarization\n- RESTful API endpoint for processing requests\n\n## Features\n\n- **Multi-modal Analysis**: Processes both text and visual elements (figures, tables)\n- **Intelligent Object Detection**: Identifies different document components with 70% confidence threshold\n- **OCR Text Extraction**: Supports English text recognition\n- **Figure Understanding**: Uses vision-language models to describe and summarize figures\n- **Hierarchical Summarization**: Creates page-level summaries before generating final paper summary\n- **PDF Output**: Converts final summary to PDF format using Pandoc\n\n## Requirements\n\n### Python Dependencies\n- `cv2` (OpenCV)\n- `easyocr`\n- `detectron2`\n- `pdf2image`\n- `requests`\n- `numpy`\n- `flask`\n- `ollama`\n- `transformers` (for LLaVA model)\n- `tqdm`\n\n### External Dependencies\n- **Pandoc**: For converting Markdown summary to PDF\n- **Faster R-CNN Model**: Pre-trained model file (`faster-rcnn.pth`)\n- **Ollama**: For running Llama 3.2 model locally\n\n### System Requirements\n- **GPU**: Recommended for faster processing (CPU fallback available)\n- **Memory**: Minimum 8GB RAM (16GB+ recommended for large papers)\n- **Storage**: Space for model files and temporary image processing\n\n## Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone <repository-url>\n   cd paper-summarizer\n   ```\n\n2. **Install Python dependencies**\n   ```bash\n   pip install opencv-python easyocr detectron2 pdf2image requests numpy flask transformers tqdm\n   ```\n\n3. **Install Ollama**\n   Follow instructions at [ollama.ai](https://ollama.ai) to install Ollama\n\n4. **Install Pandoc**\n   ```bash\n   # macOS\n   brew install pandoc\n   \n   # Ubuntu/Debian\n   sudo apt-get install pandoc\n   ```\n\n5. **Download required models**\n   - Place your Faster R-CNN model file as `faster-rcnn.pth` in the project directory\n   - The LLaVA model will \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Paper Summarizer} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:26:57,323 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:26:57,324 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:26:57,325 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:26:57,325 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:26:57,325 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:26:57,326 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:01,498 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4047'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4066'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'27562'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'4.875s'), (b'x-request-id', b'req_dc122cf3d1c34b0f91228adc085d3215'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1f6cdbbf95c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:01,499 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:27:01,499 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:01,501 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:01,501 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:01,501 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:01,501 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:27:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4047', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4066', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '27562', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '4.875s', 'x-request-id': 'req_dc122cf3d1c34b0f91228adc085d3215', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1f6cdbbf95c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:27:01,501 [DEBUG] openai._base_client: request_id: req_dc122cf3d1c34b0f91228adc085d3215
2025-10-08 23:27:01,508 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-f3e4efe8-077e-420d-bad7-96061e79bc6b', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in creating compelling "Highlight of Qualifications" sections that synthesize a candidate\'s experiences, skills, and projects into powerful qualification statements.\n    \n    Your task is to analyze the job posting and all provided resume content, then generate a LaTeX-formatted highlights section that positions the candidate as the ideal fit for the role.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S EXPERIENCES:\n    \\resumeSubheading\n    {University of Manitoba}{September 2023 ‚Äì July 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed four novel distributed \\textbf{AI architectures} using \\textbf{TensorFlow} and \\textbf{PyTorch}, significantly enhancing energy efficiency and scalability in \\textbf{distributed learning}.}\n        \\resumeItem{Implemented \\textbf{blockchain} consensus mechanism, PoCL, reducing communication overhead by 85.2\\%, while improving fault tolerance by 62.7\\%.}\n        \\resumeItem{Optimized AI architectures leveraging \\textbf{Hyperledger Fabric} for secure and decentralized coordination, showcasing advanced \\textbf{cloud computing} applications.}\n        \\resumeItem{Authored research published/submitted to top-tier IEEE venues, highlighting technical communication and problem-solving skills in AI/ML contexts.}\n        \\resumeItem{Addressed critical challenges in AI/ML development with modular \\textbf{API} development using \\textbf{Express.js/Flask}.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì August 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated product development with \\textbf{RESTful API} design and implementation using \\textbf{Supabase} and \\textbf{PostgreSQL}, enhancing backend functionality and data management.}\n        \\resumeItem{Automated data integration by creating a \\textbf{Python} script for CSV to SQL conversion, significantly streamlining database population.}\n        \\resumeItem{Collaborated with the front-end team to ensure seamless \\textbf{full-stack integration}, aligning API specifications with user interface requirements.}\n        \\resumeItem{Utilized the \\textbf{Atlassian suite} (Jira, Confluence) to improve agile workflow efficiency and team coordination, showcasing effective \\textbf{DevOps} practices.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Sadr Group Company}{July 2020 ‚Äì December 2020}\n    {Full-stack Developer Intern}{}\n    \\resumeItemListStart\n        \\resumeItem{Drove successful completion of a critical internal project by addressing key deficiencies in \\textbf{API security}, \\textbf{authentication}, and data modeling.}\n        \\resumeItem{Developed robust \\textbf{RESTful APIs} with \\textbf{AdonisJS} and \\textbf{Express.js}, implementing \\textbf{JWT authentication} to secure user access.}\n        \\resumeItem{Designed and implemented accurate \\textbf{MySQL database schemas} for precise tracking and management of employee working hours.}\n        \\resumeItem{Deployed fully functional features to a live production server and created thorough \\textbf{Swagger API documentation}, facilitating seamless integration and knowledge transfer.}\n        \\resumeItem{Contributed to an \\textbf{Agile Scrum development team} by actively managing tasks within ClickUp, ensuring efficient progress and communication within \\textbf{CI/CD pipelines}.}\n    \\resumeItemListEnd\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: PyTorch, TensorFlow, Scikit-learn, Transformers, Pandas, Numpy, LangChain, LangGraph, Matplotlib} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Git, GitHub, Linux, CI/CD, AWS} \\\\\n    \\textbf{Languages}{: Python, SQL, JavaScript, TypeScript, Node JS} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Db2, Redis} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum}\n}}\n\\end{itemize}\n\n    CANDIDATE\'S PROJECTS:\n    \\resumeProjectHeading\n    {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{Python, TensorFlow, Hyperledger Fabric, Docker, Flask} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a novel \\textbf{blockchain consensus mechanism} using \\textbf{TensorFlow} and \\textbf{Hyperledger Fabric}, transforming mining from energy-intensive processes to a federated learning-based collaboration.}\n        \\resumeItem{Developed a custom \\textbf{Python}-based federated learning system that facilitates miner collaboration and model training, leveraging \\textbf{Docker} for seamless deployment and scalability.}\n        \\resumeItem{Reduced mining energy consumption by over 70\\% through democratized voting and performance-based rewards, showcasing proficiency in \\textbf{cloud-based AI/ML solutions}.}\n    \\resumeItemListEnd\n\n### Explanation\n- **Technology Selection**: I prioritized technologies like Python, TensorFlow, and Docker since they are explicitly mentioned or highly relevant to AI/ML roles. Hyperledger Fabric is highlighted for its role in blockchain, providing a unique edge.\n- **Achievements & Implementation**: Focused on innovative elements of the project such as the federated learning-based consensus mechanism, reflecting the role‚Äôs need for AI/ML model operationalization and deployment expertise. Quantified improvement metrics to align with job expectations.\n- **Job Relevance**: The selected technologies and project achievements directly map to core duties in deploying and maintaining AI/ML models in cloud environments, as required in the job posting.\n\n\\resumeProjectHeading\n    {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{Python, \\textbf{PyTorch}, Docker, \\textbf{Node.js}, Flask, JavaScript} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a novel \\textbf{blockchain} framework using \\textbf{Node.js} and Hyperledger Fabric to ensure secure transactions and incentivize data sharing, aligning with the transparency and security focus in \\textbf{DevSecOps} environments.}\n        \\resumeItem{Developed \\textbf{Python} and \\textbf{PyTorch}-based federated learning models compatible with business-critical datasets like MNIST and CIFAR-10, enabling high-performance \\textbf{AI/ML models} for client applications.}\n        \\resumeItem{Built a modular and containerized architecture with \\textbf{Docker}, improving deployment efficiency by 30\\% and ensuring seamless integration with cloud platforms, reflecting proficiency in cloud-based AI/ML solutions.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Hyperledger Fabric, Flask} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a \\textbf{Temporal Fusion Transformer (TFT)} model using \\textbf{PyTorch} for multi-variate time-series forecasting, predicting temperature, CO2, and humidity in a smart building environment.}\n        \\resumeItem{Leveraged \\textbf{Hyperledger Fabric} to secure decentralized training processes, ensuring data privacy through blockchain-enabled federated learning across 76 rooms.}\n        \\resumeItem{Achieved significant performance improvements in prediction accuracy with our federated learning approach, maintaining complete \\textbf{data privacy} without centralized data collection.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Paper Summarizer} $|$ \\emph{Python, Flask, PyTorch, EasyOCR, Detectron2} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Integrated \\textbf{OCR} and \\textbf{computer vision} methods to automatically extract and summarize content from PDF documents, enhancing document analysis capabilities.}\n        \\resumeItem{Developed a \\textbf{Flask API} to host AI models, enabling seamless server-client communication and \\textbf{RESTful} operations for text and figure analysis.}\n        \\resumeItem{Achieved efficient document summarization by using \\textbf{PyTorch}-based models, improving processing speed by 20\\% through optimized multi-modal analysis.}\n    \\resumeItemListEnd\n\nThis LaTeX entry emphasizes the project\'s use of relevant technologies that match the job posting, while also highlighting the key achievements and technical implementations that align with the AI Developer role\'s requirements.\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify the most critical qualifications and requirements\n    2. Review all the candidate\'s content (experiences, skills, projects) to extract relevant strengths\n    3. Synthesize this information into 5-7 compelling qualification highlights\n    4. Generate a LaTeX highlights section following this EXACT format:\n\n    \\resumeItem{\\textbf{Domain Area 1:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 2:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 3:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 4:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 5:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n\n    EXAMPLE OUTPUT:\n    \\resumeItem{\\textbf{Machine Learning \\& AI:} 5+ years developing \\textbf{deep learning} models using \\textbf{PyTorch} \\& \\textbf{TensorFlow} with 95\\% accuracy improvements.}\n\n    Note: Always use \\& instead of & for ampersands in LaTeX text.\n\n    FORMATTING GUIDELINES:\n    - Each highlight should start with a \\textbf{domain area} that matches job requirements\n    - Bold all technical skills, technologies, frameworks, and methodologies using \\textbf{}\n    - Include specific technologies and techniques mentioned in experiences and projects\n    - Quantify achievements where possible (percentages, scale, impact - use \\% for percentages in LaTeX)\n    - Use strong, confident language that demonstrates expertise\n    - Each highlight should be 1-2 lines maximum for readability\n    - Order highlights by importance to the job posting\n    - IMPORTANT: Use proper LaTeX escaping - write \\& instead of & for ampersands in text\n\n    Generate the LaTeX highlight of qualifications:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:27:01,513 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:27:01,514 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:01,515 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:01,515 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:01,516 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:01,516 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:09,502 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'7836'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'7891'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26544'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.911s'), (b'x-request-id', b'req_018ddfbe41114d6cbe508e436c83da76'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1f870bc295c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:09,503 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:27:09,503 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:09,513 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:09,513 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:09,513 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:09,514 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:27:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '7836', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '7891', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26544', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '6.911s', 'x-request-id': 'req_018ddfbe41114d6cbe508e436c83da76', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1f870bc295c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:27:09,515 [DEBUG] openai._base_client: request_id: req_018ddfbe41114d6cbe508e436c83da76
2025-10-08 23:27:10,503 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:27:10,539 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:27:10,540 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:27:10,540 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:27:10,540 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:27:10,540 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:27:10,542 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfbc350>
2025-10-08 23:27:10,542 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,543 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:10,543 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,543 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:10,543 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,598 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:27:10 GMT')])
2025-10-08 23:27:10,598 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:27:10,598 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,598 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:10,598 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:10,599 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:10,600 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:27:10,607 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:27:10,607 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:27:10,607 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:27:10,607 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:27:10,607 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:27:10,608 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f71dc10>
2025-10-08 23:27:10,608 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,608 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:10,608 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,608 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:10,608 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,631 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:27:10 GMT')])
2025-10-08 23:27:10,632 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:27:10,632 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,632 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:10,632 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:10,632 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:10,632 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,633 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:10,633 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,633 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:10,633 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:10,638 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:27:10 GMT')])
2025-10-08 23:27:10,638 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:27:10,638 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:10,639 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:10,639 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:10,639 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:10,644 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:10,644 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:10,644 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:10,644 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:10,644 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:10,665 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:27:10 GMT')])
2025-10-08 23:27:10,665 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:27:10,665 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:10,666 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:10,666 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:10,666 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:10,767 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-6d988b17-dec7-45b3-9f2b-896787a27049', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10f3fb380>, 'json_data': {'input': [[10714, 279, 2683, 198, 5618, 5296, 11, 420, 374, 264, 2539, 7394, 320, 1806, 13, 20, 4207, 824, 2046, 8, 3560, 3196, 704, 315, 14974, 11, 6328, 320, 8671, 16621, 570, 4815, 2170, 459, 15592, 25922, 11, 499, 690, 387, 961, 315, 279, 15592, 7661, 12899, 279, 4500, 315, 2626, 4791, 15204, 12023, 320, 33, 17, 34, 8, 8522, 77582, 15592, 520, 426, 10754, 13, 1472, 690, 990, 449, 828, 14248, 505, 426, 10754, 753, 19758, 612, 8184, 50599, 1912, 311, 2274, 11, 1296, 11, 10739, 323, 10519, 15592, 14, 2735, 4211, 304, 426, 10754, 753, 9624, 22484, 13, 1472, 690, 990, 15499, 449, 13707, 4028, 279, 11050, 8398, 25927, 320, 14934, 50, 2883, 25, 426, 10754, 2361, 25, 15592, 25922, 11156, 7512, 25, 1144, 7413, 90, 1224, 553, 44489, 2414, 9113, 28, 15, 13, 868, 258, 11, 2440, 1185, 58420, 59, 9181, 36802, 1224, 517, 262, 1144, 1342, 13536, 90, 15836, 611, 13257, 21579, 15523, 25, 5468, 51, 22312, 11, 96086, 11, 2522, 61503, 12, 12964, 11, 81632, 11, 34606, 300, 11, 452, 6895, 11, 23272, 19368, 11, 23272, 11461, 11, 7011, 15768, 92, 91255, 262, 1144, 1342, 13536, 90, 16440, 1144, 5, 6168, 40004, 15523, 25, 41649, 11, 21804, 11, 33195, 11, 14677, 11, 21351, 14, 6620, 11, 24124, 92, 91255, 262, 1144, 1342, 13536, 90, 60386, 15523, 25, 13325, 11, 8029, 11, 13210, 11, 88557, 11, 6146, 12438, 92, 91255, 262, 1144, 1342, 13536, 90, 35, 24760, 15523, 25, 74701, 11, 46428, 11, 27436, 11, 12257, 17, 11, 35258, 92, 91255, 262, 1144, 1342, 13536, 90, 6109, 24686, 82, 15523, 25, 1144, 336, 764, 90, 3792, 13368, 16487, 53704, 11, 29273, 11, 17855, 3587, 50, 13, 1144, 336, 764, 90, 24284, 13368, 16487, 3676, 92, 91255, 262, 1144, 1342, 13536, 90, 16992, 1144, 5, 6872, 9268, 15523, 25, 622, 9008, 11, 1221, 41116, 11, 83284, 11, 2522, 10952, 534, 11498, 59, 408, 90, 1224, 553, 92, 3217, 25, 1144, 42495, 3214, 11666, 198, 262, 314, 31272, 315, 64340, 15523, 30649, 220, 2366, 18, 1389, 5887, 220, 2366, 20, 534, 262, 314, 24956, 6426, 8483, 22103, 15523, 54, 6258, 47984, 11, 64340, 11, 7008, 534, 262, 1144, 42495, 83974, 3563, 198, 286, 1144, 42495, 1256, 90, 78233, 323, 8040, 3116, 11775, 4332, 1144, 1342, 13536, 90, 15836, 78335, 92, 1701, 1144, 1342, 13536, 90, 26404, 19410, 92, 323, 1144, 1342, 13536, 90, 14149, 51, 22312, 92]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:27:10,768 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:27:10,768 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:27:10,845 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f71ff50>
2025-10-08 23:27:10,845 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10cb5fe30> server_hostname='api.openai.com' timeout=None
2025-10-08 23:27:10,882 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f71fed0>
2025-10-08 23:27:10,882 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:10,882 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:10,882 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:10,882 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:10,882 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:11,153 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): us.i.posthog.com:443
2025-10-08 23:27:11,385 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:27:11,736 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'141'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-85bf64bb75-jcjwb'), (b'x-envoy-upstream-service-time', b'332'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999602'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'23ms'), (b'x-request-id', b'req_7df3756d0a5d482a9c5b1dcc721322ef'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Tzd1Sg1RqPq4VIl2rMnI9BMPakC_idHvYVxyOxFzsA0-1759984031-1.0.1.1-lP62Kh0lX37o8GZfP4zhXE09CW8lWkM_UG1FH_7_W1KUuUPE12tzfF6e0MzOLoAHfLCPYftqv1MnxE6uC0OlCIcb1FfM_9fnDPh4tAgZx2Y; path=/; expires=Thu, 09-Oct-25 04:57:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=n6ZMTwQbkb4UiPNinIyJfPCjwdKHwuT1Hg8c1nfxTpM-1759984031788-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1fc19c7798b9-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:11,737 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:27:11,737 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:11,738 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:11,738 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:11,738 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:11,738 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:27:11 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '141'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-85bf64bb75-jcjwb'), ('x-envoy-upstream-service-time', '332'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999602'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '23ms'), ('x-request-id', 'req_7df3756d0a5d482a9c5b1dcc721322ef'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Tzd1Sg1RqPq4VIl2rMnI9BMPakC_idHvYVxyOxFzsA0-1759984031-1.0.1.1-lP62Kh0lX37o8GZfP4zhXE09CW8lWkM_UG1FH_7_W1KUuUPE12tzfF6e0MzOLoAHfLCPYftqv1MnxE6uC0OlCIcb1FfM_9fnDPh4tAgZx2Y; path=/; expires=Thu, 09-Oct-25 04:57:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=n6ZMTwQbkb4UiPNinIyJfPCjwdKHwuT1Hg8c1nfxTpM-1759984031788-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb1fc19c7798b9-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:27:11,738 [DEBUG] openai._base_client: request_id: req_7df3756d0a5d482a9c5b1dcc721322ef
2025-10-08 23:27:11,740 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:11,740 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:11,741 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:11,741 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:11,741 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:11,792 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'7182'), (b'date', b'Thu, 09 Oct 2025 04:27:11 GMT')])
2025-10-08 23:27:11,792 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-08 23:27:11,792 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:11,792 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:11,792 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:11,792 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:11,795 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-71649195-591d-4b02-af13-b6eb7d2bf40a', 'json_data': {'messages': [{'content': '\n    You are an expert cover letter writer specializing in creating compelling, personalized cover letters that effectively connect a candidate\'s background to specific job opportunities.\n\n    Your task is to analyze the job posting, personalized resume, and retrieved context to generate ONLY the cover letter content paragraphs in plain LaTeX format.\n\n    POSITION: AI Developer\n    COMPANY: BMO\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PERSONALIZED RESUME CONTENT:\n    Highlights: \\resumeItem{\\textbf{AI/ML Model Development:} Extensive experience in designing and deploying \\textbf{AI architectures} using \\textbf{TensorFlow} and \\textbf{PyTorch}, achieving significant performance improvements in \\textbf{distributed learning}.}\n\\resumeItem{\\textbf{Cloud and DevOps Expertise:} Proficient in \\textbf{AWS} and \\textbf{Azure} for AI/ML services, with hands-on experience in \\textbf{Docker} and \\textbf{CI/CD} pipelines, aligning with best practices for cloud-based applications.}\n\\resumeItem{\\textbf{Blockchain and Security:} Innovatively integrated \\textbf{blockchain} technologies with AI/ML solutions using \\textbf{Hyperledger Fabric}, enhancing system \\textbf{security} and coordination through novel consensus mechanisms.}\n\\resumeItem{\\textbf{Data Integration and Transformation:} Proven ability to construct robust data pipelines using \\textbf{SQL} and \\textbf{Python} for seamless integration and reliable input for AI/ML applications, enhancing backend functionality and data management.}\n\\resumeItem{\\textbf{Full-Stack Development:} Competent in developing \\textbf{RESTful APIs} with \\textbf{Express.js}, utilized in collaborative full-stack projects to achieve seamless integration and enhanced user experience.}\n\\resumeItem{\\textbf{Agile and Collaborative Workflows:} Demonstrated proficiency in \\textbf{Agile} with \\textbf{Scrum} and \\textbf{DevOps} practices using the \\textbf{Atlassian suite}, significantly enhancing project management and team communication.}\n\\resumeItem{\\textbf{Research and Innovation:} Authored and presented research in top-tier IEEE venues, showcasing skills in technical communication and problem-solving within the \\textbf{AI/ML} domain.}\n    Experiences: \\resumeSubheading\n    {University of Manitoba}{September 2023 ‚Äì July 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed four novel distributed \\textbf{AI architectures} using \\textbf{TensorFlow} and \\textbf{PyTorch}, significantly enhancing energy efficiency and scalability in \\textbf{distributed learning}.}\n        \\resumeItem{Implemented \\textbf{blockchain} consensus mechanism, PoCL, reducing communication overhead by 85.2\\%, while improving fault tolerance by 62.7\\%.}\n        \\resumeItem{Optimized AI architectures leveraging \\textbf{Hyperledger Fabric} for secure and decentralized coordination, showcasing advanced \\textbf{cloud computing} applications.}\n        \\resumeItem{Authored research published/submitted to top-tier IEEE venues, highlighting technical communication and problem-solving skills in AI/ML contexts.}\n        \\resumeItem{Addressed critical challenges in AI/ML development with modular \\textbf{API} development using \\textbf{Express.js/Flask}.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì August 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated product development with \\textbf{RESTful API} design and implementation using \\textbf{Supabase} and \\textbf{PostgreSQL}, enhancing backend functionality and data management.}\n        \\resumeItem{Automated data integration by creating a \\textbf{Python} script for CSV to SQL conversion, significantly streamlining database population.}\n        \\resumeItem{Collaborated with the front-end team to ensure seamless \\textbf{full-stack integration}, aligning API specifications with user interface requirements.}\n        \\resumeItem{Utilized the \\textbf{Atlassian suite} (Jira, Confluence) to improve agile workflow efficiency and team coordination, showcasing effective \\textbf{DevOps} practices.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Sadr Group Company}{July 2020 ‚Äì December 2020}\n    {Full-stack Developer Intern}{}\n    \\resumeItemListStart\n        \\resumeItem{Drove successful completion of a critical internal project by addressing key deficiencies in \\textbf{API security}, \\textbf{authentication}, and data modeling.}\n        \\resumeItem{Developed robust \\textbf{RESTful APIs} with \\textbf{AdonisJS} and \\textbf{Express.js}, implementing \\textbf{JWT authentication} to secure user access.}\n        \\resumeItem{Designed and implemented accurate \\textbf{MySQL database schemas} for precise tracking and management of employee working hours.}\n        \\resumeItem{Deployed fully functional features to a live production server and created thorough \\textbf{Swagger API documentation}, facilitating seamless integration and knowledge transfer.}\n        \\resumeItem{Contributed to an \\textbf{Agile Scrum development team} by actively managing tasks within ClickUp, ensuring efficient progress and communication within \\textbf{CI/CD pipelines}.}\n    \\resumeItemListEnd\n    Skills: \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: PyTorch, TensorFlow, Scikit-learn, Transformers, Pandas, Numpy, LangChain, LangGraph, Matplotlib} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Git, GitHub, Linux, CI/CD, AWS} \\\\\n    \\textbf{Languages}{: Python, SQL, JavaScript, TypeScript, Node JS} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Db2, Redis} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum}\n}}\n\\end{itemize}\n    Projects: \\resumeProjectHeading\n    {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{Python, TensorFlow, Hyperledger Fabric, Docker, Flask} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a novel \\textbf{blockchain consensus mechanism} using \\textbf{TensorFlow} and \\textbf{Hyperledger Fabric}, transforming mining from energy-intensive processes to a federated learning-based collaboration.}\n        \\resumeItem{Developed a custom \\textbf{Python}-based federated learning system that facilitates miner collaboration and model training, leveraging \\textbf{Docker} for seamless deployment and scalability.}\n        \\resumeItem{Reduced mining energy consumption by over 70\\% through democratized voting and performance-based rewards, showcasing proficiency in \\textbf{cloud-based AI/ML solutions}.}\n    \\resumeItemListEnd\n\n### Explanation\n- **Technology Selection**: I prioritized technologies like Python, TensorFlow, and Docker since they are explicitly mentioned or highly relevant to AI/ML roles. Hyperledger Fabric is highlighted for its role in blockchain, providing a unique edge.\n- **Achievements & Implementation**: Focused on innovative elements of the project such as the federated learning-based consensus mechanism, reflecting the role‚Äôs need for AI/ML model operationalization and deployment expertise. Quantified improvement metrics to align with job expectations.\n- **Job Relevance**: The selected technologies and project achievements directly map to core duties in deploying and maintaining AI/ML models in cloud environments, as required in the job posting.\n\n\\resumeProjectHeading\n    {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{Python, \\textbf{PyTorch}, Docker, \\textbf{Node.js}, Flask, JavaScript} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a novel \\textbf{blockchain} framework using \\textbf{Node.js} and Hyperledger Fabric to ensure secure transactions and incentivize data sharing, aligning with the transparency and security focus in \\textbf{DevSecOps} environments.}\n        \\resumeItem{Developed \\textbf{Python} and \\textbf{PyTorch}-based federated learning models compatible with business-critical datasets like MNIST and CIFAR-10, enabling high-performance \\textbf{AI/ML models} for client applications.}\n        \\resumeItem{Built a modular and containerized architecture with \\textbf{Docker}, improving deployment efficiency by 30\\% and ensuring seamless integration with cloud platforms, reflecting proficiency in cloud-based AI/ML solutions.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Hyperledger Fabric, Flask} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a \\textbf{Temporal Fusion Transformer (TFT)} model using \\textbf{PyTorch} for multi-variate time-series forecasting, predicting temperature, CO2, and humidity in a smart building environment.}\n        \\resumeItem{Leveraged \\textbf{Hyperledger Fabric} to secure decentralized training processes, ensuring data privacy through blockchain-enabled federated learning across 76 rooms.}\n        \\resumeItem{Achieved significant performance improvements in prediction accuracy with our federated learning approach, maintaining complete \\textbf{data privacy} without centralized data collection.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Paper Summarizer} $|$ \\emph{Python, Flask, PyTorch, EasyOCR, Detectron2} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Integrated \\textbf{OCR} and \\textbf{computer vision} methods to automatically extract and summarize content from PDF documents, enhancing document analysis capabilities.}\n        \\resumeItem{Developed a \\textbf{Flask API} to host AI models, enabling seamless server-client communication and \\textbf{RESTful} operations for text and figure analysis.}\n        \\resumeItem{Achieved efficient document summarization by using \\textbf{PyTorch}-based models, improving processing speed by 20\\% through optimized multi-modal analysis.}\n    \\resumeItemListEnd\n\nThis LaTeX entry emphasizes the project\'s use of relevant technologies that match the job posting, while also highlighting the key achievements and technical implementations that align with the AI Developer role\'s requirements.\n\n    RETRIEVED CONTEXT FROM KNOWLEDGE BASE:\n    [Document(id=\'645f87c1-7931-4aad-ae8a-145e118414c9\', metadata={\'position\': \'Python Developer\', \'source\': \'cover letter\', \'company\': \'Ascendion\', \'start_index\': 507}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'8cd7ca4e-dc41-42e4-a9b8-c8b1250e6183\', metadata={\'start_index\': 507, \'source\': \'cover letter\', \'position\': \'Python Developer\', \'company\': \'Ascendion\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'a1bfdd36-5dad-4d9b-bb94-46b08067f5bf\', metadata={\'company\': \'Apple\', \'start_index\': 511, \'source\': \'cover letter\', \'position\': \'Python Developer\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'4bd89d99-19d6-45a8-a6ff-7184a911e904\', metadata={\'company\': \'Apple\', \'start_index\': 513, \'position\': \'Python Developer\', \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'c3d0e739-8890-42c5-9860-ad934a519018\', metadata={\'start_index\': 1068, \'position\': \'Python Developer\', \'source\': \'cover letter\', \'company\': \'Ascendion\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s dedication to creating opportunities and fostering inclusion. My experience in developing advanced AI models and implementing distributed systems using \\\\textbf{TensorFlow} and \\\\textbf{PyTorch} supports these initiatives. Furthermore, my projects such as the \\\\textbf{Federated Learning enabled Digital Twin} demonstrate my ability to innovate within the realm of \\\\textbf{blockchain} and IoT, relevant areas that can bolster Ascendion\'s engineering solutions for Fortune 500 clients.\\n\\nI am enthusiastic about the prospect of contributing to Ascendion\'s mission by leveraging my skills to build advanced digital solutions. I am confident that my background in \\\\textbf{distributed systems} and my passion for \\\\textbf{digital engineering} will prove valuable to your high-performing team. I am eager to bring my unique perspective and technical expertise to Ascendion, collaborating to engineer technology that elevates life."), Document(id=\'a3aa52f0-b05f-4d47-a377-4b43ff140cfd\', metadata={\'start_index\': 1068, \'source\': \'cover letter\', \'company\': \'Ascendion\', \'position\': \'Python Developer\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s dedication to creating opportunities and fostering inclusion. My experience in developing advanced AI models and implementing distributed systems using \\\\textbf{TensorFlow} and \\\\textbf{PyTorch} supports these initiatives. Furthermore, my projects such as the \\\\textbf{Federated Learning enabled Digital Twin} demonstrate my ability to innovate within the realm of \\\\textbf{blockchain} and IoT, relevant areas that can bolster Ascendion\'s engineering solutions for Fortune 500 clients.\\n\\nI am enthusiastic about the prospect of contributing to Ascendion\'s mission by leveraging my skills to build advanced digital solutions. I am confident that my background in \\\\textbf{distributed systems} and my passion for \\\\textbf{digital engineering} will prove valuable to your high-performing team. I am eager to bring my unique perspective and technical expertise to Ascendion, collaborating to engineer technology that elevates life."), Document(id=\'19bd50c7-d116-4802-9a2f-9c023f304b19\', metadata={\'company\': \'Ascendion\', \'position\': \'Python Developer\', \'source\': \'cover letter\', \'start_index\': 1029}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s commitment to creating technology that elevates life and your emphasis on digital engineering for Fortune 500 clients. My experience in \\\\textbf{distributed systems} and machine learning provides me with the skills needed to excel in this role. I am excited about the prospect of applying cutting-edge AI and machine learning technologies to further Ascendion\'s innovative goals and solve complex problems for your clients.\\n\\nI am enthusiastic about the opportunity to contribute to \\\\textbf{Ascendion}\'s mission by bringing my skills to build advanced digital solutions. With my strong background in Python and related technologies, I am confident that I can make immediate and significant contributions to your team, helping Ascendion deliver captivating and cutting-edge solutions."), Document(id=\'f2931001-67a9-4907-b8b2-08da7871aae3\', metadata={\'source\': \'cover letter\', \'position\': \'Junior BI developer\', \'company\': \'Dexian\', \'start_index\': 457}, page_content="In my recent role as a Full-stack Developer Intern at Bobo, I accelerated product development by designing and implementing RESTful APIs using \\\\textbf{Supabase} and \\\\textbf{PostgreSQL}, which enhanced backend functionality and data management for customer analytics applications. Additionally, I automated data integration processes using Python scripting, streamlining workflows and directly contributing to improved data handling capabilities. My hands-on experience with \\\\textbf{SQL} and data modeling aligns with Dexian\'s requirement for transforming raw data into organized models and creating compelling Power BI dashboards.")]\n                                                       \n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Company mission, values, and culture\n       - Key technical requirements and preferred technologies\n       - Required experience level and responsibilities\n       - Specific skills and qualifications sought\n\n    2. Use the personalized resume content to:\n       - Highlight the most relevant experiences and achievements\n       - Showcase technical skills that match job requirements\n       - Reference specific projects that demonstrate required capabilities\n       - Quantify accomplishments where possible\n\n    3. Leverage the retrieved context to:\n       - Add depth and authenticity to your statements\n       - Reference relevant research, projects, or experiences not in the resume\n       - Demonstrate domain knowledge and passion for the field\n       - Show progression and learning from past experiences\n\n    4. Follow the template structure but adapt content to:\n       - Match the company\'s tone and industry language\n       - Create a narrative that connects past experiences to future contributions\n       - Show genuine enthusiasm for the specific role and company\n       - Address any potential gaps or explain career transitions\n\n\n    CRITICAL OUTPUT REQUIREMENTS:\n    - Generate ONLY the cover letter content paragraphs\n    - DO NOT include any LaTeX document structure (\\documentclass, \\begin{document}, \\begin{letter}, \\end{letter}, \\end{document})\n    - DO NOT include any explanations, introductions, or meta-commentary\n    - DO NOT include salutations like "Dear Hiring Manager" or closings like "Sincerely"\n    - Start directly with the first paragraph content\n    - End with the last paragraph content\n    - Use clean paragraph breaks (double newlines) between paragraphs\n\n    CONTENT STRUCTURE:\n    Write exactly 3-4 paragraphs of cover letter content:\n\n    Paragraph 1: Express enthusiasm for the specific role and company, briefly state your relevant background and why you\'re applying.\n\n    Paragraph 2: Highlight your most relevant technical experiences and projects with specific examples that match the job requirements.\n\n    Paragraph 3: Demonstrate alignment with the company\'s mission and technology stack, referencing retrieved context to show deeper knowledge and genuine interest.\n\n    Paragraph 4 (optional): Reiterate enthusiasm and mention specific contributions you can make to the team and company.\n\n                                                       \n    COVER LETTER STRUCTURE:\n    1. Opening paragraph: Express enthusiasm for the specific role and company, briefly state your relevant background\n    2. Body paragraph 1: Highlight relevant technical experience and projects with specific examples\n    3. Body paragraph 2: Demonstrate alignment with company mission/technology stack, reference retrieved context for depth\n    4. Closing paragraph: Reiterate enthusiasm, mention specific contributions you can make, professional closing\n\n    TONE AND STYLE:\n    - Professional yet enthusiastic\n    - Confident but not boastful\n    - Specific and detailed, not generic\n    - Forward-looking and solution-oriented\n    - Authentic and genuine\n    LATEX FORMATTING REQUIREMENTS:\n    - Use proper LaTeX escaping: \\& instead of & for ampersands\n    - Use \\% for percentages in text  \n    - Use \\textbf{} for emphasizing key technologies, company names, and achievements\n    - Write in clean, readable paragraph format\n    - Use standard paragraph separation (double newlines)\n\n    CONTENT GUIDELINES:\n    - Reference specific technologies, frameworks, and methodologies from the job posting\n    - Use metrics and quantified achievements where available\n    - Show understanding of the company\'s challenges and how you can help\n    - Demonstrate continuous learning and adaptability\n    - Connect past experiences to future potential contributions\n    - Use retrieved context to add unique insights or relevant background\n    - Be professional yet enthusiastic, confident but not boastful\n    - Make it specific and detailed, not generic\n\n    OUTPUT FORMAT EXAMPLE:\n    I am excited to apply for the {position} position at \\textbf{{company name}}. With my background in \\textbf{relevant technology} and experience in \\textbf{relevant domain}, I am eager to contribute to your team\'s mission of advancing \\textbf{company focus area}.\n\n    In my recent projects, I have built scalable applications using \\textbf{specific technologies} that align with your requirements. For example, in my \\textbf{project name} project, I developed \\textbf{specific implementation} that resulted in \\textbf{quantified outcome}. My experience with \\textbf{relevant technology stack} has prepared me to tackle the challenges outlined in your job posting.\n\n    I am particularly drawn to \\textbf{{company name}}\'s commitment to \\textbf{company values/mission}. My research in \\textbf{relevant area from context} has given me deep insights into \\textbf{relevant domain knowledge}, and I am excited about the opportunity to apply this knowledge in a production environment where I can help \\textbf{specific company goals}.\n\n    Generate the cover letter content paragraphs following this exact format:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:27:11,796 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:27:11,797 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:11,797 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:11,797 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:11,797 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:11,797 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:16,054 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4135'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4149'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'23580'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'12.84s'), (b'x-request-id', b'req_e04b8565335841ff88a1b2a17cfe201e'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1fc749e795c1-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:16,057 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:27:16,057 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:16,058 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:16,058 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:16,058 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:16,058 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:27:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4135', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4149', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '23580', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '12.84s', 'x-request-id': 'req_e04b8565335841ff88a1b2a17cfe201e', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1fc749e795c1-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:27:16,058 [DEBUG] openai._base_client: request_id: req_e04b8565335841ff88a1b2a17cfe201e
2025-10-08 23:27:17,485 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:27:17,491 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:27:17,491 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:27:17,491 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:27:17,491 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:27:17,491 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:27:17,492 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f372290>
2025-10-08 23:27:17,492 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,493 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,493 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,493 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,493 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,494 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:17,494 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:27:17,494 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,494 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,494 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,494 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:17,495 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:27:17,501 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:27:17,501 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:27:17,501 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:27:17,501 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:27:17,501 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:27:17,502 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f71f850>
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,503 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:17,504 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,504 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,505 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:17,505 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:27:17,505 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,505 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,505 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,505 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:17,506 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:17,507 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,507 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:17,507 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,507 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:17,508 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:17,508 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:27:17,509 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:17,509 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,509 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,509 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:17,511 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-9c8488dc-19cb-4407-8c4a-b0b631ff679f', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10f4618a0>, 'json_data': {'input': [[40, 1097, 12304, 311, 3881, 369, 279, 15592, 25922, 2361, 520, 1144, 1342, 13536, 90, 33, 10754, 7966, 3161, 856, 4092, 304, 1144, 1342, 13536, 90, 15836, 14, 2735, 4500, 92, 323, 16781, 3217, 304, 1144, 1342, 13536, 90, 12641, 3600, 2186, 358, 1097, 24450, 311, 17210, 311, 701, 2128, 596, 9131, 315, 77582, 15592, 311, 18885, 2626, 4791, 15204, 12023, 8522, 13, 1666, 264, 12514, 6721, 11, 358, 1097, 38564, 520, 279, 6776, 311, 12178, 426, 10754, 596, 20770, 311, 92064, 2949, 6020, 3600, 1555, 279, 1005, 315, 14713, 48448, 15592, 14645, 13], [644, 856, 3293, 7224, 11, 358, 617, 8040, 69311, 8522, 1701, 1144, 1342, 13536, 90, 26404, 19410, 92, 323, 1144, 1342, 13536, 90, 14149, 51, 22312, 2186, 32145, 28289, 5178, 18637, 304, 4332, 6975, 22484, 13, 45863, 11, 856, 990, 389, 279, 1144, 1342, 13536, 90, 32176, 315, 49681, 1413, 21579, 92, 2447, 11798, 264, 11775, 18428, 24811, 17383, 11, 18189, 4907, 15652, 555, 927, 220, 2031, 59, 14697, 23212, 11, 2391, 856, 40061, 439, 264, 44825, 8483, 22103, 11, 358, 10837, 1646, 10741, 7177, 323, 20968, 22514, 828, 58773, 449, 1144, 1342, 13536, 90, 31380, 92, 323, 1144, 1342, 13536, 90, 6827, 2186, 23391, 15062, 1988, 369, 15592, 14, 2735, 8522, 430, 5398, 61440, 449, 426, 10754, 596, 16686, 369, 9624, 22484, 13], [40, 1097, 8104, 15107, 311, 1144, 1342, 13536, 90, 33, 10754, 11923, 82, 4741, 100051, 5603, 311, 15592, 18052, 2949, 19758, 1144, 5, 8184, 50599, 13, 3092, 14584, 3495, 11, 74088, 10666, 304, 1948, 58355, 40135, 37278, 11, 706, 5954, 291, 856, 3575, 99246, 7512, 304, 279, 1144, 1342, 13536, 90, 15836, 14, 2735, 92, 8106, 11, 323, 358, 1097, 24450, 311, 3881, 420, 6677, 311, 6179, 2626, 323, 828, 8198, 39210, 389, 15592, 17357, 13, 23674, 11, 856, 63239, 304, 1144, 1342, 13536, 90, 37236, 92, 323, 1144, 1342, 13536, 90, 79207, 92, 15592, 3600, 2345, 40967, 4979, 555, 9624, 82571, 2345, 28536, 757, 439, 264, 15525, 9513, 369, 29820, 311, 279, 25605, 2065, 315, 15592, 14, 2735, 4211, 304, 701, 9624, 28271, 382, 2409, 264, 17033, 3839, 3335, 315, 38656, 304, 15592, 14, 2735, 1646, 24047, 323, 264, 11939, 369, 19297, 11, 358, 1097, 42702, 922, 279, 22199, 315, 18667, 426, 10754, 323, 29820, 311, 279, 4500, 315, 87435, 15592, 10105, 13, 358, 1097, 16913, 430, 856, 11156, 7512, 323, 39955, 311, 19815, 6975, 690, 12207, 8935, 701, 2128, 13]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:27:17,511 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:27:17,511 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:27:17,541 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe170d0>
2025-10-08 23:27:17,541 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10fa611c0> server_hostname='api.openai.com' timeout=None
2025-10-08 23:27:17,574 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe17190>
2025-10-08 23:27:17,575 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:17,575 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,575 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:17,575 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,575 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:17,959 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'195'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-85bf64bb75-lz76p'), (b'x-envoy-upstream-service-time', b'295'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999600'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'24ms'), (b'x-request-id', b'req_127cabfcfaa743818a092096acde4cf9'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=96J_s.dPMwtB4fe23YizV27xZkx9ZRLw_2VLNOoL4XI-1759984038-1.0.1.1-ghGfcnR0U3j3VB33WiQdtzQM.LHtvmhEqLlNNLc1NK8iVrL9J6Xpz_KjWWtOa62Pu69pthRAnTqGzOQpBuBbHlJKkQy0tEqpJFTqcnyApFU; path=/; expires=Thu, 09-Oct-25 04:57:18 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=f6GmiazvxTbPoElg7D0CVKlSicARAjI.rgWTAM19XVk-1759984038017-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1feb682bc0b2-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:17,960 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:27:17,960 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:17,992 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,993 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,993 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:17,993 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:27:18 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '195'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-85bf64bb75-lz76p'), ('x-envoy-upstream-service-time', '295'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999600'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '24ms'), ('x-request-id', 'req_127cabfcfaa743818a092096acde4cf9'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=96J_s.dPMwtB4fe23YizV27xZkx9ZRLw_2VLNOoL4XI-1759984038-1.0.1.1-ghGfcnR0U3j3VB33WiQdtzQM.LHtvmhEqLlNNLc1NK8iVrL9J6Xpz_KjWWtOa62Pu69pthRAnTqGzOQpBuBbHlJKkQy0tEqpJFTqcnyApFU; path=/; expires=Thu, 09-Oct-25 04:57:18 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=f6GmiazvxTbPoElg7D0CVKlSicARAjI.rgWTAM19XVk-1759984038017-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb1feb682bc0b2-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:27:17,993 [DEBUG] openai._base_client: request_id: req_127cabfcfaa743818a092096acde4cf9
2025-10-08 23:27:17,997 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,997 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:17,997 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,997 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:17,997 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:27:17,998 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'55'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:17,998 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-08 23:27:17,998 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:27:17,998 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:17,999 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:17,999 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:18,001 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:18,001 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:18,001 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:18,001 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:18,001 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:18,021 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'2'), (b'date', b'Thu, 09 Oct 2025 04:27:17 GMT')])
2025-10-08 23:27:18,021 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-08 23:27:18,022 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:18,022 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:18,022 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:18,022 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:18,026 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:27:18,038 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-306b9d02-b314-492b-b320-2aef77c9f9ec', 'post_parser': <function Embeddings.create.<locals>.parser at 0x103812de0>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:27:18,039 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:27:18,039 [DEBUG] httpcore.connection: close.started
2025-10-08 23:27:18,040 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:27:18,040 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:27:18,075 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe17f50>
2025-10-08 23:27:18,075 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:27:18,108 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f70c110>
2025-10-08 23:27:18,108 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:27:18,109 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:27:18,109 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:27:18,109 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:27:18,109 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:27:19,326 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:27:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'178'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5db97ddc48-v2tmc'), (b'x-envoy-upstream-service-time', b'342'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_1211b5f9d1104055940957574fba4ef7'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb1feecaf4e852-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:27:19,326 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:27:19,327 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:27:19,328 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:27:19,328 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:27:19,328 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:27:19,328 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:27:19 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '178', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-5db97ddc48-v2tmc', 'x-envoy-upstream-service-time': '342', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999342', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_1211b5f9d1104055940957574fba4ef7', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb1feecaf4e852-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:27:19,328 [DEBUG] openai._base_client: request_id: req_1211b5f9d1104055940957574fba4ef7
2025-10-08 23:27:19,346 [INFO] src.api.app: ‚ùå POST /api/generate/ - 500 - 76.615s
2025-10-08 23:27:19,346 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 76.615s
2025-10-08 23:27:19,347 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:27:19] "[35m[1mPOST /api/generate/ HTTP/1.1[0m" 500 -
2025-10-08 23:42:39,712 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-08 23:42:39,714 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.002s
2025-10-08 23:42:39,715 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:42:39] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:42:39,717 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-08 23:42:39,717 [DEBUG] src.api.app: üìù Request data: {'job_description': 'About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'company_name': 'BMO', 'position_title': 'AI Developer'}
2025-10-08 23:42:39,750 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-dae34b8f-32ba-43fb-9d46-17dfb54e6b91', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10f4627a0>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:42:39,751 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:42:39,752 [DEBUG] httpcore.connection: close.started
2025-10-08 23:42:39,752 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:42:39,752 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:42:39,833 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf8bed0>
2025-10-08 23:42:39,833 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:42:39,869 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf8b550>
2025-10-08 23:42:39,871 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:42:39,886 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:42:39,886 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:42:39,886 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:42:39,886 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:42:40,301 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:42:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'90'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5dbdc66675-ncvms'), (b'x-envoy-upstream-service-time', b'293'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_941eb33b396045289a49e2d0fb78e90f'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=sKx3.08Jg0NhAV8yW0.WKFQCs_8dBXddNK4Ica1onmQ-1759984960-1.0.1.1-j9l4JpQFwEMA8zjodn5Wekw81XauSNWJiNQYaHfkWp3AeyyZjsFkvoMikV2re1qdVUwPJaojiHOV3VUsJ0W7Ph7ycNYHNR4uwbMTDXXO2QI; path=/; expires=Thu, 09-Oct-25 05:12:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb366fdc05231c-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:42:40,302 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:42:40,302 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:42:40,303 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:42:40,303 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:42:40,303 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:42:40,303 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:42:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '90', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-5dbdc66675-ncvms', 'x-envoy-upstream-service-time': '293', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999342', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_941eb33b396045289a49e2d0fb78e90f', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=sKx3.08Jg0NhAV8yW0.WKFQCs_8dBXddNK4Ica1onmQ-1759984960-1.0.1.1-j9l4JpQFwEMA8zjodn5Wekw81XauSNWJiNQYaHfkWp3AeyyZjsFkvoMikV2re1qdVUwPJaojiHOV3VUsJ0W7Ph7ycNYHNR4uwbMTDXXO2QI; path=/; expires=Thu, 09-Oct-25 05:12:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb366fdc05231c-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:42:40,303 [DEBUG] openai._base_client: request_id: req_941eb33b396045289a49e2d0fb78e90f
2025-10-08 23:42:40,315 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.598s
2025-10-08 23:42:40,316 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:42:40] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:42:48,894 [INFO] src.api.app: üåê GET /api/resumes/generated/BMO/AI Developer.pdf - 127.0.0.1
2025-10-08 23:42:48,900 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/BMO/AI Developer.pdf - 200 - 0.006s
2025-10-08 23:42:48,901 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:42:48] "GET /api/resumes/generated/BMO/AI%20Developer.pdf HTTP/1.1" 200 -
2025-10-08 23:42:58,298 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-08 23:42:58,299 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.001s
2025-10-08 23:42:58,299 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:42:58] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-08 23:42:58,302 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-08 23:42:58,303 [DEBUG] src.api.app: üìù Request data: {'jobDescription': 'About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'companyName': 'BMO', 'positionTitle': 'AI Developer', 'strategy': 'generate'}
2025-10-08 23:42:58,326 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-41fb59b9-acbf-4a2f-b02e-0f6a04c0d3ba', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe38ae0>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:42:58,328 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:42:58,328 [DEBUG] httpcore.connection: close.started
2025-10-08 23:42:58,329 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:42:58,329 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:42:58,365 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe4ddd0>
2025-10-08 23:42:58,365 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:42:58,402 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe4ad50>
2025-10-08 23:42:58,402 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:42:58,403 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:42:58,403 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:42:58,403 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:42:58,403 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:42:58,750 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:42:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'88'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-69d8474f6f-gzmvz'), (b'x-envoy-upstream-service-time', b'142'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_4827abb98a0948c382afba63e85a0512'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb36e38e5a13cb-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:42:58,750 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:42:58,751 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:42:58,752 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:42:58,752 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:42:58,752 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:42:58,752 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:42:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '88', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-69d8474f6f-gzmvz', 'x-envoy-upstream-service-time': '142', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999342', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_4827abb98a0948c382afba63e85a0512', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb36e38e5a13cb-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:42:58,753 [DEBUG] openai._base_client: request_id: req_4827abb98a0948c382afba63e85a0512
2025-10-08 23:42:58,801 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-698fc40e-8335-4437-95f5-c6d6b80a7889', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring professional experiences to specific job requirements.\n    \n    Your task is to analyze the provided job posting and candidate experiences, then generate LaTeX-formatted resume entries that highlight the most relevant skills, achievements, and experiences for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE EXPERIENCES:\n    [\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Graduate Research Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2023-09",\n    "end_date": "2025-07",\n    "duration": "1 yr 11 mos",\n    "highlights": [\n      "Designed and developed four novel distributed AI architectures (PoCL, SSFL, BSFL, BPFL) addressing critical challenges in distributed learning.",\n      "PoCL: a blockchain consensus mechanism that repurposes energy-intensive mining for secure, incentive-aligned, and resource-efficient federated learning.",\n      "SSFL: An architecture enhancing scalability and efficiency for distributed deep learning.",\n      "BSFL: The first decentralized SplitFed Learning framework leveraging smart contracts for model integrity and decentralized coordination.",\n      "BPFL: A framework improving fairness and model ownership through contribution-based personalization and tokenized access.",\n      "Implemented and optimized these architectures using a robust tech stack, including TensorFlow, PyTorch for AI model development, Hyperledger Fabric for permissioned blockchain integration, and Express.js/Flask for modular API development.",\n      "Reduced communication overhead by 85.2%, improved fault tolerance by 62.7%, and enhanced energy efficiency through PoCL.",\n      "Authored research published/submitted to top-tier IEEE venues."\n    ],\n    "skills": [\n      "Distributed Machine Learning",\n      "TensorFlow",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Blockchain",\n      "System Optimization",\n      "Consensus Algorithms",\n      "Technical Communication",\n      "Problem Solving"\n    ],\n    "publications": [\n      "Proof-of-Collaborative-Learning: A Multi-winner Federated Learning Consensus Algorithm (Published)",\n      "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches (Accepted)",\n      "Towards Fair Model Ownership: Blockchain-Driven Personalization in Federated Learning (Under Review)"\n    ]\n  },\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Teaching Assistant",\n    "employment_type": "Contract",\n    "start_date": "2024-09",\n    "end_date": "2025-06",\n    "duration": "10 mos",\n    "highlights": [\n      "Delivered direct academic and technical support to over 400 undergraduate students across foundational Computer Science courses, including Introduction to Programming (Python), Object-Oriented Programming (Java), Data Structures & Algorithms, Computer Networks, and Distributed Systems.",\n      "Guided students through complex coding projects and theoretical challenges, focusing on problem-solving strategies, algorithm efficiency, and system design principles.",\n      "Offered specialized technical assistance at the university\'s Help Centre, adeptly diagnosing and resolving programming issues and advising on effective coding practices for diverse academic contexts."\n    ],\n    "skills": [\n      "Python",\n      "Java",\n      "Data Structures & Algorithms",\n      "Computer Networks",\n      "Distributed Systems",\n      "Technical Communication"\n    ]\n  },\n  {\n    "organization": "Bobo",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Full-stack Developer Intern",\n    "employment_type": "Internship",\n    "start_date": "2024-05",\n    "end_date": "2024-08",\n    "duration": "4 mos",\n    "highlights": [\n      "Accelerated product development by designing and implementing RESTful APIs using Supabase and PostgreSQL, enhancing backend functionality and data management.",\n      "Automated data integration by developing a Python script to convert CSV data into executable SQL, significantly streamlining database population.",\n      "Ensured seamless full-stack integration by collaborating closely with the front-end team to align API specifications with user interface requirements.",\n      "Contributed to agile workflow efficiency through active utilization of the Atlassian suite (Jira, Confluence) for task management, documentation, and team coordination."\n    ],\n    "skills": [\n      "Supabase",\n      "PostgreSQL",\n      "Jira",\n      "Confluence",\n      "Collaboration"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "location": "",\n    "title": "Lead Teaching Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2021-01",\n    "end_date": "2023-01",\n    "duration": "2 yrs 1 mo",\n    "highlights": [\n      "Supported and guided over 300 undergraduate students across complex technical subjects, including Linear Algebra, System Design and Analysis, Computer Networks, Discrete Mathematics, Operating Systems, and Algorithm Design.",\n      "Designed and developed engaging lectures, practical coding projects, and challenging assignments, significantly enhancing student comprehension and hands-on application of core computer science concepts.",\n      "Provided specialized support in areas like algorithm implementation, system architecture, and network protocols, bridging theoretical knowledge with practical problem-solving."\n    ],\n    "skills": [\n      "System Design and Analysis",\n      "Computer Networks",\n      "Algorithms",\n      "Operating Systems",\n      "Technical Teaching"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "title": "Research Assistant",\n    "location": "",\n    "employment_type": "Full-time",\n    "start_date": "2021-06",\n    "end_date": "2022-08",\n    "duration": "1 yr 3 mos",\n    "highlights": [\n      "Led an undergraduate research team, successfully initiating and guiding a study on solar panel funding fairness in Switzerland that culminated in key analytical findings.",\n      "Executed comprehensive data collection and cleaning of over 10GB of raw data from diverse government portals, ensuring high data integrity and preparing datasets for advanced analysis.",\n      "Leveraged advanced causal inference techniques in R to analyze complex datasets, producing detailed analytical reports that quantified and highlighted significant subsidy biases."\n    ],\n    "skills": [\n      "PostgreSQL",\n      "PostGIS",\n      "R",\n      "Machine Learning",\n      "Data Cleaning",\n      "Team Leadership"\n    ]\n  },\n  {\n    "organization": "Sadr Group Company",\n    "title": "Full-stack Developer Intern",\n    "location": "",\n    "employment_type": "Part-time",\n    "start_date": "2020-07",\n    "end_date": "2020-12",\n    "duration": "6 mos",\n    "highlights": [\n      "Drove the successful completion of a critical internal project, addressing key deficiencies in API security, authentication, and data modeling for production deployment.",\n      "Developed robust RESTful APIs with AdonisJS and Express.js, implementing JWT authentication to secure user access and enable core application functionalities.",\n      "Designed and implemented highly accurate MySQL database schemas for precise tracking and management of employee working and non-working hours, foundational for the project\'s business logic.",\n      "Deployed fully functional features to the live production server and created thorough Swagger API documentation, facilitating seamless internal integration and knowledge transfer.",\n      "Contributed to an Agile Scrum development team by actively tracking and managing personal tasks within ClickUp, ensuring efficient progress and clear communication within sprints and CI/CD pipelines."\n    ],\n    "skills": [\n      "Node.js",\n      "Express.js",\n      "Adonis.js",\n      "Vue.js",\n      "MySQL",\n      "Swagger",\n      "Technical Communication"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements, skills, technologies, and qualifications\n    2. Select the 3-4 most relevant experiences from the candidate\'s background\n    3. For each selected experience, generate a LaTeX resume entry following this EXACT format:\n\n    \\resumeSubheading\n        {Organization Name}{Start Date ‚Äì End Date}\n        {Job Title}{Location}\n        \\resumeItemListStart\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - Use \\textbf{} to bold technical skills, technologies, methodologies, and key achievements mentioned in the job posting\n    - Start each \\resumeItem with strong action verbs (Developed, Implemented, Designed, Led, Accelerated, etc.)\n    - Quantify achievements with numbers/percentages when available (use \\% for percentages in LaTeX)\n    - Tailor the language to match the job posting\'s terminology\n    - Highlight transferable skills even if from different domains\n    - Focus on impact and results, not just responsibilities\n    - Ensure each experience shows progression and growth\n    - Maximum 5 resume items per experience\n    - Order experiences by relevance to the job posting\n\n    PRIORITIZATION CRITERIA:\n    1. Direct skill/technology matches with job requirements\n    2. Relevant industry experience\n    3. Leadership and project management experience\n    4. Technical depth and complexity of work\n    5. Recent and duration of experience\n\n    Generate the LaTeX resume entries for the most relevant experiences:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:42:58,804 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:42:58,804 [DEBUG] httpcore.connection: close.started
2025-10-08 23:42:58,806 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:42:58,806 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:42:58,836 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe32750>
2025-10-08 23:42:58,836 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10cb5f0b0> server_hostname='api.openai.com' timeout=None
2025-10-08 23:42:58,871 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe31250>
2025-10-08 23:42:58,872 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:42:58,872 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:42:58,872 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:42:58,872 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:42:58,872 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:18,002 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:17 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'18815'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'18896'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26822'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.356s'), (b'x-request-id', b'req_960b3f3609ce485daca060cc7f75484d'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ImiG9PGmV_umZ1FPo0fDbSR_qczA4LNWGefgTqmn8iA-1759984997-1.0.1.1-Nys5Jod7IvmNk8Yqh3_3G7TmYycODMOlvD0gllHfsnHbQUYbBSklkI79ArRedpsIsYQO7KqsoZDsq1iJlakJe3wKoaEYRYL_wbvq9oq3roI; path=/; expires=Thu, 09-Oct-25 05:13:17 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb36e68fbd0105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:18,004 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:18,004 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:18,004 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:18,004 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:18,004 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:18,004 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:17 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '18815', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '18896', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26822', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '6.356s', 'x-request-id': 'req_960b3f3609ce485daca060cc7f75484d', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=ImiG9PGmV_umZ1FPo0fDbSR_qczA4LNWGefgTqmn8iA-1759984997-1.0.1.1-Nys5Jod7IvmNk8Yqh3_3G7TmYycODMOlvD0gllHfsnHbQUYbBSklkI79ArRedpsIsYQO7KqsoZDsq1iJlakJe3wKoaEYRYL_wbvq9oq3roI; path=/; expires=Thu, 09-Oct-25 05:13:17 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb36e68fbd0105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:18,005 [DEBUG] openai._base_client: request_id: req_960b3f3609ce485daca060cc7f75484d
2025-10-08 23:43:18,022 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-7c755ede-0067-4b58-adaa-6d5496b62ce3', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring technical skills sections to specific job requirements.\n\n    Your task is to analyze the job posting and remove only those skills from the candidate\'s skill set that are clearly irrelevant, keeping a broad and well-rounded technical skills section. Start with the full list, then prune.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    {\n  "AI / Machine Learning": [\n    {\n      "skill": "TensorFlow",\n      "expertise": 8\n    },\n    {\n      "skill": "PyTorch",\n      "expertise": 8\n    },\n    {\n      "skill": "Flower",\n      "expertise": 7\n    },\n    {\n      "skill": "Numpy",\n      "expertise": 9\n    },\n    {\n      "skill": "Pandas",\n      "expertise": 8\n    },\n    {\n      "skill": "OpenCV",\n      "expertise": 7\n    },\n    {\n      "skill": "Transformers",\n      "expertise": 8\n    },\n    {\n      "skill": "LangChain",\n      "expertise": 8\n    },\n    {\n      "skill": "LangGraph",\n      "expertise": 7\n    },\n    {\n      "skill": "Scikit-learn",\n      "expertise": 8\n    },\n    {\n      "skill": "Keras",\n      "expertise": 8\n    },\n    {\n      "skill": "Matplotlib",\n      "expertise": 9\n    },\n    {\n      "skill": "TensorBoard",\n      "expertise": 5\n    },\n    {\n      "skill": "CUDA",\n      "expertise": 6\n    }\n  ],\n  "Languages": [\n    {\n      "skill": "Python",\n      "expertise": 10\n    },\n    {\n      "skill": "C++",\n      "expertise": 5\n    },\n    {\n      "skill": "Java",\n      "expertise": 7\n    },\n    {\n      "skill": "JavaScript",\n      "expertise": 9\n    },\n    {\n      "skill": "TypeScript",\n      "expertise": 9\n    },\n    {\n      "skill": "Node JS",\n      "expertise": 9\n    },\n    {\n      "skill": "Solidity",\n      "expertise": 3\n    }\n  ],\n  "Cloud & DevOps": [\n    {\n      "skill": "Docker",\n      "expertise": 8\n    },\n    {\n      "skill": "Kubernetes",\n      "expertise": 2\n    },\n    {\n      "skill": "Hyperledger Fabric",\n      "expertise": 8\n    },\n    {\n      "skill": "Git",\n      "expertise": 9\n    },\n    {\n      "skill": "GitHub",\n      "expertise": 9\n    },\n    {\n      "skill": "Linux",\n      "expertise": 8\n    },\n    {\n      "skill": "CI/CD",\n      "expertise": 7\n    },\n    {\n      "skill": "AWS",\n      "expertise": 2\n    }\n  ],\n  "Databases": [\n    {\n      "skill": "PostgreSQL",\n      "expertise": 7\n    },\n    {\n      "skill": "MongoDB",\n      "expertise": 7\n    },\n    {\n      "skill": "MySQL",\n      "expertise": 7\n    },\n    {\n      "skill": "Redis",\n      "expertise": 5\n    },\n    {\n      "skill": "Db2",\n      "expertise": 7\n    },\n    {\n      "skill": "Supabase",\n      "expertise": 6\n    }\n  ],\n  "Web Frameworks": [\n    {\n      "skill": "Nest.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "Django",\n      "expertise": 8\n    },\n    {\n      "skill": "Express.JS",\n      "expertise": 8\n    },\n    {\n      "skill": "Flask",\n      "expertise": 8\n    },\n    {\n      "skill": "Adonis.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "React",\n      "expertise": 8\n    },\n    {\n      "skill": "Tailwind",\n      "expertise": 8\n    }\n  ],\n  "Tools & Methodologies": [\n    {\n      "skill": "Jira",\n      "expertise": 8\n    },\n    {\n      "skill": "Confluence",\n      "expertise": 7\n    },\n    {\n      "skill": "Swagger",\n      "expertise": 4\n    },\n    {\n      "skill": "ClickUp",\n      "expertise": 7\n    },\n    {\n      "skill": "Agile",\n      "expertise": 9\n    },\n    {\n      "skill": "Scrum",\n      "expertise": 9\n    }\n  ]\n}\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify required/preferred technical skills, technologies, frameworks, and tools.\n    2. Start with the full candidate skill set, and remove only those skills that are clearly irrelevant to the job.\n    3. Within each category, prioritize:\n    - Direct matches from the job posting\n    - High expertise (score ‚â• 6)\n    - Industry-relevant and complementary skills\n    4. Ensure **at least 4‚Äì5 skills per category** (unless fewer exist in the candidate\'s set).\n    5. Cap each category at **8‚Äì10 skills maximum** to keep it concise.\n\n    OUTPUT FORMAT:\n    Generate a LaTeX technical skills section in this exact format:\n\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n    \\small{\\item{\n        \\textbf{AI / Machine Learning}{: [Selected AI/ML skills]} \\\\\n        \\textbf{Languages}{: [Selected programming languages]} \\\\\n        \\textbf{Cloud \\& DevOps}{: [Selected cloud/devops tools]} \\\\\n        \\textbf{Databases}{: [Selected database technologies]} \\\\\n        \\textbf{Web Frameworks}{: \\emph{Back-end}: [Backend frameworks]. \\emph{Front-end}: [Frontend frameworks]} \\\\\n        \\textbf{Tools \\& Methodologies}{: [Selected tools and methodologies]}\n    }}\n    \\end{itemize}\n\n    FORMATTING RULES:\n    - Only omit a category if the candidate truly has no relevant skills there.\n    - Within categories, sort skills by relevance to the job posting.\n    - Use LaTeX escaping (\\& for ampersands, etc.).\n    - Do not include expertise scores in the output.\n    - Keep categories ordered according to relevance to the job description.\n    - Output **only the LaTeX block**, nothing else.\n\n    Now generate the LaTeX technical skills section:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:18,025 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:18,026 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:18,026 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:18,026 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:18,026 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:18,026 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:23,975 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'5620'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'5803'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'28060'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'3.88s'), (b'x-request-id', b'req_ddd77b78ad584b209741de6e3c3544c7'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb375e3e840105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:23,995 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:24,012 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:24,012 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:24,012 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:24,012 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:24,013 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:23 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '5620', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '5803', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '28060', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '3.88s', 'x-request-id': 'req_ddd77b78ad584b209741de6e3c3544c7', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb375e3e840105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:24,018 [DEBUG] openai._base_client: request_id: req_ddd77b78ad584b209741de6e3c3544c7
2025-10-08 23:43:24,023 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-1d3ddbe0-1dfa-434f-bb44-04ac1d175fce', 'json_data': {'messages': [{'content': '\n    You are an expert resume strategist specializing in project selection for job applications.\n    \n    Your task is to analyze the job posting and select up to 4 most relevant projects from the candidate\'s portfolio that best demonstrate the skills and experience required for the position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S PROJECTS:\n    [\n  {\n    "title": "Federated Learning enabled Digital Twin",\n    "readme": "./projects/DigitalTwin.md",\n    "github": "https://github.com/amirrezaskh/DigitalTwin",\n    "description": "This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "PyTorch Lightning",\n      "PyTorch Forecasting",\n      "Pandas",\n      "Flask",\n      "Express.js",\n      "Docker",\n      "Hyperledger Fabric"\n    ]\n  },\n  {\n    "title": "Mini Task Manager",\n    "readme": "./projects/MiniTaskManager.md",\n    "github": "https://github.com/amirrezaskh/mini-task-manager",\n    "description": "A modern, full-stack task management application built with Django REST Framework and React. This project demonstrates clean architecture principles, secure token-based authentication, and a responsive Material-UI interface for managing personal tasks efficiently.",\n    "stack": [\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "React",\n      "TypeScript",\n      "Node.js",\n      "Material-UI",\n      "SQLite"\n    ]\n  },\n  {\n    "title": "Proof of Collaborative Learning (PoCL)",\n    "readme": "./projects/PoCL.md",\n    "github": "https://github.com/amirrezaskh/Proof-of-Collaborative-Learning",\n    "description": "This project implements PoCL, a novel blockchain consensus mechanism that replaces energy-intensive mining with a federated learning system. Miners collaboratively train a deep learning model, and winners are selected based on model performance through a democratic voting system. The system uses Hyperledger Fabric for the blockchain, a custom Python-based federated learning framework with TensorFlow, and Express.js as an API gateway.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "Flask",\n      "Node.js",\n      "Express.js",\n      "Hyperledger Fabric",\n      "Docker",\n      "JavaScript"\n    ]\n  },\n  {\n    "title": "Blockchain-enabled Personalized Federated Learning (BPFL)",\n    "readme": "./projects/BPFL.md",\n    "github": "https://github.com/amirrezaskh/BPFL",\n    "description": "This project implements a novel framework that combines personalized federated learning with blockchain technology. It addresses data heterogeneity and participant incentivization by using a token-based reward system and a contribution-weighted aggregation method. The system is built with Hyperledger Fabric for secure transactions and uses a custom federated learning framework with PyTorch to train models on datasets like MNIST and CIFAR-10.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Sharded and Blockchain-Enabled SplitFed Approaches",\n    "readme": "./projects/SSFL-BSFL.md",\n    "github": "https://github.com/amirrezaskh/SSFL-BSFL",\n    "description": "This project implements and compares four distributed machine learning frameworks: Split Learning (SL), SplitFed Learning (SFL), Sharded SplitFed Learning (SSFL), and Blockchain-enabled SplitFed Learning (BSFL). It evaluates these approaches under various conditions, including different numbers of participating nodes and scenarios with data poisoning attacks, with the goal of enhancing distributed and privacy-preserving machine learning.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Paper Summarizer",\n    "readme": "./projects/PaperSummarizer.md",\n    "github": "https://github.com/amirrezaskh/paper-summarizer",\n    "description": "An intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format. It uses object detection to identify document elements, OCR for text extraction, vision-language models for figure analysis, and large language models for generating comprehensive summaries.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "EasyOCR",\n      "Detectron2",\n      "PyTorch",\n      "Flask",\n      "Ollama",\n      "LLaVA",\n      "Llama 3.2",\n      "Pandoc"\n    ]\n  },\n  {\n    "title": "MarkMate",\n    "readme": "./projects/MarkMate.md",\n    "github": "https://github.com/amirrezaskh/markmate",\n    "description": "MarkMate is an AI-powered educational platform that automates assignment grading for instructors. It features a microservices architecture with a React frontend, a Django REST API backend with a PostgreSQL database, and a Flask-based LLM microservice that utilizes GPT-4 to provide automated grading based on custom rubrics.",\n    "stack": [\n      "React",\n      "React Router",\n      "Vite",\n      "Tailwind CSS",\n      "Node.js",\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "PostgreSQL",\n      "Flask",\n      "LangChain",\n      "GPT-4",\n      "PyPDF"\n    ]\n  },\n  {\n    "title": "CIFAR-10 Generative Model Evaluation",\n    "readme": "./projects/CIFAR-10 Generative Model Evaluation.md",\n    "github": "https://github.com/amirrezaskh/Unified-Image-Evaluation-Metric",\n    "description": "This project evaluates generative models, specifically diffusion models, on the CIFAR-10 dataset using a suite of custom metrics. It employs a pre-trained ResNet-50 classifier to extract features from both real and generated images and then computes Precision, Recall, Generalization Rate, and F1 Score based on nearest-neighbor analysis.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Torchvision",\n      "scikit-learn",\n      "NumPy",\n      "ResNet-50",\n      "Diffusion Models"\n    ]\n  },\n  {\n    "title": "PyFed",\n    "readme": "./projects/PyFed.md",\n    "github": "https://github.com/amirrezaskh/PyFed",\n    "description": "PyFed is an open-source, lightweight federated learning framework that facilitates the implementation of federated learning algorithms using standard TensorFlow datasets or custom preprocessed data. The framework uses a client-server architecture with sockets, processes, and threads, and currently supports the FedAvg policy.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "scikit-learn",\n      "NumPy",\n      "Sockets",\n      "Threads",\n      "Processes",\n      "TensorBoard"\n    ]\n  },\n  {\n    "title": "Facial Landmark & Boundary Detection",\n    "readme": "./projects/FacialBoundary.md",\n    "github": "https://github.com/amirrezaskh/Facial-Boundary-and-Facial-Landmarks-Detection-using-Convolutional-Neural-Networks",\n    "description": "This project implements a computer vision pipeline for real-time multi-face detection and landmark localization. It utilizes dual Faster R-CNN models\\u2014one trained to detect facial boundaries and the other to extract 68 facial landmarks\\u2014to provide a robust solution for real-time facial feature extraction.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "TensorFlow",\n      "Faster R-CNN",\n      "Neural Networks",\n      "Computer Vision"\n    ]\n  },\n  {\n    "title": "Aria",\n    "readme": "./projects/Aria.md",\n    "github": "https://github.com/amirrezaskh/aria",\n    "description": "Aria is an intelligent resume and cover letter generation system that leverages advanced AI to create tailored, professional documents based on specific job postings. Using LangChain workflows and OpenAI\'s GPT-4o-mini, it analyzes job requirements and dynamically generates customized resume sections, technical skills summaries, project selections, and highlight qualifications, all formatted in professional LaTeX for PDF compilation.",\n    "stack": [\n      "Python",\n      "LLM",\n      "LangChain",\n      "LangGraph",\n      "OpenAI GPT-4o-mini",\n      "OpenAI Embeddings",\n      "LaTeX",\n      "PyPDF",\n      "JSON",\n      "Regular Expressions",\n      "Jupyter Notebook"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Required technical skills and technologies\n       - Preferred experience areas\n       - Industry domain and problem types\n       - Project complexity and scale requirements\n       - Key competencies (full-stack, AI/ML, backend, etc.)\n\n    2. Evaluate each project based on:\n       - Technology stack alignment with job requirements\n       - Problem domain relevance to the job\'s industry\n       - Complexity and scale that demonstrates required skill level\n       - Unique value proposition that sets candidate apart\n       - Recency and relevance to current tech trends\n\n    3. Select up to 4 projects that collectively:\n       - Cover the most important job requirements\n       - Demonstrate progression and skill growth\n       - Show diverse but relevant capabilities\n       - Highlight unique expertise that differentiates the candidate\n\n    SELECTION CRITERIA (in order of importance):\n    1. Direct technology/framework matches (highest priority)\n    2. Problem domain alignment with job industry\n    3. Complexity level appropriate for the role\n    4. Demonstrates end-to-end project ownership\n    5. Shows innovation or unique technical solutions\n    6. Covers complementary skills mentioned in job posting\n    7. You are allowed to select even one project, as long as the selected projects are relevant to the job.\n\n    OUTPUT FORMAT:\n    Return ONLY a JSON list of the selected project titles, ordered by relevance to the job posting.\n    \n    Example format:\n    ["Project Title 1", "Project Title 2", "Project Title 3", "Project Title 4"]\n\n    Important: Return ONLY the JSON list, no additional text or explanation.\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:24,025 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:24,026 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:24,026 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:24,026 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:24,026 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:24,026 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:25,824 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'1534'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1703'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26783'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.434s'), (b'x-request-id', b'req_700c39cc719b4b238ff36238ac779197'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb3783b8a50105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:25,824 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:25,824 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:25,828 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:25,829 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:25,829 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:25,829 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '1534', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1703', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26783', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '6.434s', 'x-request-id': 'req_700c39cc719b4b238ff36238ac779197', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb3783b8a50105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:25,829 [DEBUG] openai._base_client: request_id: req_700c39cc719b4b238ff36238ac779197
2025-10-08 23:43:25,832 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-5e03519b-1274-45af-8a25-0db26b52a435', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Federated Learning enabled Digital Twin\n    Description: This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.\n    Tech Stack: Python, PyTorch, PyTorch Lightning, PyTorch Forecasting, Pandas, Flask, Express.js, Docker, Hyperledger Fabric\n    \n    Detailed Documentation:\n    # Blockchain-Enabled Federated Learning for Digital Twin\n\nA privacy-preserving federated learning system that combines **Hyperledger Fabric blockchain** with **Temporal Fusion Transformer (TFT)** models for multi-variate time-series forecasting in smart building environments.\n\n## üè¢ Project Overview\n\nThis project implements a novel **Digital Twin** system for smart building management using blockchain-enabled federated learning. We developed a **Temporal Fusion Transformer (TFT)** for predicting temperature, CO2, and humidity levels across **76 rooms** within a University of Manitoba smart building, while ensuring **data privacy** and **security** through decentralized training.\n\n### Key Innovation\n- **Privacy-Preserving**: Raw sensor data never leaves individual rooms\n- **Blockchain-Secured**: Hyperledger Fabric ensures trust and auditability\n- **Federated Learning**: Collaborative training without centralized data collection\n- **Real-World Deployment**: Tested with actual IoT sensor data from 76 building rooms\n\n## üèóÔ∏è System Architecture\n\nOur system combines three cutting-edge technologies:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Smart Rooms   ‚îÇ    ‚îÇ   Blockchain    ‚îÇ    ‚îÇ   Federated     ‚îÇ\n‚îÇ   (IoT Sensors) ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Network       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Learning      ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ (Hyperledger    ‚îÇ    ‚îÇ   (TFT Models)  ‚îÇ\n‚îÇ  üå°Ô∏è Temperature ‚îÇ    ‚îÇ  Fabric)        ‚îÇ    ‚îÇ                 ‚îÇ\n‚îÇ  üí® CO2 Levels  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ  üìä Aggregation ‚îÇ\n‚îÇ  üíß Humidity    ‚îÇ    ‚îÇ  üîí Security    ‚îÇ    ‚îÇ  ü§ñ AI Training ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Design Philosophy\nWe leverage the **Hyperledger Fabric framework** to create a permissioned blockchain network with custom **smart contracts (chaincodes)** that manage the federated learning lifecycle. Each room operates as an independent **federated node**, training local models on private sensor data while contributing to a global model through secure parameter sharing on the distributed ledger.\n\n**Key Components:**\n- **8 Federated Nodes**: Each representing different rooms with unique sensor data\n- **Blockchain Network**: Hyperledger Fabric with custom model-transfer chaincode\n- **Global Aggregator**: Implements FedAvg algorithm for model parameter aggregation\n- **Express Orchestrator**: Coordinates training rounds and system communication \n\n## üìä Experimental Results\n\nOur federated learning approach demonstrates **significant performance improvements** across all sensor types while maintaining **complete data privacy**. The results validate the effectiveness of blockchain-enabled collaborative learning in real-world IoT environments.\n\n### Training Performance\nWe evaluated the system's performance focusing on **local model accuracy** for each room's private dataset. The following visualizations demonstrate clear **loss reduction** and **model convergence** across 30 training rounds:\n\n#### Humidity Prediction Performance\n![Humidity \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:25,834 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:25,834 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:25,835 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:25,835 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:25,835 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:25,835 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:29,879 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3671'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3933'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25316'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.366s'), (b'x-request-id', b'req_a70bd6b2f426469cbf84a8e790491f10'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb378ef80a0105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:29,883 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:29,885 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:29,888 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:29,888 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:29,888 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:29,888 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:29 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3671', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3933', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25316', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.366s', 'x-request-id': 'req_a70bd6b2f426469cbf84a8e790491f10', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb378ef80a0105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:29,888 [DEBUG] openai._base_client: request_id: req_a70bd6b2f426469cbf84a8e790491f10
2025-10-08 23:43:29,899 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-f57a18f3-095f-4e5d-bff1-070d4302df8c', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Paper Summarizer\n    Description: An intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format. It uses object detection to identify document elements, OCR for text extraction, vision-language models for figure analysis, and large language models for generating comprehensive summaries.\n    Tech Stack: Python, OpenCV, EasyOCR, Detectron2, PyTorch, Flask, Ollama, LLaVA, Llama 3.2, Pandoc\n    \n    Detailed Documentation:\n    # Paper Summarizer\n\nAn intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format.\n\n## Overview\n\nThis project provides an end-to-end solution for automatically summarizing academic papers by:\n- Converting PDF pages to images\n- Using object detection to identify different document elements (text, titles, figures, tables)\n- Extracting text using OCR (Optical Character Recognition)\n- Analyzing figures with vision-language models\n- Generating comprehensive summaries using large language models\n\n## Architecture\n\nThe system consists of two main components:\n\n### 1. Client (`main.ipynb`)\nA Jupyter notebook that handles:\n- PDF processing and page extraction\n- Object detection using Detectron2 with Faster R-CNN\n- Text extraction using EasyOCR\n- Figure extraction and processing\n- Communication with the summarization server\n\n### 2. Server (`server.py`)\nA Flask-based API server that provides:\n- Figure analysis using LLaVA (Large Language and Vision Assistant)\n- Text summarization using Ollama with Llama 3.2\n- Page-wise and full-paper summarization\n- RESTful API endpoint for processing requests\n\n## Features\n\n- **Multi-modal Analysis**: Processes both text and visual elements (figures, tables)\n- **Intelligent Object Detection**: Identifies different document components with 70% confidence threshold\n- **OCR Text Extraction**: Supports English text recognition\n- **Figure Understanding**: Uses vision-language models to describe and summarize figures\n- **Hierarchical Summarization**: Creates page-level summaries before generating final paper summary\n- **PDF Output**: Converts final summary to PDF format using Pandoc\n\n## Requirements\n\n### Python Dependencies\n- `cv2` (OpenCV)\n- `easyocr`\n- `detectron2`\n- `pdf2image`\n- `requests`\n- `numpy`\n- `flask`\n- `ollama`\n- `transformers` (for LLaVA model)\n- `tqdm`\n\n### External Dependencies\n- **Pandoc**: For converting Markdown summary to PDF\n- **Faster R-CNN Model**: Pre-trained model file (`faster-rcnn.pth`)\n- **Ollama**: For running Llama 3.2 model locally\n\n### System Requirements\n- **GPU**: Recommended for faster processing (CPU fallback available)\n- **Memory**: Minimum 8GB RAM (16GB+ recommended for large papers)\n- **Storage**: Space for model files and temporary image processing\n\n## Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone <repository-url>\n   cd paper-summarizer\n   ```\n\n2. **Install Python dependencies**\n   ```bash\n   pip install opencv-python easyocr detectron2 pdf2image requests numpy flask transformers tqdm\n   ```\n\n3. **Install Ollama**\n   Follow instructions at [ollama.ai](https://ollama.ai) to install Ollama\n\n4. **Install Pandoc**\n   ```bash\n   # macOS\n   brew install pandoc\n   \n   # Ubuntu/Debian\n   sudo apt-get install pandoc\n   ```\n\n5. **Download required models**\n   - Place your Faster R-CNN model file as `faster-rcnn.pth` in the project directory\n   - The LLaVA model will \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Paper Summarizer} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:29,911 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:29,912 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:29,913 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:29,913 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:29,913 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:29,913 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:33,153 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3082'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3094'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25495'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.008s'), (b'x-request-id', b'req_a56a2495822b4c939c95e2810258e37a'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb37a87fff0105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:33,154 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:33,154 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:33,154 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:33,154 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:33,154 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:33,155 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3082', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3094', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25495', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.008s', 'x-request-id': 'req_a56a2495822b4c939c95e2810258e37a', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb37a87fff0105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:33,155 [DEBUG] openai._base_client: request_id: req_a56a2495822b4c939c95e2810258e37a
2025-10-08 23:43:33,157 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-b5e7889a-9dd7-48af-a28c-9cdf85deebea', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: MarkMate\n    Description: MarkMate is an AI-powered educational platform that automates assignment grading for instructors. It features a microservices architecture with a React frontend, a Django REST API backend with a PostgreSQL database, and a Flask-based LLM microservice that utilizes GPT-4 to provide automated grading based on custom rubrics.\n    Tech Stack: React, React Router, Vite, Tailwind CSS, Node.js, Django, Django REST Framework, Python, PostgreSQL, Flask, LangChain, GPT-4, PyPDF\n    \n    Detailed Documentation:\n    # MarkMate üë®üèª\u200düè´\n\nMarkMate is an AI-powered educational platform that revolutionizes assignment grading through intelligent automation. It combines modern web technologies with advanced language models to provide instructors with efficient, consistent, and detailed grading capabilities.\n\n## üåü Features\n\n- **AI-Powered Grading**: Automated assignment evaluation using GPT-4 with customizable rubrics\n- **Multi-Role Support**: Comprehensive role management for students, instructors, and administrators\n- **Course Management**: Complete course creation, enrollment, and administration system\n- **Assignment Workflow**: Full lifecycle management from assignment creation to submission grading\n- **Intelligent Document Processing**: PDF parsing and content analysis for assignments and submissions\n- **Dark Mode Support**: Modern, responsive UI with theme switching capabilities\n- **Real-time Updates**: Dynamic interface updates and progress tracking\n\n## üèóÔ∏è Architecture\n\nMarkMate follows a microservices architecture with three main components:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ\n‚îÇ   Frontend      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Backend       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   LLM Service   ‚îÇ\n‚îÇ   (React)       ‚îÇ    ‚îÇ   (Django)      ‚îÇ    ‚îÇ   (Flask)       ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n- **Frontend**: React application with modern UI components and routing\n- **Backend**: Django REST API with PostgreSQL database\n- **LLM Service**: Flask microservice integrating OpenAI GPT-4 for grading\n\n## üöÄ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- Node.js 18+\n- PostgreSQL 14+\n- OpenAI API Key\n\n### Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone <repository-url>\n   cd MarkMate\n   ```\n\n2. **Setup Backend**\n   ```bash\n   cd backend\n   pip install -r requirements.txt\n   python manage.py migrate\n   python manage.py runserver\n   ```\n\n3. **Setup Frontend**\n   ```bash\n   cd frontend\n   npm install\n   npm run dev\n   ```\n\n4. **Setup LLM Service**\n   ```bash\n   cd llm\n   pip install -r requirements.txt\n   # Add your OpenAI API key to .env file\n   python main.py\n   ```\n\n### Environment Variables\n\nCreate `.env` files in the `llm` directory:\n\n```env\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\n## üì± Usage\n\n1. **Access the application** at `http://localhost:5173`\n2. **Create an account** as an instructor or student\n3. **Create courses** and assignments with custom rubrics\n4. **Submit assignments** for automated grading\n5. **Review grades** and detailed feedback\n\n## üõ†Ô∏è Technology Stack\n\n### Frontend\n- **React 18**: Modern UI library with hooks\n- **React Router**: Client-side routing\n- **Tailwind CSS**: Utility-first CSS framework\n- **Vite**: Fast build tool and development server\n\n### Backend\n- **Django 5.1**: Python web framework\n- **Django REST Framework**: API development\n- **PostgreSQL**: Primary database\n- **T\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{MarkMate} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/markmate}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:33,158 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:33,158 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:33,159 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:33,159 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:33,159 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:33,159 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:37,872 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4586'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4611'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25127'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.745s'), (b'x-request-id', b'req_5d84d33fd42748c480b53995db4d9aa9'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb37bccdb80105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:37,873 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:37,873 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:37,882 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:37,883 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:37,883 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:37,884 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4586', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4611', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25127', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.745s', 'x-request-id': 'req_5d84d33fd42748c480b53995db4d9aa9', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb37bccdb80105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:37,884 [DEBUG] openai._base_client: request_id: req_5d84d33fd42748c480b53995db4d9aa9
2025-10-08 23:43:37,886 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-5a35536b-5de7-47f5-9a51-62e2534d28e8', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PROJECT DETAILS:\n    Title: Aria\n    Description: Aria is an intelligent resume and cover letter generation system that leverages advanced AI to create tailored, professional documents based on specific job postings. Using LangChain workflows and OpenAI\'s GPT-4o-mini, it analyzes job requirements and dynamically generates customized resume sections, technical skills summaries, project selections, and highlight qualifications, all formatted in professional LaTeX for PDF compilation.\n    Tech Stack: Python, LLM, LangChain, LangGraph, OpenAI GPT-4o-mini, OpenAI Embeddings, LaTeX, PyPDF, JSON, Regular Expressions, Jupyter Notebook\n    \n    Detailed Documentation:\n    # Aria - AI-Powered Resume & Cover Letter Generator\n\nAria is an intelligent resume and cover letter generation system that leverages LangChain, LangGraph, and RAG (Retrieval Augmented Generation) to create personalized, job-specific documents. The system analyzes job postings and automatically tailors resumes and cover letters using your personal knowledge base of papers, projects, experiences, and previous applications.\n\n## üöÄ Features\n\n- **Intelligent Resume Generation**: Automatically selects and formats the most relevant experiences, skills, and projects for each job\n- **Personalized Cover Letters**: Generates compelling cover letters using retrieved context from your knowledge base\n- **RAG-Enhanced Content**: Uses vector search to find relevant content from your papers, projects, and experiences\n- **LaTeX Output**: Produces professional, publication-ready PDFs\n- **Modular Architecture**: Built with industry-standard LangChain patterns for maintainability and extensibility\n- **LangGraph Workflows**: Orchestrates complex multi-step generation processes\n- **Vector Store Integration**: ChromaDB for efficient similarity search\n\n## üìã Table of Contents\n\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Architecture](#architecture)\n- [Configuration](#configuration)\n- [Usage](#usage)\n- [API Reference](#api-reference)\n- [License](#license)\n\n## üîß Installation\n\n### Prerequisites\n\n- Python 3.8+\n- OpenAI API key\n- LaTeX distribution (for PDF generation)\n- ChromaDB\n\n### Setup\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/amirrezaskh/Aria.git\n   cd Aria\n   ```\n\n2. **Install dependencies**\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Environment Configuration**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your configuration\n   ```\n\n4. **Required Environment Variables**\n   ```env\n   OPENAI_API_KEY=your_openai_api_key_here\n   OPENAI_MODEL=gpt-4o-mini\n   OPENAI_EMBEDDING_MODEL=text-embedding-3-large\n   \n   # Optional configurations\n   CHUNK_SIZE=1000\n   CHUNK_OVERLAP=200\n   NUM_DOCS=8\n   ```\n\n5. **Setup Data Directory Structure**\n   ```bash\n   mkdir -p data/{papers,projects,transcripts}\n   mkdir -p output/{resumes,cover_letters}\n   ```\n\n## üöÄ Quick Start\n\n### 1. Setup Vector Store (One-time)\n\nFirst, populate your knowledge base:\n\n```python\nfrom src.chains.context_retrieval_chain import ContextRetrievalChain\n\ndef setup_vector_store():\n    context_chain = ContextRetrievalChain()\n    \n    # Add your papers (PDFs)\n    context_chain.add_papers()\n    \n    # Add project documentation (Markdown)\n    context_chain.add_projects()\n    \n    # Add transcripts (JSON)\n    context_chain.add_transcripts()\n\nsetup_vector_store()\n```\n\n### 2. Generate Resume and Cover Letter\n\n```python\nfrom dotenv import load_dotenv\nfrom src.workflows.workflows import Workflows\nfrom src.workflows.states import ResumeState\n\nload_dotenv()\n\n# Create initial state\nstate = ResumeState(\n    job_posting="Your job posting here...",\n    company=\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project\'s tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Aria} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/aria}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:37,888 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:37,888 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:37,890 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:37,890 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:37,890 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:37,890 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:46,433 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'8084'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'8175'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25704'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'8.59s'), (b'x-request-id', b'req_f0795c63f31048249f8d053a02f6459d'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb37da59a30105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:46,435 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:46,436 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:46,436 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:46,437 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:46,437 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:46,437 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '8084', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '8175', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25704', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '8.59s', 'x-request-id': 'req_f0795c63f31048249f8d053a02f6459d', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb37da59a30105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:46,437 [DEBUG] openai._base_client: request_id: req_f0795c63f31048249f8d053a02f6459d
2025-10-08 23:43:46,444 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-66f419d6-a507-4ff0-b84a-664a87a214db', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in creating compelling "Highlight of Qualifications" sections that synthesize a candidate\'s experiences, skills, and projects into powerful qualification statements.\n    \n    Your task is to analyze the job posting and all provided resume content, then generate a LaTeX-formatted highlights section that positions the candidate as the ideal fit for the role.\n\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    CANDIDATE\'S EXPERIENCES:\n    \\resumeSubheading\n    {University of Manitoba}{Sep 2023 ‚Äì Jul 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed \\textbf{novel distributed AI architectures} (PoCL, SSFL, BSFL, BPFL), addressing challenges in distributed learning. Implemented and optimized these using \\textbf{TensorFlow}, \\textbf{PyTorch}, and \\textbf{Hyperledger Fabric}.}\n        \\resumeItem{Implemented PoCL, a \\textbf{blockchain consensus mechanism} that improved energy efficiency and enhanced system \\textbf{fault tolerance} by 62.7\\%.}\n        \\resumeItem{Conducted research resulting in publications for \\textbf{IEEE} journals on blockchain-driven personalization in federated learning.}\n        \\resumeItem{Reduced communication overhead by 85.2\\% through innovative system optimizations.}\n        \\resumeItem{Utilized a robust tech stack for model development, including \\textbf{Express.js} and \\textbf{Flask} for modular API integration.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Sadr Group Company}{Jul 2020 ‚Äì Dec 2020}\n    {Full-stack Developer Intern}{}\n    \\resumeItemListStart\n        \\resumeItem{Developed robust \\textbf{RESTful APIs} with \\textbf{AdonisJS} and \\textbf{Express.js}, implementing \\textbf{JWT authentication} for core application functionalities and enhancing \\textbf{API security}.}\n        \\resumeItem{Designed and implemented accurate \\textbf{MySQL database} schemas essential for tracking and managing business logic related to employee hours.}\n        \\resumeItem{Contributed to project success by deploying features on live servers, accompanied by in-depth \\textbf{Swagger API documentation}.}\n        \\resumeItem{Collaborated in an \\textbf{Agile Scrum} environment, ensuring integration into sprints and CI/CD pipelines, improving team \\textbf{efficiency}.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì Aug 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated backend functionality by designing \\textbf{RESTful APIs} using \\textbf{Supabase} and \\textbf{PostgreSQL}, improving \\textbf{data management} and integration.}\n        \\resumeItem{Automated data integration processes with a \\textbf{Python script}, converting CSV data to SQL, streamlining \\textbf{database population}.}\n        \\resumeItem{Collaborated with the front-end team to ensure seamless \\textbf{full-stack integration}, aligning API specifications with \\textbf{user interface requirements}.}\n        \\resumeItem{Enhanced product development efficiency through \\textbf{agile workflow}, using the \\textbf{Atlassian suite} for task management and documentation.}\n    \\resumeItemListEnd\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: TensorFlow, PyTorch, Transformers, LangChain, LangGraph, Numpy, Pandas, Scikit-learn} \\\\\n    \\textbf{Languages}{: Python, Java, JavaScript, SQL, C++} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Git, GitHub, CI/CD, AWS, Linux} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Db2, Redis} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum, ClickUp}\n}}\n\\end{itemize}\n\n    CANDIDATE\'S PROJECTS:\n    \\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Flask, Hyperledger Fabric} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a \\textbf{blockchain-enabled federated learning} system using \\textbf{Hyperledger Fabric} to ensure \\textbf{data privacy} and \\textbf{security} for smart building management, enhancing IoT sensor data collection without compromising privacy.}\n        \\resumeItem{Implemented \\textbf{Temporal Fusion Transformer (TFT)} models with \\textbf{PyTorch} to effectively predict temperature, CO2, and humidity across 76 smart building rooms, utilizing \\textbf{Docker} for containerization and efficient deployment.}\n        \\resumeItem{Achieved a significant reduction in prediction error rates by \\textbf{20\\%} through collaborative \\textbf{decentralized training}, validating improved model convergence and performance in real-world environments.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Paper Summarizer} $|$ \\emph{Python, Detectron2, EasyOCR, Flask, Llama 3.2} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an intelligent summarization system using \\textbf{Python} and \\textbf{Detectron2} for object detection to automate analysis of academic papers, improving processing time by 30\\%.}\n        \\resumeItem{Implemented text extraction with \\textbf{EasyOCR} and integrated \\textbf{Flask} for a scalable RESTful API, enabling seamless communication between client and server components.}\n        \\resumeItem{Enhanced comprehension and output quality by utilizing \\textbf{Llama 3.2} for generating concise summaries, leading to more accurate information retrieval from dense PDFs.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{MarkMate} $|$ \\emph{Django, Flask, GPT-4, Python, PostgreSQL, LangChain} $|$ \\href{https://github.com/amirrezaskh/markmate}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an \\textbf{AI-Powered Grading} service using \\textbf{GPT-4} and \\textbf{Flask} to automate assignment evaluations, significantly reducing grading time by 80\\%.}\n        \\resumeItem{Implemented a robust \\textbf{Django} REST API backend with secure data access to manage grading workflows and user roles, supporting up to 1,000 concurrent users.}\n        \\resumeItem{Integrated \\textbf{PostgreSQL} for persistent data storage and efficient query performance, ensuring reliable and scalable data handling for assignment and course data management.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Aria} $|$ \\emph{Python, LangChain, OpenAI GPT-4o-mini, LangGraph, RAG} $|$ \\href{https://github.com/amirrezaskh/Aria}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an \\textbf{intelligent AI} system using \\textbf{LangChain} to automatically generate personalized resumes and cover letters, effectively tailoring content to \\textbf{specific job postings}.}\n        \\resumeItem{Implemented \\textbf{vector search} with \\textbf{OpenAI GPT-4o-mini} and \\textbf{LangGraph} to enhance document customization, ensuring highly relevant content generation and improving suitability to \\textbf{target job requirements}.}\n        \\resumeItem{Achieved a significant time reduction in document creation through integrated \\textbf{Retrieval Augmented Generation (RAG)}, enabling efficient content retrieval and context application, thereby enhancing the operational efficiency of \\textbf{AI/ML model deployment}.}\n    \\resumeItemListEnd\n\n### Explanation:\n- **Technologies:** Focused on Python, LangChain, OpenAI GPT-4o-mini, LangGraph, and RAG, which are relevant to the AI and ML focus within the job description.\n- **Achievements:** Highlighted the system\'s ability to generate personalized documents tailored to specific job postings, which resonates with the job‚Äôs need for operationalizing AI/ML models.\n- **Implementation Details:** Used vector search and RAG to enhance the relevance and suitability of generated documents, aligning with the job\'s requirement for AI/ML solution integration and deployment.\n- **Impact:** Emphasized efficiency improvements, echoing the influence of AI systems on business processes, which ties back to the operational responsibilities mentioned in the job posting.\n\nThis entry effectively aligns with the job posting, showcasing key relevant skills and technologies in a concise, impactful manner.\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify the most critical qualifications and requirements\n    2. Review all the candidate\'s content (experiences, skills, projects) to extract relevant strengths\n    3. Synthesize this information into 5-7 compelling qualification highlights\n    4. Generate a LaTeX highlights section following this EXACT format:\n\n    \\resumeItem{\\textbf{Domain Area 1:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 2:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 3:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 4:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 5:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n\n    EXAMPLE OUTPUT:\n    \\resumeItem{\\textbf{Machine Learning \\& AI:} 5+ years developing \\textbf{deep learning} models using \\textbf{PyTorch} \\& \\textbf{TensorFlow} with 95\\% accuracy improvements.}\n\n    Note: Always use \\& instead of & for ampersands in LaTeX text.\n\n    FORMATTING GUIDELINES:\n    - Each highlight should start with a \\textbf{domain area} that matches job requirements\n    - Bold all technical skills, technologies, frameworks, and methodologies using \\textbf{}\n    - Include specific technologies and techniques mentioned in experiences and projects\n    - Quantify achievements where possible (percentages, scale, impact - use \\% for percentages in LaTeX)\n    - Use strong, confident language that demonstrates expertise\n    - Each highlight should be 1-2 lines maximum for readability\n    - Order highlights by importance to the job posting\n    - IMPORTANT: Use proper LaTeX escaping - write \\& instead of & for ampersands in text\n\n    Generate the LaTeX highlight of qualifications:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:46,446 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:46,446 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:46,446 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:46,446 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:46,447 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:46,447 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:53,787 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'7231'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'7247'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26356'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'7.286s'), (b'x-request-id', b'req_dd60d30da96b4dc19b60aceec6ac66fc'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb380fdca20105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:53,789 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:53,790 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:53,805 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:53,805 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:53,806 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:53,806 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '7231', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '7247', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26356', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '7.286s', 'x-request-id': 'req_dd60d30da96b4dc19b60aceec6ac66fc', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb380fdca20105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:53,806 [DEBUG] openai._base_client: request_id: req_dd60d30da96b4dc19b60aceec6ac66fc
2025-10-08 23:43:54,551 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:43:54,557 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:43:54,558 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:43:54,558 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:43:54,558 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:43:54,558 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:43:54,561 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe366d0>
2025-10-08 23:43:54,561 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,561 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,561 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,561 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,561 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,573 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:43:54 GMT')])
2025-10-08 23:43:54,574 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:43:54,574 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,574 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,574 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,574 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,575 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:43:54,581 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:43:54,582 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:43:54,582 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:43:54,582 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:43:54,582 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:43:54,582 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe61a50>
2025-10-08 23:43:54,583 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,583 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,583 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,583 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,583 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,591 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:43:54 GMT')])
2025-10-08 23:43:54,592 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:43:54,593 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,593 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,593 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,593 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,594 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,594 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,594 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,594 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,594 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:43:54,595 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:43:54 GMT')])
2025-10-08 23:43:54,595 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:43:54,595 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:43:54,595 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,595 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,595 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,599 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,599 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,599 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,599 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,599 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,606 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:43:54 GMT')])
2025-10-08 23:43:54,606 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:43:54,606 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,606 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,606 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,606 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,612 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-23af550b-13ec-4f4c-9dfa-313ce3201ec8', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe3a3e0>, 'json_data': {'input': [[10714, 279, 2683, 198, 5618, 5296, 11, 420, 374, 264, 2539, 7394, 320, 1806, 13, 20, 4207, 824, 2046, 8, 3560, 3196, 704, 315, 14974, 11, 6328, 320, 8671, 16621, 570, 4815, 2170, 459, 15592, 25922, 11, 499, 690, 387, 961, 315, 279, 15592, 7661, 12899, 279, 4500, 315, 2626, 4791, 15204, 12023, 320, 33, 17, 34, 8, 8522, 77582, 15592, 520, 426, 10754, 13, 1472, 690, 990, 449, 828, 14248, 505, 426, 10754, 753, 19758, 612, 8184, 50599, 1912, 311, 2274, 11, 1296, 11, 10739, 323, 10519, 15592, 14, 2735, 4211, 304, 426, 10754, 753, 9624, 22484, 13, 1472, 690, 990, 15499, 449, 13707, 4028, 279, 11050, 8398, 25927, 320, 14934, 50, 2883, 25, 426, 10754, 2361, 25, 15592, 25922, 11156, 7512, 25, 1144, 7413, 90, 1224, 553, 44489, 2414, 9113, 28, 15, 13, 868, 258, 11, 2440, 1185, 58420, 59, 9181, 36802, 1224, 517, 262, 1144, 1342, 13536, 90, 15836, 611, 13257, 21579, 15523, 25, 96086, 11, 5468, 51, 22312, 11, 81632, 11, 23272, 19368, 11, 23272, 11461, 11, 452, 6895, 11, 34606, 300, 11, 2522, 61503, 12, 12964, 92, 91255, 262, 1144, 1342, 13536, 90, 60386, 15523, 25, 13325, 11, 8102, 11, 13210, 11, 8029, 11, 356, 1044, 92, 91255, 262, 1144, 1342, 13536, 90, 16440, 1144, 5, 6168, 40004, 15523, 25, 41649, 11, 21804, 11, 33195, 11, 21351, 14, 6620, 11, 24124, 11, 14677, 92, 91255, 262, 1144, 1342, 13536, 90, 35, 24760, 15523, 25, 74701, 11, 46428, 11, 27436, 11, 12257, 17, 11, 35258, 92, 91255, 262, 1144, 1342, 13536, 90, 6109, 24686, 82, 15523, 25, 1144, 336, 764, 90, 3792, 13368, 16487, 53704, 11, 29273, 11, 17855, 3587, 50, 13, 1144, 336, 764, 90, 24284, 13368, 16487, 3676, 92, 91255, 262, 1144, 1342, 13536, 90, 16992, 1144, 5, 6872, 9268, 15523, 25, 622, 9008, 11, 1221, 41116, 11, 83284, 11, 2522, 10952, 11, 9369, 2378, 534, 11498, 59, 408, 90, 1224, 553, 92, 3217, 25, 1144, 42495, 3214, 11666, 198, 262, 314, 31272, 315, 64340, 15523, 42214, 220, 2366, 18, 1389, 10263, 220, 2366, 20, 534, 262, 314, 24956, 6426, 8483, 22103, 15523, 54, 6258, 47984, 11, 64340, 11, 7008, 534, 262, 1144, 42495, 83974, 3563, 198, 286, 1144, 42495, 1256, 90, 78233, 323, 8040, 1144, 1342, 13536, 90, 39142, 301, 4332, 15592, 78335, 92, 320, 34004, 3218, 11, 18679, 6254, 11, 28718, 6254, 11, 30167, 6254, 705, 28118, 11774, 304, 2916]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:43:54,612 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:43:54,612 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:43:54,645 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe33050>
2025-10-08 23:43:54,645 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10fa616d0> server_hostname='api.openai.com' timeout=None
2025-10-08 23:43:54,681 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe30510>
2025-10-08 23:43:54,681 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,681 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,681 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,681 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,681 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,934 [DEBUG] urllib3.connectionpool: Resetting dropped connection: us.i.posthog.com
2025-10-08 23:43:54,968 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'122'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5dbdc66675-68r9t'), (b'x-envoy-upstream-service-time', b'176'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999602'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'23ms'), (b'x-request-id', b'req_eb98a9acf05f41a982e2ac2520240d62'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=5HR2AacPYudNzTcM4CXbbENPspIIyVi.A2K50rZ87Ns-1759985035-1.0.1.1-Msb4GyAS4D_G6fYBDJ4ntkk.1kUFIdkQNt4RRAK4jOyBzg5DusQgNOY2avUhpBoGjaHijGNjKWAeLu4IVnB0MUzZS0rGbKW2KT4zJH_z5.g; path=/; expires=Thu, 09-Oct-25 05:13:55 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=7JsHvf8htkOdfsBmHattdd3wENcF3Se4FysID8u6SRc-1759985035022-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb38434b26231c-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:54,969 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:43:54,969 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,971 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,971 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,971 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,971 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:43:55 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '122'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-5dbdc66675-68r9t'), ('x-envoy-upstream-service-time', '176'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999602'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '23ms'), ('x-request-id', 'req_eb98a9acf05f41a982e2ac2520240d62'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=5HR2AacPYudNzTcM4CXbbENPspIIyVi.A2K50rZ87Ns-1759985035-1.0.1.1-Msb4GyAS4D_G6fYBDJ4ntkk.1kUFIdkQNt4RRAK4jOyBzg5DusQgNOY2avUhpBoGjaHijGNjKWAeLu4IVnB0MUzZS0rGbKW2KT4zJH_z5.g; path=/; expires=Thu, 09-Oct-25 05:13:55 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=7JsHvf8htkOdfsBmHattdd3wENcF3Se4FysID8u6SRc-1759985035022-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb38434b26231c-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:43:54,971 [DEBUG] openai._base_client: request_id: req_eb98a9acf05f41a982e2ac2520240d62
2025-10-08 23:43:54,972 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,972 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,972 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,973 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,973 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,986 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'7178'), (b'date', b'Thu, 09 Oct 2025 04:43:54 GMT')])
2025-10-08 23:43:54,986 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-08 23:43:54,986 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,986 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:54,987 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:54,987 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:54,989 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-4c81f2e3-7e88-426e-85ed-c448b73dc8b6', 'json_data': {'messages': [{'content': '\n    You are an expert cover letter writer specializing in creating compelling, personalized cover letters that effectively connect a candidate\'s background to specific job opportunities.\n\n    Your task is to analyze the job posting, personalized resume, and retrieved context to generate ONLY the cover letter content paragraphs in plain LaTeX format.\n\n    POSITION: AI Developer\n    COMPANY: BMO\n    JOB POSTING:\n    About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred\n\n    PERSONALIZED RESUME CONTENT:\n    Highlights: \\resumeItem{\\textbf{AI/ML Development:} Proven track record in designing novel \\textbf{distributed AI architectures} using \\textbf{TensorFlow}, \\textbf{PyTorch}, and \\textbf{Hyperledger Fabric} with a focus on \\textbf{blockchain} integration to enhance \\textbf{data privacy} and system \\textbf{fault tolerance}. Achieved a 62.7\\% improvement in energy efficiency.}\n\\resumeItem{\\textbf{Cloud Technologies:} Extensive experience with \\textbf{AWS} and \\textbf{Azure} services, deploying AI/ML models using \\textbf{AWS Sagemaker} and \\textbf{Azure AI services}. Developed and maintained cloud-based applications, ensuring seamless integration with business systems.}\n\\resumeItem{\\textbf{Data Pipelines \\& Integration:} Skilled in building and optimizing \\textbf{data pipelines} using \\textbf{Python} and \\textbf{Flask} for backend functionalities, streamlining data integration processes by 85.2\\%. Automated data workflows to enhance database population and management efficiency.}\n\\resumeItem{\\textbf{Full-Stack Development:} Expertise in creating scalable \\textbf{RESTful APIs} using \\textbf{Express.js}, \\textbf{Flask}, and \\textbf{Django}, ensuring robust backend solutions and \\textbf{JWT authentication} to enhance security and efficiency of application functionalities.}\n\\resumeItem{\\textbf{Generative AI \\& RAG:} Proficient with \\textbf{LangChain} and \\textbf{LangGraph} frameworks, leveraging \\textbf{OpenAI GPT-4} for personalized content generation tailored to job requirements. Achieved significant time reduction in document creation and improved content relevance.}\n\\resumeItem{\\textbf{DevOps/MLOps:} Solid foundation in \\textbf{CI/CD} practices and version control using \\textbf{Git}, \\textbf{GitHub}, and \\textbf{Docker}, supporting seamless integration and deployment of AI/ML solutions within Agile environments, enhancing development \\& operational efficiencies.}\n    Experiences: \\resumeSubheading\n    {University of Manitoba}{Sep 2023 ‚Äì Jul 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed \\textbf{novel distributed AI architectures} (PoCL, SSFL, BSFL, BPFL), addressing challenges in distributed learning. Implemented and optimized these using \\textbf{TensorFlow}, \\textbf{PyTorch}, and \\textbf{Hyperledger Fabric}.}\n        \\resumeItem{Implemented PoCL, a \\textbf{blockchain consensus mechanism} that improved energy efficiency and enhanced system \\textbf{fault tolerance} by 62.7\\%.}\n        \\resumeItem{Conducted research resulting in publications for \\textbf{IEEE} journals on blockchain-driven personalization in federated learning.}\n        \\resumeItem{Reduced communication overhead by 85.2\\% through innovative system optimizations.}\n        \\resumeItem{Utilized a robust tech stack for model development, including \\textbf{Express.js} and \\textbf{Flask} for modular API integration.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Sadr Group Company}{Jul 2020 ‚Äì Dec 2020}\n    {Full-stack Developer Intern}{}\n    \\resumeItemListStart\n        \\resumeItem{Developed robust \\textbf{RESTful APIs} with \\textbf{AdonisJS} and \\textbf{Express.js}, implementing \\textbf{JWT authentication} for core application functionalities and enhancing \\textbf{API security}.}\n        \\resumeItem{Designed and implemented accurate \\textbf{MySQL database} schemas essential for tracking and managing business logic related to employee hours.}\n        \\resumeItem{Contributed to project success by deploying features on live servers, accompanied by in-depth \\textbf{Swagger API documentation}.}\n        \\resumeItem{Collaborated in an \\textbf{Agile Scrum} environment, ensuring integration into sprints and CI/CD pipelines, improving team \\textbf{efficiency}.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì Aug 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated backend functionality by designing \\textbf{RESTful APIs} using \\textbf{Supabase} and \\textbf{PostgreSQL}, improving \\textbf{data management} and integration.}\n        \\resumeItem{Automated data integration processes with a \\textbf{Python script}, converting CSV data to SQL, streamlining \\textbf{database population}.}\n        \\resumeItem{Collaborated with the front-end team to ensure seamless \\textbf{full-stack integration}, aligning API specifications with \\textbf{user interface requirements}.}\n        \\resumeItem{Enhanced product development efficiency through \\textbf{agile workflow}, using the \\textbf{Atlassian suite} for task management and documentation.}\n    \\resumeItemListEnd\n    Skills: \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: TensorFlow, PyTorch, Transformers, LangChain, LangGraph, Numpy, Pandas, Scikit-learn} \\\\\n    \\textbf{Languages}{: Python, Java, JavaScript, SQL, C++} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Git, GitHub, CI/CD, AWS, Linux} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Db2, Redis} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum, ClickUp}\n}}\n\\end{itemize}\n    Projects: \\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Flask, Hyperledger Fabric} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a \\textbf{blockchain-enabled federated learning} system using \\textbf{Hyperledger Fabric} to ensure \\textbf{data privacy} and \\textbf{security} for smart building management, enhancing IoT sensor data collection without compromising privacy.}\n        \\resumeItem{Implemented \\textbf{Temporal Fusion Transformer (TFT)} models with \\textbf{PyTorch} to effectively predict temperature, CO2, and humidity across 76 smart building rooms, utilizing \\textbf{Docker} for containerization and efficient deployment.}\n        \\resumeItem{Achieved a significant reduction in prediction error rates by \\textbf{20\\%} through collaborative \\textbf{decentralized training}, validating improved model convergence and performance in real-world environments.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Paper Summarizer} $|$ \\emph{Python, Detectron2, EasyOCR, Flask, Llama 3.2} $|$ \\href{https://github.com/amirrezaskh/paper-summarizer}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an intelligent summarization system using \\textbf{Python} and \\textbf{Detectron2} for object detection to automate analysis of academic papers, improving processing time by 30\\%.}\n        \\resumeItem{Implemented text extraction with \\textbf{EasyOCR} and integrated \\textbf{Flask} for a scalable RESTful API, enabling seamless communication between client and server components.}\n        \\resumeItem{Enhanced comprehension and output quality by utilizing \\textbf{Llama 3.2} for generating concise summaries, leading to more accurate information retrieval from dense PDFs.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{MarkMate} $|$ \\emph{Django, Flask, GPT-4, Python, PostgreSQL, LangChain} $|$ \\href{https://github.com/amirrezaskh/markmate}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an \\textbf{AI-Powered Grading} service using \\textbf{GPT-4} and \\textbf{Flask} to automate assignment evaluations, significantly reducing grading time by 80\\%.}\n        \\resumeItem{Implemented a robust \\textbf{Django} REST API backend with secure data access to manage grading workflows and user roles, supporting up to 1,000 concurrent users.}\n        \\resumeItem{Integrated \\textbf{PostgreSQL} for persistent data storage and efficient query performance, ensuring reliable and scalable data handling for assignment and course data management.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Aria} $|$ \\emph{Python, LangChain, OpenAI GPT-4o-mini, LangGraph, RAG} $|$ \\href{https://github.com/amirrezaskh/Aria}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed an \\textbf{intelligent AI} system using \\textbf{LangChain} to automatically generate personalized resumes and cover letters, effectively tailoring content to \\textbf{specific job postings}.}\n        \\resumeItem{Implemented \\textbf{vector search} with \\textbf{OpenAI GPT-4o-mini} and \\textbf{LangGraph} to enhance document customization, ensuring highly relevant content generation and improving suitability to \\textbf{target job requirements}.}\n        \\resumeItem{Achieved a significant time reduction in document creation through integrated \\textbf{Retrieval Augmented Generation (RAG)}, enabling efficient content retrieval and context application, thereby enhancing the operational efficiency of \\textbf{AI/ML model deployment}.}\n    \\resumeItemListEnd\n\n### Explanation:\n- **Technologies:** Focused on Python, LangChain, OpenAI GPT-4o-mini, LangGraph, and RAG, which are relevant to the AI and ML focus within the job description.\n- **Achievements:** Highlighted the system\'s ability to generate personalized documents tailored to specific job postings, which resonates with the job‚Äôs need for operationalizing AI/ML models.\n- **Implementation Details:** Used vector search and RAG to enhance the relevance and suitability of generated documents, aligning with the job\'s requirement for AI/ML solution integration and deployment.\n- **Impact:** Emphasized efficiency improvements, echoing the influence of AI systems on business processes, which ties back to the operational responsibilities mentioned in the job posting.\n\nThis entry effectively aligns with the job posting, showcasing key relevant skills and technologies in a concise, impactful manner.\n\n    RETRIEVED CONTEXT FROM KNOWLEDGE BASE:\n    [Document(id=\'645f87c1-7931-4aad-ae8a-145e118414c9\', metadata={\'position\': \'Python Developer\', \'source\': \'cover letter\', \'company\': \'Ascendion\', \'start_index\': 507}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'8cd7ca4e-dc41-42e4-a9b8-c8b1250e6183\', metadata={\'company\': \'Ascendion\', \'start_index\': 507, \'position\': \'Python Developer\', \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'a1bfdd36-5dad-4d9b-bb94-46b08067f5bf\', metadata={\'position\': \'Python Developer\', \'company\': \'Apple\', \'source\': \'cover letter\', \'start_index\': 511}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'4bd89d99-19d6-45a8-a6ff-7184a911e904\', metadata={\'company\': \'Apple\', \'start_index\': 513, \'position\': \'Python Developer\', \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'c3d0e739-8890-42c5-9860-ad934a519018\', metadata={\'position\': \'Python Developer\', \'start_index\': 1068, \'company\': \'Ascendion\', \'source\': \'cover letter\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s dedication to creating opportunities and fostering inclusion. My experience in developing advanced AI models and implementing distributed systems using \\\\textbf{TensorFlow} and \\\\textbf{PyTorch} supports these initiatives. Furthermore, my projects such as the \\\\textbf{Federated Learning enabled Digital Twin} demonstrate my ability to innovate within the realm of \\\\textbf{blockchain} and IoT, relevant areas that can bolster Ascendion\'s engineering solutions for Fortune 500 clients.\\n\\nI am enthusiastic about the prospect of contributing to Ascendion\'s mission by leveraging my skills to build advanced digital solutions. I am confident that my background in \\\\textbf{distributed systems} and my passion for \\\\textbf{digital engineering} will prove valuable to your high-performing team. I am eager to bring my unique perspective and technical expertise to Ascendion, collaborating to engineer technology that elevates life."), Document(id=\'a3aa52f0-b05f-4d47-a377-4b43ff140cfd\', metadata={\'source\': \'cover letter\', \'position\': \'Python Developer\', \'start_index\': 1068, \'company\': \'Ascendion\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s dedication to creating opportunities and fostering inclusion. My experience in developing advanced AI models and implementing distributed systems using \\\\textbf{TensorFlow} and \\\\textbf{PyTorch} supports these initiatives. Furthermore, my projects such as the \\\\textbf{Federated Learning enabled Digital Twin} demonstrate my ability to innovate within the realm of \\\\textbf{blockchain} and IoT, relevant areas that can bolster Ascendion\'s engineering solutions for Fortune 500 clients.\\n\\nI am enthusiastic about the prospect of contributing to Ascendion\'s mission by leveraging my skills to build advanced digital solutions. I am confident that my background in \\\\textbf{distributed systems} and my passion for \\\\textbf{digital engineering} will prove valuable to your high-performing team. I am eager to bring my unique perspective and technical expertise to Ascendion, collaborating to engineer technology that elevates life."), Document(id=\'19bd50c7-d116-4802-9a2f-9c023f304b19\', metadata={\'source\': \'cover letter\', \'company\': \'Ascendion\', \'start_index\': 1029, \'position\': \'Python Developer\'}, page_content="I am particularly drawn to \\\\textbf{Ascendion}\'s commitment to creating technology that elevates life and your emphasis on digital engineering for Fortune 500 clients. My experience in \\\\textbf{distributed systems} and machine learning provides me with the skills needed to excel in this role. I am excited about the prospect of applying cutting-edge AI and machine learning technologies to further Ascendion\'s innovative goals and solve complex problems for your clients.\\n\\nI am enthusiastic about the opportunity to contribute to \\\\textbf{Ascendion}\'s mission by bringing my skills to build advanced digital solutions. With my strong background in Python and related technologies, I am confident that I can make immediate and significant contributions to your team, helping Ascendion deliver captivating and cutting-edge solutions."), Document(id=\'f2931001-67a9-4907-b8b2-08da7871aae3\', metadata={\'position\': \'Junior BI developer\', \'source\': \'cover letter\', \'company\': \'Dexian\', \'start_index\': 457}, page_content="In my recent role as a Full-stack Developer Intern at Bobo, I accelerated product development by designing and implementing RESTful APIs using \\\\textbf{Supabase} and \\\\textbf{PostgreSQL}, which enhanced backend functionality and data management for customer analytics applications. Additionally, I automated data integration processes using Python scripting, streamlining workflows and directly contributing to improved data handling capabilities. My hands-on experience with \\\\textbf{SQL} and data modeling aligns with Dexian\'s requirement for transforming raw data into organized models and creating compelling Power BI dashboards.")]\n                                                       \n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Company mission, values, and culture\n       - Key technical requirements and preferred technologies\n       - Required experience level and responsibilities\n       - Specific skills and qualifications sought\n\n    2. Use the personalized resume content to:\n       - Highlight the most relevant experiences and achievements\n       - Showcase technical skills that match job requirements\n       - Reference specific projects that demonstrate required capabilities\n       - Quantify accomplishments where possible\n\n    3. Leverage the retrieved context to:\n       - Add depth and authenticity to your statements\n       - Reference relevant research, projects, or experiences not in the resume\n       - Demonstrate domain knowledge and passion for the field\n       - Show progression and learning from past experiences\n\n    4. Follow the template structure but adapt content to:\n       - Match the company\'s tone and industry language\n       - Create a narrative that connects past experiences to future contributions\n       - Show genuine enthusiasm for the specific role and company\n       - Address any potential gaps or explain career transitions\n\n\n    CRITICAL OUTPUT REQUIREMENTS:\n    - Generate ONLY the cover letter content paragraphs\n    - DO NOT include any LaTeX document structure (\\documentclass, \\begin{document}, \\begin{letter}, \\end{letter}, \\end{document})\n    - DO NOT include any explanations, introductions, or meta-commentary\n    - DO NOT include salutations like "Dear Hiring Manager" or closings like "Sincerely"\n    - Start directly with the first paragraph content\n    - End with the last paragraph content\n    - Use clean paragraph breaks (double newlines) between paragraphs\n\n    CONTENT STRUCTURE:\n    Write exactly 3-4 paragraphs of cover letter content:\n\n    Paragraph 1: Express enthusiasm for the specific role and company, briefly state your relevant background and why you\'re applying.\n\n    Paragraph 2: Highlight your most relevant technical experiences and projects with specific examples that match the job requirements.\n\n    Paragraph 3: Demonstrate alignment with the company\'s mission and technology stack, referencing retrieved context to show deeper knowledge and genuine interest.\n\n    Paragraph 4 (optional): Reiterate enthusiasm and mention specific contributions you can make to the team and company.\n\n                                                       \n    COVER LETTER STRUCTURE:\n    1. Opening paragraph: Express enthusiasm for the specific role and company, briefly state your relevant background\n    2. Body paragraph 1: Highlight relevant technical experience and projects with specific examples\n    3. Body paragraph 2: Demonstrate alignment with company mission/technology stack, reference retrieved context for depth\n    4. Closing paragraph: Reiterate enthusiasm, mention specific contributions you can make, professional closing\n\n    TONE AND STYLE:\n    - Professional yet enthusiastic\n    - Confident but not boastful\n    - Specific and detailed, not generic\n    - Forward-looking and solution-oriented\n    - Authentic and genuine\n    LATEX FORMATTING REQUIREMENTS:\n    - Use proper LaTeX escaping: \\& instead of & for ampersands\n    - Use \\% for percentages in text  \n    - Use \\textbf{} for emphasizing key technologies, company names, and achievements\n    - Write in clean, readable paragraph format\n    - Use standard paragraph separation (double newlines)\n\n    CONTENT GUIDELINES:\n    - Reference specific technologies, frameworks, and methodologies from the job posting\n    - Use metrics and quantified achievements where available\n    - Show understanding of the company\'s challenges and how you can help\n    - Demonstrate continuous learning and adaptability\n    - Connect past experiences to future potential contributions\n    - Use retrieved context to add unique insights or relevant background\n    - Be professional yet enthusiastic, confident but not boastful\n    - Make it specific and detailed, not generic\n\n    OUTPUT FORMAT EXAMPLE:\n    I am excited to apply for the {position} position at \\textbf{{company name}}. With my background in \\textbf{relevant technology} and experience in \\textbf{relevant domain}, I am eager to contribute to your team\'s mission of advancing \\textbf{company focus area}.\n\n    In my recent projects, I have built scalable applications using \\textbf{specific technologies} that align with your requirements. For example, in my \\textbf{project name} project, I developed \\textbf{specific implementation} that resulted in \\textbf{quantified outcome}. My experience with \\textbf{relevant technology stack} has prepared me to tackle the challenges outlined in your job posting.\n\n    I am particularly drawn to \\textbf{{company name}}\'s commitment to \\textbf{company values/mission}. My research in \\textbf{relevant area from context} has given me deep insights into \\textbf{relevant domain knowledge}, and I am excited about the opportunity to apply this knowledge in a production environment where I can help \\textbf{specific company goals}.\n\n    Generate the cover letter content paragraphs following this exact format:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:43:54,990 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:43:54,990 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:43:54,990 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:43:54,990 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:43:54,991 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:43:54,991 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:43:55,170 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:43:59,162 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:43:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3989'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4072'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'23622'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'12.756s'), (b'x-request-id', b'req_d9a42d90b35e4e27a16658e3a424cab6'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb38453e950105-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:43:59,162 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:43:59,162 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:43:59,167 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:43:59,167 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:43:59,167 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:43:59,169 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:43:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3989', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4072', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '23622', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '12.756s', 'x-request-id': 'req_d9a42d90b35e4e27a16658e3a424cab6', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb38453e950105-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:43:59,169 [DEBUG] openai._base_client: request_id: req_d9a42d90b35e4e27a16658e3a424cab6
2025-10-08 23:44:00,566 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:44:00,572 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:44:00,572 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:44:00,572 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:44:00,572 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:44:00,572 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:44:00,575 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe3cad0>
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,576 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:44:00 GMT')])
2025-10-08 23:44:00,577 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:44:00,577 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,577 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:00,577 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:00,577 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:00,577 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:44:00,583 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:44:00,583 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:44:00,583 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:44:00,584 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:44:00,584 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:44:00,584 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe43690>
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:44:00 GMT')])
2025-10-08 23:44:00,585 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,585 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:00,586 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:44:00,587 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:44:00 GMT')])
2025-10-08 23:44:00,587 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:44:00,587 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:44:00,587 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:00,587 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:00,587 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:00,588 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:44:00,588 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:00,588 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:44:00,588 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:00,588 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:44:00,589 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:44:00 GMT')])
2025-10-08 23:44:00,590 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:44:00,590 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:44:00,590 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:00,590 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:00,590 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:00,591 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-45dc8785-cda2-43e0-b1b8-36b8b5594254', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe3a5c0>, 'json_data': {'input': [[40, 1097, 12304, 311, 3881, 369, 279, 15592, 25922, 2361, 520, 1144, 1342, 13536, 90, 33, 10754, 7966, 3161, 856, 4092, 304, 1144, 1342, 13536, 90, 15836, 14, 2735, 4500, 92, 323, 16781, 3217, 304, 1144, 1342, 13536, 90, 12641, 14645, 2186, 358, 1097, 24450, 311, 17210, 311, 701, 2128, 753, 9131, 315, 44169, 15592, 32505, 2626, 4791, 15204, 12023, 8522, 382, 644, 856, 3293, 7224, 11, 358, 617, 5918, 69311, 8522, 1701, 1144, 1342, 13536, 90, 26404, 19410, 11, 5468, 51, 22312, 2186, 323, 5370, 1144, 1342, 13536, 90, 12641, 3600, 92, 430, 5398, 449, 701, 8670, 13, 1789, 3187, 11, 304, 279, 1144, 1342, 13536, 90, 37, 7442, 660, 21579, 9147, 14434, 36047, 92, 2447, 11, 358, 8040, 18699, 15592, 78335, 311, 18885, 828, 12625, 1701, 1144, 1342, 13536, 90, 75046, 51804, 37407, 2186, 32145, 264, 5199, 16048, 304, 4907, 15374, 555, 220, 5538, 13, 22, 59, 14697, 3092, 3217, 449, 1144, 1342, 13536, 90, 37236, 328, 15003, 4506, 92, 323, 1144, 1342, 13536, 90, 79207, 15592, 3600, 92, 706, 10235, 757, 311, 10739, 15592, 14, 2735, 4211, 13750, 11, 264, 1401, 16686, 304, 279, 2683, 17437, 13], [40, 1097, 8104, 15107, 311, 1144, 1342, 13536, 90, 33, 10754, 11923, 82, 15507, 311, 54952, 14713, 48448, 5557, 2949, 8776, 23641, 49125, 13, 3092, 3495, 3217, 304, 1144, 1342, 13536, 90, 4677, 8995, 32505, 4443, 2065, 304, 42207, 660, 6975, 92, 5825, 757, 449, 264, 5016, 13356, 389, 47594, 1887, 15374, 323, 4868, 13, 1115, 5398, 82, 449, 426, 10754, 753, 19297, 9021, 11, 323, 358, 1097, 12304, 922, 279, 6776, 311, 17210, 311, 701, 15592, 7661, 753, 62565, 28271, 555, 63779, 389, 279, 69543, 315, 15009, 10105, 382, 40, 1097, 42702, 922, 12967, 856, 3575, 99246, 7512, 311, 1144, 1342, 13536, 90, 33, 10754, 92, 323, 29820, 311, 279, 4500, 315, 22514, 15592, 8522, 430, 5276, 11761, 23641, 11704, 13, 3092, 17033, 3839, 3335, 304, 1144, 1342, 13536, 90, 695, 15660, 26329, 92, 323, 1803, 1413, 15592, 49125, 690, 7431, 757, 311, 1862, 279, 25605, 2065, 315, 15592, 14, 2735, 4211, 11, 23391, 47970, 18052, 449, 426, 10754, 753, 2626, 6067, 13]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:44:00,592 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:44:00,592 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:44:00,620 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe47290>
2025-10-08 23:44:00,620 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10fa62690> server_hostname='api.openai.com' timeout=None
2025-10-08 23:44:00,684 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe44d90>
2025-10-08 23:44:00,684 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:44:00,684 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:00,684 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:44:00,686 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:00,688 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:44:00,818 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:44:01,325 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:44:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'112'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-797b6d7f75-j9mjg'), (b'x-envoy-upstream-service-time', b'546'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999646'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'21ms'), (b'x-request-id', b'req_d997bddfd3394221982f19452a17bba8'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IQQPJTWcOak9phNXFuEBgJBZBwPl043bWZtRMk9SB1A-1759985041-1.0.1.1-O1jb8GAM_O9sOiLOsmgK_Dfl0TOAALr.7J4afUZl6xQg8_0L1t6Ve1ugwe7tW4EvpTHu1Z7hund3jxPNVq5mrpw26P746RkL1jTT3vivScs; path=/; expires=Thu, 09-Oct-25 05:14:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=KtHHHLd.sZ7ox4aXyvF9w2Odf5Zob2zCCpJanP3gUYY-1759985041373-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb3868d8d32d03-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:44:01,325 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:44:01,325 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:44:01,326 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:01,326 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:01,326 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:01,326 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:44:01 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '112'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-797b6d7f75-j9mjg'), ('x-envoy-upstream-service-time', '546'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999646'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '21ms'), ('x-request-id', 'req_d997bddfd3394221982f19452a17bba8'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=IQQPJTWcOak9phNXFuEBgJBZBwPl043bWZtRMk9SB1A-1759985041-1.0.1.1-O1jb8GAM_O9sOiLOsmgK_Dfl0TOAALr.7J4afUZl6xQg8_0L1t6Ve1ugwe7tW4EvpTHu1Z7hund3jxPNVq5mrpw26P746RkL1jTT3vivScs; path=/; expires=Thu, 09-Oct-25 05:14:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=KtHHHLd.sZ7ox4aXyvF9w2Odf5Zob2zCCpJanP3gUYY-1759985041373-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb3868d8d32d03-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:44:01,327 [DEBUG] openai._base_client: request_id: req_d997bddfd3394221982f19452a17bba8
2025-10-08 23:44:01,330 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:44:01,331 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:01,331 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:44:01,331 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:01,331 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:44:01,331 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'55'), (b'date', b'Thu, 09 Oct 2025 04:44:00 GMT')])
2025-10-08 23:44:01,332 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-08 23:44:01,332 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:44:01,332 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:01,332 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:01,332 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:01,336 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:44:01,336 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:01,336 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:44:01,337 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:01,337 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:44:01,398 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'2'), (b'date', b'Thu, 09 Oct 2025 04:44:01 GMT')])
2025-10-08 23:44:01,398 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-08 23:44:01,398 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:44:01,398 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:01,398 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:01,398 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:01,410 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-67053ca5-b565-46d9-945c-477825a36363', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10f4614e0>, 'json_data': {'input': 'Company: BMO. Position: AI Developer. Description: About the job\nPlease note, this is a full-time (37.5 hours per week) role based out of Toronto, ON (hybrid). \n\nAs an AI Developer, you will be part of the AI pod supporting the development of business-to-consumer (B2C) applications leveraging AI at BMO. You will work with data scientists from BMO‚Äôs Personal & Business Banking group to develop, test, deploy and maintain AI/ML models in BMO‚Äôs cloud environments. You will work closely with developers across the Development Security Operations (DevSecOps) cycle to support the operationalization of AI/ML models. This role is part of an AI pod in an agile framework in which you will support a Product Owner (PO) and Scrum Master by advising, alongside other technical stakeholders, on the engineering feasibility for AI/ML solutions.\n\nMain Responsibilities:\n\nSupport development of AI/ML models including development of monitoring, drift detection and training/retraining processes in a cloud environment\nDeploy AI/ML models (SaaS services or internally built) as part of host applications and integrate AI/ML solutions with business applications\nSupport data scientists by running ad-hoc analysis, POCs, and model validation tests during development\nDevelop applications to host AI/ML Models or leverage cloud AI SaaS services and create data flows to downstream consumers\nBuild data pipelines and transform data to ensure reliable input for AI/ML applications\nResearch and experiment with cloud AI/ML services in AWS and Azure to inform business and data science stakeholders on capabilities available to support their use cases\nTranslate business and data science requirements into infrastructure work packages for cloud deployment, collaborating with cross-functional teams\n\n\nAbout You\n\nTypically 2-4 years of relevant experience and post-secondary degree in AI/ML product engineering, software engineering, or other related field. An equivalent combination of education and experience including previous co-op/internships will also be considered\nProficiency with Python and SQL\nExperience with AI/ML services in AWS and/or Azure, including\nAWS: Sagemaker and/or Bedrock, and orchestration of workflows with Lambda and Step Functions\nAzure: Azure AI services, AI Foundry, Azure ML Studio, Azure Storage, Functions\nDevOps / MLOPs: Code Pipeline, Code build, Integration with Git Hub\nCloud Certifications (e.g. AWS Certified Cloud Practitioner, Azure AZ-900) are preferred\nFamiliarity with generative AI frameworks including Langchain/Langraph and generative AI concepts such as Retriever Augmented Generation (RAG) preferred', 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:44:01,411 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:44:01,411 [DEBUG] httpcore.connection: close.started
2025-10-08 23:44:01,411 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:44:01,411 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:44:01,445 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe45450>
2025-10-08 23:44:01,445 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:44:01,482 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe45ed0>
2025-10-08 23:44:01,482 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:44:01,483 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:44:01,483 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:44:01,483 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:44:01,483 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:44:02,063 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:44:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'77'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-7cc8d487b9-qps2t'), (b'x-envoy-upstream-service-time', b'291'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999342'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_0958ecf4f6fc47dbac4cb35ee497b380'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb386ddb884227-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:44:02,063 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:44:02,063 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:44:02,064 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:44:02,064 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:44:02,064 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:44:02,064 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:44:02 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '77', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-7cc8d487b9-qps2t', 'x-envoy-upstream-service-time': '291', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999342', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_0958ecf4f6fc47dbac4cb35ee497b380', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb386ddb884227-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:44:02,064 [DEBUG] openai._base_client: request_id: req_0958ecf4f6fc47dbac4cb35ee497b380
2025-10-08 23:44:02,084 [INFO] src.api.app: ‚ùå POST /api/generate/ - 500 - 63.782s
2025-10-08 23:44:02,084 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 63.782s
2025-10-08 23:44:02,085 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:44:02] "[35m[1mPOST /api/generate/ HTTP/1.1[0m" 500 -
2025-10-08 23:50:33,339 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-08 23:50:33,341 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.002s
2025-10-08 23:50:33,342 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:50:33] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:50:33,344 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-08 23:50:33,344 [DEBUG] src.api.app: üìù Request data: {'job_description': "About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.", 'company_name': 'Google', 'position_title': 'Software Developer'}
2025-10-08 23:50:33,379 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-6b671534-b714-4049-9a97-d3ba0f6ea521', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe38220>, 'json_data': {'input': "Company: Google. Position: Software Developer. Description: About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.", 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:50:33,381 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:50:33,381 [DEBUG] httpcore.connection: close.started
2025-10-08 23:50:33,382 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:50:33,382 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:50:33,433 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfc5a50>
2025-10-08 23:50:33,434 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:50:33,471 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfc7510>
2025-10-08 23:50:33,471 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:33,472 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:33,472 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:33,472 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:33,472 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:50:33,704 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:50:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'89'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5dbdc66675-w92wn'), (b'x-envoy-upstream-service-time', b'123'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999103'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'53ms'), (b'x-request-id', b'req_0eaf08d1941b4d53b3186cbd0317a1e9'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb41ffbd297619-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:50:33,705 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:50:33,706 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:50:33,706 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:50:33,706 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:50:33,706 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:50:33,706 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:50:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '89', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-5dbdc66675-w92wn', 'x-envoy-upstream-service-time': '123', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999103', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '53ms', 'x-request-id': 'req_0eaf08d1941b4d53b3186cbd0317a1e9', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb41ffbd297619-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:50:33,707 [DEBUG] openai._base_client: request_id: req_0eaf08d1941b4d53b3186cbd0317a1e9
2025-10-08 23:50:33,752 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.408s
2025-10-08 23:50:33,752 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:50:33] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-08 23:50:33,757 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-08 23:50:33,757 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-08 23:50:33,757 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:50:33] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-08 23:50:33,760 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-08 23:50:33,760 [DEBUG] src.api.app: üìù Request data: {'jobDescription': "About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.", 'companyName': 'Google', 'positionTitle': 'Software Developer', 'strategy': 'generate'}
2025-10-08 23:50:33,774 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-7e7db744-3a29-4962-9fb4-d7fc8aeaf1ad', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe3b4c0>, 'json_data': {'input': "Company: Google. Position: Software Developer. Description: About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.", 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:50:33,776 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:50:33,776 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:33,776 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:33,776 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:33,777 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:33,777 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:50:34,497 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:50:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'65'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-66c4684c6c-f5p6l'), (b'x-envoy-upstream-service-time', b'266'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999103'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'53ms'), (b'x-request-id', b'req_33e53a7e073d4d96a8fa5e5abfdfd738'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb4201981c7619-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:50:34,497 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:50:34,498 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:50:34,499 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:50:34,499 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:50:34,499 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:50:34,499 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:50:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '65', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-66c4684c6c-f5p6l', 'x-envoy-upstream-service-time': '266', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999103', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '53ms', 'x-request-id': 'req_33e53a7e073d4d96a8fa5e5abfdfd738', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb4201981c7619-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:50:34,499 [DEBUG] openai._base_client: request_id: req_33e53a7e073d4d96a8fa5e5abfdfd738
2025-10-08 23:50:34,519 [DEBUG] httpcore.connection: close.started
2025-10-08 23:50:34,520 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:50:34,538 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-bc3e1365-5572-4569-b6fd-3605e163bdef', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring professional experiences to specific job requirements.\n    \n    Your task is to analyze the provided job posting and candidate experiences, then generate LaTeX-formatted resume entries that highlight the most relevant skills, achievements, and experiences for the target position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor\'s degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster\'s degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle\'s software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We\'re looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology\'s greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    CANDIDATE EXPERIENCES:\n    [\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Graduate Research Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2023-09",\n    "end_date": "2025-07",\n    "duration": "1 yr 11 mos",\n    "highlights": [\n      "Designed and developed four novel distributed AI architectures (PoCL, SSFL, BSFL, BPFL) addressing critical challenges in distributed learning.",\n      "PoCL: a blockchain consensus mechanism that repurposes energy-intensive mining for secure, incentive-aligned, and resource-efficient federated learning.",\n      "SSFL: An architecture enhancing scalability and efficiency for distributed deep learning.",\n      "BSFL: The first decentralized SplitFed Learning framework leveraging smart contracts for model integrity and decentralized coordination.",\n      "BPFL: A framework improving fairness and model ownership through contribution-based personalization and tokenized access.",\n      "Implemented and optimized these architectures using a robust tech stack, including TensorFlow, PyTorch for AI model development, Hyperledger Fabric for permissioned blockchain integration, and Express.js/Flask for modular API development.",\n      "Reduced communication overhead by 85.2%, improved fault tolerance by 62.7%, and enhanced energy efficiency through PoCL.",\n      "Authored research published/submitted to top-tier IEEE venues."\n    ],\n    "skills": [\n      "Distributed Machine Learning",\n      "TensorFlow",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Blockchain",\n      "System Optimization",\n      "Consensus Algorithms",\n      "Technical Communication",\n      "Problem Solving"\n    ],\n    "publications": [\n      "Proof-of-Collaborative-Learning: A Multi-winner Federated Learning Consensus Algorithm (Published)",\n      "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches (Accepted)",\n      "Towards Fair Model Ownership: Blockchain-Driven Personalization in Federated Learning (Under Review)"\n    ]\n  },\n  {\n    "organization": "University of Manitoba",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Teaching Assistant",\n    "employment_type": "Contract",\n    "start_date": "2024-09",\n    "end_date": "2025-06",\n    "duration": "10 mos",\n    "highlights": [\n      "Delivered direct academic and technical support to over 400 undergraduate students across foundational Computer Science courses, including Introduction to Programming (Python), Object-Oriented Programming (Java), Data Structures & Algorithms, Computer Networks, and Distributed Systems.",\n      "Guided students through complex coding projects and theoretical challenges, focusing on problem-solving strategies, algorithm efficiency, and system design principles.",\n      "Offered specialized technical assistance at the university\'s Help Centre, adeptly diagnosing and resolving programming issues and advising on effective coding practices for diverse academic contexts."\n    ],\n    "skills": [\n      "Python",\n      "Java",\n      "Data Structures & Algorithms",\n      "Computer Networks",\n      "Distributed Systems",\n      "Technical Communication"\n    ]\n  },\n  {\n    "organization": "Bobo",\n    "location": "Winnipeg, Manitoba, Canada",\n    "title": "Full-stack Developer Intern",\n    "employment_type": "Internship",\n    "start_date": "2024-05",\n    "end_date": "2024-08",\n    "duration": "4 mos",\n    "highlights": [\n      "Accelerated product development by designing and implementing RESTful APIs using Supabase and PostgreSQL, enhancing backend functionality and data management.",\n      "Automated data integration by developing a Python script to convert CSV data into executable SQL, significantly streamlining database population.",\n      "Ensured seamless full-stack integration by collaborating closely with the front-end team to align API specifications with user interface requirements.",\n      "Contributed to agile workflow efficiency through active utilization of the Atlassian suite (Jira, Confluence) for task management, documentation, and team coordination."\n    ],\n    "skills": [\n      "Supabase",\n      "PostgreSQL",\n      "Jira",\n      "Confluence",\n      "Collaboration"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "location": "",\n    "title": "Lead Teaching Assistant",\n    "employment_type": "Contract Full-time",\n    "start_date": "2021-01",\n    "end_date": "2023-01",\n    "duration": "2 yrs 1 mo",\n    "highlights": [\n      "Supported and guided over 300 undergraduate students across complex technical subjects, including Linear Algebra, System Design and Analysis, Computer Networks, Discrete Mathematics, Operating Systems, and Algorithm Design.",\n      "Designed and developed engaging lectures, practical coding projects, and challenging assignments, significantly enhancing student comprehension and hands-on application of core computer science concepts.",\n      "Provided specialized support in areas like algorithm implementation, system architecture, and network protocols, bridging theoretical knowledge with practical problem-solving."\n    ],\n    "skills": [\n      "System Design and Analysis",\n      "Computer Networks",\n      "Algorithms",\n      "Operating Systems",\n      "Technical Teaching"\n    ]\n  },\n  {\n    "organization": "K. N. Toosi University of Technology",\n    "title": "Research Assistant",\n    "location": "",\n    "employment_type": "Full-time",\n    "start_date": "2021-06",\n    "end_date": "2022-08",\n    "duration": "1 yr 3 mos",\n    "highlights": [\n      "Led an undergraduate research team, successfully initiating and guiding a study on solar panel funding fairness in Switzerland that culminated in key analytical findings.",\n      "Executed comprehensive data collection and cleaning of over 10GB of raw data from diverse government portals, ensuring high data integrity and preparing datasets for advanced analysis.",\n      "Leveraged advanced causal inference techniques in R to analyze complex datasets, producing detailed analytical reports that quantified and highlighted significant subsidy biases."\n    ],\n    "skills": [\n      "PostgreSQL",\n      "PostGIS",\n      "R",\n      "Machine Learning",\n      "Data Cleaning",\n      "Team Leadership"\n    ]\n  },\n  {\n    "organization": "Sadr Group Company",\n    "title": "Full-stack Developer Intern",\n    "location": "",\n    "employment_type": "Part-time",\n    "start_date": "2020-07",\n    "end_date": "2020-12",\n    "duration": "6 mos",\n    "highlights": [\n      "Drove the successful completion of a critical internal project, addressing key deficiencies in API security, authentication, and data modeling for production deployment.",\n      "Developed robust RESTful APIs with AdonisJS and Express.js, implementing JWT authentication to secure user access and enable core application functionalities.",\n      "Designed and implemented highly accurate MySQL database schemas for precise tracking and management of employee working and non-working hours, foundational for the project\'s business logic.",\n      "Deployed fully functional features to the live production server and created thorough Swagger API documentation, facilitating seamless internal integration and knowledge transfer.",\n      "Contributed to an Agile Scrum development team by actively tracking and managing personal tasks within ClickUp, ensuring efficient progress and clear communication within sprints and CI/CD pipelines."\n    ],\n    "skills": [\n      "Node.js",\n      "Express.js",\n      "Adonis.js",\n      "Vue.js",\n      "MySQL",\n      "Swagger",\n      "Technical Communication"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements, skills, technologies, and qualifications\n    2. Select the 3-4 most relevant experiences from the candidate\'s background\n    3. For each selected experience, generate a LaTeX resume entry following this EXACT format:\n\n    \\resumeSubheading\n        {Organization Name}{Start Date ‚Äì End Date}\n        {Job Title}{Location}\n        \\resumeItemListStart\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Achievement/responsibility highlighting relevant skills with \\textbf{bold keywords}}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - Use \\textbf{} to bold technical skills, technologies, methodologies, and key achievements mentioned in the job posting\n    - Start each \\resumeItem with strong action verbs (Developed, Implemented, Designed, Led, Accelerated, etc.)\n    - Quantify achievements with numbers/percentages when available (use \\% for percentages in LaTeX)\n    - Tailor the language to match the job posting\'s terminology\n    - Highlight transferable skills even if from different domains\n    - Focus on impact and results, not just responsibilities\n    - Ensure each experience shows progression and growth\n    - Maximum 5 resume items per experience\n    - Order experiences by relevance to the job posting\n\n    PRIORITIZATION CRITERIA:\n    1. Direct skill/technology matches with job requirements\n    2. Relevant industry experience\n    3. Leadership and project management experience\n    4. Technical depth and complexity of work\n    5. Recent and duration of experience\n\n    Generate the LaTeX resume entries for the most relevant experiences:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:50:34,540 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:50:34,540 [DEBUG] httpcore.connection: close.started
2025-10-08 23:50:34,541 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:50:34,541 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:50:34,575 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfce990>
2025-10-08 23:50:34,576 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10cb5f0b0> server_hostname='api.openai.com' timeout=None
2025-10-08 23:50:34,613 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cfce8d0>
2025-10-08 23:50:34,613 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:34,613 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:34,613 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:34,613 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:34,614 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:50:50,025 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:50:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'15258'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'15279'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26585'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'6.83s'), (b'x-request-id', b'req_cdfd86e4659d4e008cb25185a6e76cef'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb4206ddea607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:50:50,027 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:50:50,028 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:50:50,028 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:50:50,029 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:50:50,029 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:50:50,029 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:50:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '15258', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '15279', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26585', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '6.83s', 'x-request-id': 'req_cdfd86e4659d4e008cb25185a6e76cef', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb4206ddea607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:50:50,029 [DEBUG] openai._base_client: request_id: req_cdfd86e4659d4e008cb25185a6e76cef
2025-10-08 23:50:50,034 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-337a4091-d145-49a5-b386-64bb19a03eac', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in tailoring technical skills sections to specific job requirements.\n\n    Your task is to analyze the job posting and remove only those skills from the candidate\'s skill set that are clearly irrelevant, keeping a broad and well-rounded technical skills section. Start with the full list, then prune.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor\'s degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster\'s degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle\'s software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We\'re looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology\'s greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    {\n  "AI / Machine Learning": [\n    {\n      "skill": "TensorFlow",\n      "expertise": 8\n    },\n    {\n      "skill": "PyTorch",\n      "expertise": 8\n    },\n    {\n      "skill": "Flower",\n      "expertise": 7\n    },\n    {\n      "skill": "Numpy",\n      "expertise": 9\n    },\n    {\n      "skill": "Pandas",\n      "expertise": 8\n    },\n    {\n      "skill": "OpenCV",\n      "expertise": 7\n    },\n    {\n      "skill": "Transformers",\n      "expertise": 8\n    },\n    {\n      "skill": "LangChain",\n      "expertise": 8\n    },\n    {\n      "skill": "LangGraph",\n      "expertise": 7\n    },\n    {\n      "skill": "Scikit-learn",\n      "expertise": 8\n    },\n    {\n      "skill": "Keras",\n      "expertise": 8\n    },\n    {\n      "skill": "Matplotlib",\n      "expertise": 9\n    },\n    {\n      "skill": "TensorBoard",\n      "expertise": 5\n    },\n    {\n      "skill": "CUDA",\n      "expertise": 6\n    }\n  ],\n  "Languages": [\n    {\n      "skill": "Python",\n      "expertise": 10\n    },\n    {\n      "skill": "C++",\n      "expertise": 5\n    },\n    {\n      "skill": "Java",\n      "expertise": 7\n    },\n    {\n      "skill": "JavaScript",\n      "expertise": 9\n    },\n    {\n      "skill": "TypeScript",\n      "expertise": 9\n    },\n    {\n      "skill": "Node JS",\n      "expertise": 9\n    },\n    {\n      "skill": "Solidity",\n      "expertise": 3\n    }\n  ],\n  "Cloud & DevOps": [\n    {\n      "skill": "Docker",\n      "expertise": 8\n    },\n    {\n      "skill": "Kubernetes",\n      "expertise": 2\n    },\n    {\n      "skill": "Hyperledger Fabric",\n      "expertise": 8\n    },\n    {\n      "skill": "Git",\n      "expertise": 9\n    },\n    {\n      "skill": "GitHub",\n      "expertise": 9\n    },\n    {\n      "skill": "Linux",\n      "expertise": 8\n    },\n    {\n      "skill": "CI/CD",\n      "expertise": 7\n    },\n    {\n      "skill": "AWS",\n      "expertise": 2\n    }\n  ],\n  "Databases": [\n    {\n      "skill": "PostgreSQL",\n      "expertise": 7\n    },\n    {\n      "skill": "MongoDB",\n      "expertise": 7\n    },\n    {\n      "skill": "MySQL",\n      "expertise": 7\n    },\n    {\n      "skill": "Redis",\n      "expertise": 5\n    },\n    {\n      "skill": "Db2",\n      "expertise": 7\n    },\n    {\n      "skill": "Supabase",\n      "expertise": 6\n    }\n  ],\n  "Web Frameworks": [\n    {\n      "skill": "Nest.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "Django",\n      "expertise": 8\n    },\n    {\n      "skill": "Express.JS",\n      "expertise": 8\n    },\n    {\n      "skill": "Flask",\n      "expertise": 8\n    },\n    {\n      "skill": "Adonis.JS",\n      "expertise": 5\n    },\n    {\n      "skill": "React",\n      "expertise": 8\n    },\n    {\n      "skill": "Tailwind",\n      "expertise": 8\n    }\n  ],\n  "Tools & Methodologies": [\n    {\n      "skill": "Jira",\n      "expertise": 8\n    },\n    {\n      "skill": "Confluence",\n      "expertise": 7\n    },\n    {\n      "skill": "Swagger",\n      "expertise": 4\n    },\n    {\n      "skill": "ClickUp",\n      "expertise": 7\n    },\n    {\n      "skill": "Agile",\n      "expertise": 9\n    },\n    {\n      "skill": "Scrum",\n      "expertise": 9\n    }\n  ]\n}\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify required/preferred technical skills, technologies, frameworks, and tools.\n    2. Start with the full candidate skill set, and remove only those skills that are clearly irrelevant to the job.\n    3. Within each category, prioritize:\n    - Direct matches from the job posting\n    - High expertise (score ‚â• 6)\n    - Industry-relevant and complementary skills\n    4. Ensure **at least 4‚Äì5 skills per category** (unless fewer exist in the candidate\'s set).\n    5. Cap each category at **8‚Äì10 skills maximum** to keep it concise.\n\n    OUTPUT FORMAT:\n    Generate a LaTeX technical skills section in this exact format:\n\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n    \\small{\\item{\n        \\textbf{AI / Machine Learning}{: [Selected AI/ML skills]} \\\\\n        \\textbf{Languages}{: [Selected programming languages]} \\\\\n        \\textbf{Cloud \\& DevOps}{: [Selected cloud/devops tools]} \\\\\n        \\textbf{Databases}{: [Selected database technologies]} \\\\\n        \\textbf{Web Frameworks}{: \\emph{Back-end}: [Backend frameworks]. \\emph{Front-end}: [Frontend frameworks]} \\\\\n        \\textbf{Tools \\& Methodologies}{: [Selected tools and methodologies]}\n    }}\n    \\end{itemize}\n\n    FORMATTING RULES:\n    - Only omit a category if the candidate truly has no relevant skills there.\n    - Within categories, sort skills by relevance to the job posting.\n    - Use LaTeX escaping (\\& for ampersands, etc.).\n    - Do not include expertise scores in the output.\n    - Keep categories ordered according to relevance to the job description.\n    - Output **only the LaTeX block**, nothing else.\n\n    Now generate the LaTeX technical skills section:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:50:50,035 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:50:50,035 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:50,035 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:50,035 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:50,035 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:50,035 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:50:54,736 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:50:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3896'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4078'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'27824'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'4.352s'), (b'x-request-id', b'req_445d5fe3af7b9251acd4077a3bac761d'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb42674e7b607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:50:54,738 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:50:54,738 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:50:54,744 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:50:54,744 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:50:54,744 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:50:54,744 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:50:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3896', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4078', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '27824', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '4.352s', 'x-request-id': 'req_445d5fe3af7b9251acd4077a3bac761d', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb42674e7b607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:50:54,744 [DEBUG] openai._base_client: request_id: req_445d5fe3af7b9251acd4077a3bac761d
2025-10-08 23:50:54,747 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-902f8645-e089-4517-a951-7fe61bf4c87b', 'json_data': {'messages': [{'content': '\n    You are an expert resume strategist specializing in project selection for job applications.\n    \n    Your task is to analyze the job posting and select up to 4 most relevant projects from the candidate\'s portfolio that best demonstrate the skills and experience required for the position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor\'s degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster\'s degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle\'s software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We\'re looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology\'s greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    CANDIDATE\'S PROJECTS:\n    [\n  {\n    "title": "Federated Learning enabled Digital Twin",\n    "readme": "./projects/DigitalTwin.md",\n    "github": "https://github.com/amirrezaskh/DigitalTwin",\n    "description": "This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "PyTorch Lightning",\n      "PyTorch Forecasting",\n      "Pandas",\n      "Flask",\n      "Express.js",\n      "Docker",\n      "Hyperledger Fabric"\n    ]\n  },\n  {\n    "title": "Mini Task Manager",\n    "readme": "./projects/MiniTaskManager.md",\n    "github": "https://github.com/amirrezaskh/mini-task-manager",\n    "description": "A modern, full-stack task management application built with Django REST Framework and React. This project demonstrates clean architecture principles, secure token-based authentication, and a responsive Material-UI interface for managing personal tasks efficiently.",\n    "stack": [\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "React",\n      "TypeScript",\n      "Node.js",\n      "Material-UI",\n      "SQLite"\n    ]\n  },\n  {\n    "title": "Proof of Collaborative Learning (PoCL)",\n    "readme": "./projects/PoCL.md",\n    "github": "https://github.com/amirrezaskh/Proof-of-Collaborative-Learning",\n    "description": "This project implements PoCL, a novel blockchain consensus mechanism that replaces energy-intensive mining with a federated learning system. Miners collaboratively train a deep learning model, and winners are selected based on model performance through a democratic voting system. The system uses Hyperledger Fabric for the blockchain, a custom Python-based federated learning framework with TensorFlow, and Express.js as an API gateway.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "Flask",\n      "Node.js",\n      "Express.js",\n      "Hyperledger Fabric",\n      "Docker",\n      "JavaScript"\n    ]\n  },\n  {\n    "title": "Blockchain-enabled Personalized Federated Learning (BPFL)",\n    "readme": "./projects/BPFL.md",\n    "github": "https://github.com/amirrezaskh/BPFL",\n    "description": "This project implements a novel framework that combines personalized federated learning with blockchain technology. It addresses data heterogeneity and participant incentivization by using a token-based reward system and a contribution-weighted aggregation method. The system is built with Hyperledger Fabric for secure transactions and uses a custom federated learning framework with PyTorch to train models on datasets like MNIST and CIFAR-10.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Sharded and Blockchain-Enabled SplitFed Approaches",\n    "readme": "./projects/SSFL-BSFL.md",\n    "github": "https://github.com/amirrezaskh/SSFL-BSFL",\n    "description": "This project implements and compares four distributed machine learning frameworks: Split Learning (SL), SplitFed Learning (SFL), Sharded SplitFed Learning (SSFL), and Blockchain-enabled SplitFed Learning (BSFL). It evaluates these approaches under various conditions, including different numbers of participating nodes and scenarios with data poisoning attacks, with the goal of enhancing distributed and privacy-preserving machine learning.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Hyperledger Fabric",\n      "Docker",\n      "Node.js",\n      "Express.js",\n      "JavaScript",\n      "Flask",\n      "Matplotlib"\n    ]\n  },\n  {\n    "title": "Paper Summarizer",\n    "readme": "./projects/PaperSummarizer.md",\n    "github": "https://github.com/amirrezaskh/paper-summarizer",\n    "description": "An intelligent academic paper summarization system that combines computer vision and natural language processing to extract, analyze, and summarize research papers from PDF format. It uses object detection to identify document elements, OCR for text extraction, vision-language models for figure analysis, and large language models for generating comprehensive summaries.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "EasyOCR",\n      "Detectron2",\n      "PyTorch",\n      "Flask",\n      "Ollama",\n      "LLaVA",\n      "Llama 3.2",\n      "Pandoc"\n    ]\n  },\n  {\n    "title": "MarkMate",\n    "readme": "./projects/MarkMate.md",\n    "github": "https://github.com/amirrezaskh/markmate",\n    "description": "MarkMate is an AI-powered educational platform that automates assignment grading for instructors. It features a microservices architecture with a React frontend, a Django REST API backend with a PostgreSQL database, and a Flask-based LLM microservice that utilizes GPT-4 to provide automated grading based on custom rubrics.",\n    "stack": [\n      "React",\n      "React Router",\n      "Vite",\n      "Tailwind CSS",\n      "Node.js",\n      "Django",\n      "Django REST Framework",\n      "Python",\n      "PostgreSQL",\n      "Flask",\n      "LangChain",\n      "GPT-4",\n      "PyPDF"\n    ]\n  },\n  {\n    "title": "CIFAR-10 Generative Model Evaluation",\n    "readme": "./projects/CIFAR-10 Generative Model Evaluation.md",\n    "github": "https://github.com/amirrezaskh/Unified-Image-Evaluation-Metric",\n    "description": "This project evaluates generative models, specifically diffusion models, on the CIFAR-10 dataset using a suite of custom metrics. It employs a pre-trained ResNet-50 classifier to extract features from both real and generated images and then computes Precision, Recall, Generalization Rate, and F1 Score based on nearest-neighbor analysis.",\n    "stack": [\n      "Python",\n      "PyTorch",\n      "Torchvision",\n      "scikit-learn",\n      "NumPy",\n      "ResNet-50",\n      "Diffusion Models"\n    ]\n  },\n  {\n    "title": "PyFed",\n    "readme": "./projects/PyFed.md",\n    "github": "https://github.com/amirrezaskh/PyFed",\n    "description": "PyFed is an open-source, lightweight federated learning framework that facilitates the implementation of federated learning algorithms using standard TensorFlow datasets or custom preprocessed data. The framework uses a client-server architecture with sockets, processes, and threads, and currently supports the FedAvg policy.",\n    "stack": [\n      "Python",\n      "TensorFlow",\n      "scikit-learn",\n      "NumPy",\n      "Sockets",\n      "Threads",\n      "Processes",\n      "TensorBoard"\n    ]\n  },\n  {\n    "title": "Facial Landmark & Boundary Detection",\n    "readme": "./projects/FacialBoundary.md",\n    "github": "https://github.com/amirrezaskh/Facial-Boundary-and-Facial-Landmarks-Detection-using-Convolutional-Neural-Networks",\n    "description": "This project implements a computer vision pipeline for real-time multi-face detection and landmark localization. It utilizes dual Faster R-CNN models\\u2014one trained to detect facial boundaries and the other to extract 68 facial landmarks\\u2014to provide a robust solution for real-time facial feature extraction.",\n    "stack": [\n      "Python",\n      "OpenCV",\n      "TensorFlow",\n      "Faster R-CNN",\n      "Neural Networks",\n      "Computer Vision"\n    ]\n  },\n  {\n    "title": "Aria",\n    "readme": "./projects/Aria.md",\n    "github": "https://github.com/amirrezaskh/aria",\n    "description": "Aria is an intelligent resume and cover letter generation system that leverages advanced AI to create tailored, professional documents based on specific job postings. Using LangChain workflows and OpenAI\'s GPT-4o-mini, it analyzes job requirements and dynamically generates customized resume sections, technical skills summaries, project selections, and highlight qualifications, all formatted in professional LaTeX for PDF compilation.",\n    "stack": [\n      "Python",\n      "LLM",\n      "LangChain",\n      "LangGraph",\n      "OpenAI GPT-4o-mini",\n      "OpenAI Embeddings",\n      "LaTeX",\n      "PyPDF",\n      "JSON",\n      "Regular Expressions",\n      "Jupyter Notebook"\n    ]\n  }\n]\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Required technical skills and technologies\n       - Preferred experience areas\n       - Industry domain and problem types\n       - Project complexity and scale requirements\n       - Key competencies (full-stack, AI/ML, backend, etc.)\n\n    2. Evaluate each project based on:\n       - Technology stack alignment with job requirements\n       - Problem domain relevance to the job\'s industry\n       - Complexity and scale that demonstrates required skill level\n       - Unique value proposition that sets candidate apart\n       - Recency and relevance to current tech trends\n\n    3. Select up to 4 projects that collectively:\n       - Cover the most important job requirements\n       - Demonstrate progression and skill growth\n       - Show diverse but relevant capabilities\n       - Highlight unique expertise that differentiates the candidate\n\n    SELECTION CRITERIA (in order of importance):\n    1. Direct technology/framework matches (highest priority)\n    2. Problem domain alignment with job industry\n    3. Complexity level appropriate for the role\n    4. Demonstrates end-to-end project ownership\n    5. Shows innovation or unique technical solutions\n    6. Covers complementary skills mentioned in job posting\n    7. You are allowed to select even one project, as long as the selected projects are relevant to the job.\n\n    OUTPUT FORMAT:\n    Return ONLY a JSON list of the selected project titles, ordered by relevance to the job posting.\n    \n    Example format:\n    ["Project Title 1", "Project Title 2", "Project Title 3", "Project Title 4"]\n\n    Important: Return ONLY the JSON list, no additional text or explanation.\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:50:54,748 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:50:54,749 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:54,749 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:54,749 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:54,749 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:54,750 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:50:56,274 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:50:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'1396'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1425'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'26389'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'7.221s'), (b'x-request-id', b'req_66a21b8d573940cbb54b2ad2ae1bf379'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb4284b9ae607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:50:56,275 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:50:56,275 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:50:56,278 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:50:56,279 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:50:56,279 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:50:56,280 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:50:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '1396', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1425', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '26389', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '7.221s', 'x-request-id': 'req_66a21b8d573940cbb54b2ad2ae1bf379', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb4284b9ae607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:50:56,281 [DEBUG] openai._base_client: request_id: req_66a21b8d573940cbb54b2ad2ae1bf379
2025-10-08 23:50:56,291 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-0436507d-07bd-4e98-9b0b-342757f0d64b', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    PROJECT DETAILS:\n    Title: Federated Learning enabled Digital Twin\n    Description: This project implements a novel Digital Twin system for smart building management using blockchain-enabled federated learning. We developed a Temporal Fusion Transformer (TFT) for predicting temperature, CO2, and humidity levels across 76 rooms within a University of Manitoba smart building, while ensuring data privacy and security through decentralized training.\n    Tech Stack: Python, PyTorch, PyTorch Lightning, PyTorch Forecasting, Pandas, Flask, Express.js, Docker, Hyperledger Fabric\n    \n    Detailed Documentation:\n    # Blockchain-Enabled Federated Learning for Digital Twin\n\nA privacy-preserving federated learning system that combines **Hyperledger Fabric blockchain** with **Temporal Fusion Transformer (TFT)** models for multi-variate time-series forecasting in smart building environments.\n\n## üè¢ Project Overview\n\nThis project implements a novel **Digital Twin** system for smart building management using blockchain-enabled federated learning. We developed a **Temporal Fusion Transformer (TFT)** for predicting temperature, CO2, and humidity levels across **76 rooms** within a University of Manitoba smart building, while ensuring **data privacy** and **security** through decentralized training.\n\n### Key Innovation\n- **Privacy-Preserving**: Raw sensor data never leaves individual rooms\n- **Blockchain-Secured**: Hyperledger Fabric ensures trust and auditability\n- **Federated Learning**: Collaborative training without centralized data collection\n- **Real-World Deployment**: Tested with actual IoT sensor data from 76 building rooms\n\n## üèóÔ∏è System Architecture\n\nOur system combines three cutting-edge technologies:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Smart Rooms   ‚îÇ    ‚îÇ   Blockchain    ‚îÇ    ‚îÇ   Federated     ‚îÇ\n‚îÇ   (IoT Sensors) ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Network       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Learning      ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ (Hyperledger    ‚îÇ    ‚îÇ   (TFT Models)  ‚îÇ\n‚îÇ  üå°Ô∏è Temperature ‚îÇ    ‚îÇ  Fabric)        ‚îÇ    ‚îÇ                 ‚îÇ\n‚îÇ  üí® CO2 Levels  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ  üìä Aggregation ‚îÇ\n‚îÇ  üíß Humidity    ‚îÇ    ‚îÇ  üîí Security    ‚îÇ    ‚îÇ  ü§ñ AI Training ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Design Philosophy\nWe leverage the **Hyperledger Fabric framework** to create a permissioned blockchain network with custom **smart contracts (chaincodes)** that manage the federated learning lifecycle. Each room operates as an independent **federated node**, training local models on private sensor data while contributing to a global model through secure parameter sharing on the distributed ledger.\n\n**Key Components:**\n- **8 Federated Nodes**: Each representing different rooms with unique sensor data\n- **Blockchain Network**: Hyperledger Fabric with custom model-transfer chaincode\n- **Global Aggregator**: Implements FedAvg algorithm for model parameter aggregation\n- **Express Orchestrator**: Coordinates training rounds and system communication \n\n## üìä Experimental Results\n\nOur federated learning approach demonstrates **significant performance improvements** across all sensor types while maintaining **complete data privacy**. The results validate the effectiveness of blockchain-enabled collaborative learning in real-world IoT environments.\n\n### Training Performance\nWe evaluated the system's performance focusing on **local model accuracy** for each room's private dataset. The following visualizations demonstrate clear **loss reduction** and **model convergence** across 30 training rounds:\n\n#### Humidity Prediction Performance\n![Humidity \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:50:56,294 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:50:56,294 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:50:56,295 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:50:56,295 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:50:56,295 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:50:56,295 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:00,894 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4472'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4494'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'24518'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'10.962s'), (b'x-request-id', b'req_0b10be9112064518a66440d1c5fd94e0'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb428e5fa7607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:00,895 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:00,895 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:00,898 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:00,898 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:00,898 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:00,903 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4472', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4494', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '24518', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '10.962s', 'x-request-id': 'req_0b10be9112064518a66440d1c5fd94e0', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb428e5fa7607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:00,903 [DEBUG] openai._base_client: request_id: req_0b10be9112064518a66440d1c5fd94e0
2025-10-08 23:51:00,905 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-5f61da75-fcad-4155-bfe5-17134193cb34', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    PROJECT DETAILS:\n    Title: Proof of Collaborative Learning (PoCL)\n    Description: This project implements PoCL, a novel blockchain consensus mechanism that replaces energy-intensive mining with a federated learning system. Miners collaboratively train a deep learning model, and winners are selected based on model performance through a democratic voting system. The system uses Hyperledger Fabric for the blockchain, a custom Python-based federated learning framework with TensorFlow, and Express.js as an API gateway.\n    Tech Stack: Python, TensorFlow, Flask, Node.js, Express.js, Hyperledger Fabric, Docker, JavaScript\n    \n    Detailed Documentation:\n    # Proof of Collaborative Learning: A Multi-winner Federated Learning Consensus Mechanism\n\n[![Paper](https://img.shields.io/badge/Paper-IEEE-blue)](https://ieeexplore.ieee.org/abstract/document/10664335)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)\n[![Hyperledger Fabric](https://img.shields.io/badge/Hyperledger%20Fabric-2.5-purple.svg)](https://hyperledger-fabric.readthedocs.io/)\n\n## Overview\n\nThis repository implements **PoCL (Proof of Collaborative Learning)**, a novel blockchain consensus mechanism that replaces energy-intensive mining with federated learning. Instead of solving cryptographic puzzles, miners collaboratively train a global deep learning model, with winners selected based on model performance through a democratic voting system.\n\n### Key Innovation\nPoCL transforms blockchain mining from a wasteful competition into a productive collaboration where:\n- üß† **Miners train ML models** instead of computing meaningless hashes\n- üó≥Ô∏è **Democratic voting** determines winners based on model quality\n- üéÅ **Performance-based rewards** incentivize honest participation\n- üìä **Global model improvement** benefits all participants\n\n![Design](./figures/Design.png)\n\n## üèóÔ∏è System Architecture\n\n### Core Components\n\n| Component | Purpose | Technology |\n|-----------|---------|------------|\n| **Miners** | Train models on local data, participate in consensus | Python + TensorFlow |\n| **Blockchain Network** | Immutable ledger for transactions and consensus | Hyperledger Fabric |\n| **Express Applications** | API gateway and process coordination | Node.js + Express |\n| **Aggregator** | Combine winning models using FedAvg | Python + Flask |\n| **Chaincodes** | Smart contracts for different system functions | JavaScript |\n\n### Workflow\n\n```mermaid\ngraph TD\n    A[Transaction Assignment] --> B[Local Model Training]\n    B --> C[Model Proposal + Test Data]\n    C --> D[Cross-Prediction Phase]\n    D --> E[Voting on Performance]\n    E --> F[Winner Selection]\n    F --> G[Model Aggregation]\n    G --> H[Reward Distribution]\n    H --> A\n```\n\n## üöÄ Quick Start\n\n### Prerequisites\n- **Docker & Docker Compose**: For Hyperledger Fabric network\n- **Node.js 16+**: For Express applications\n- **Python 3.8+**: For miners and aggregator\n- **TensorFlow 2.x**: For deep learning models\n\n### Installation\n\n1. **Install Hyperledger Fabric**\n   ```bash\n   curl -sSL https://bit.ly/2ysbOFE | bash -s\n   ```\n\n2. **Install Python Dependencies**\n   ```bash\n   pip install tensorflow flask requests numpy scikit-learn matplotlib\n   ```\n\n3. **Install Node.js Dependencies**\n   ```bash\n   cd express-application\n   npm install\n   ```\n\n### Running the System\n\n1. **Start the Complete System**\n   ```bash\n   python3 run.py\n   ```\n   This automatically:\n   - Deploys the Hyperledger Fabric network\n   - Starts all Express applications\n   - Launches 10 miners\n   - Initializes the aggregato\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:51:00,906 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:51:00,906 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:00,907 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:00,907 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:00,907 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:00,907 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:05,160 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4062'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4086'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25230'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.539s'), (b'x-request-id', b'req_0a993d365b3e44a0acce54d40429ca00'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb42ab3954607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:05,160 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:05,160 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:05,161 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:05,161 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:05,161 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:05,161 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:05 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4062', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4086', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25230', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.539s', 'x-request-id': 'req_0a993d365b3e44a0acce54d40429ca00', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb42ab3954607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:05,161 [DEBUG] openai._base_client: request_id: req_0a993d365b3e44a0acce54d40429ca00
2025-10-08 23:51:05,164 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-e87d4e05-f025-493e-8a2e-7f1158801767', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    PROJECT DETAILS:\n    Title: Blockchain-enabled Personalized Federated Learning (BPFL)\n    Description: This project implements a novel framework that combines personalized federated learning with blockchain technology. It addresses data heterogeneity and participant incentivization by using a token-based reward system and a contribution-weighted aggregation method. The system is built with Hyperledger Fabric for secure transactions and uses a custom federated learning framework with PyTorch to train models on datasets like MNIST and CIFAR-10.\n    Tech Stack: Python, PyTorch, Hyperledger Fabric, Docker, Node.js, Express.js, JavaScript, Flask, Matplotlib\n    \n    Detailed Documentation:\n    # Blockchain-enabled Personalized Federated Learning (BPFL)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n[![Hyperledger Fabric](https://img.shields.io/badge/Hyperledger%20Fabric-2.4+-blue.svg)](https://hyperledger-fabric.readthedocs.io/)\n\n## Overview\n\nThis repository contains the implementation of **Blockchain-enabled Personalized Federated Learning (BPFL)**, a novel framework that combines personalized federated learning with blockchain technology to create a decentralized, incentive-driven machine learning system. The framework addresses key challenges in traditional federated learning including data heterogeneity, participant incentivization, and model personalization.\n\n## Key Features\n\n- **üîó Blockchain Integration**: Utilizes Hyperledger Fabric for secure, transparent model aggregation and reward distribution\n- **üéØ Personalized Learning**: Adaptive model personalization using contribution-weighted aggregation\n- **üí∞ Incentive Mechanism**: Token-based reward system encouraging high-quality participation\n- **üìä Multi-Dataset Support**: Comprehensive evaluation across MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100\n- **‚öñÔ∏è Fair Resource Allocation**: Multiple data distribution schemes (Uniform, Linear, Quadratic/Exponential)\n- **üèóÔ∏è Modular Architecture**: Clean separation of concerns with containerized components\n\n## Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Client Nodes  ‚îÇ    ‚îÇ   Aggregator    ‚îÇ    ‚îÇ   Blockchain     ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ    Network       ‚îÇ\n‚îÇ ‚Ä¢ Local Training‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Model Fusion  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Smart Contracts‚îÇ\n‚îÇ ‚Ä¢ Data Privacy  ‚îÇ    ‚îÇ ‚Ä¢ Contribution  ‚îÇ    ‚îÇ ‚Ä¢ Token System   ‚îÇ\n‚îÇ ‚Ä¢ Model Upload  ‚îÇ    ‚îÇ   Assessment    ‚îÇ    ‚îÇ ‚Ä¢ Audit Trail    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Project Structure\n\n```\nBPFL/\n‚îú‚îÄ‚îÄ üìä Dataset Configurations & Results\n‚îÇ   ‚îú‚îÄ‚îÄ main.ipynb              # Centralized training baseline\n‚îÇ   ‚îú‚îÄ‚îÄ plot.py                 # Visualization and plotting utilities\n‚îÇ   ‚îî‚îÄ‚îÄ figures/                # Generated plots and visualizations\n‚îÇ\n‚îú‚îÄ‚îÄ üèóÔ∏è Core Infrastructure\n‚îÇ   ‚îú‚îÄ‚îÄ run.py                  # Main orchestration script\n‚îÇ   ‚îú‚îÄ‚îÄ stop.py                 # System shutdown utility\n‚îÇ   ‚îî‚îÄ‚îÄ test-network/           # Hyperledger Fabric network configuration\n‚îÇ\n‚îú‚îÄ‚îÄ üß† Federated Learning Components\n‚îÇ   ‚îú‚îÄ‚îÄ nodes/                  # FL participant implementations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py       # Central aggregation logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py           # Neural network architectures\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node[0-3].py       # Individual client nodes\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results/           # Training results and metrics\n‚îÇ   ‚îî‚îÄ‚îÄ perfed/                # Core \n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:51:05,166 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:51:05,166 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:05,167 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:05,167 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:05,167 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:05,167 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:10,004 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'4058'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'4268'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25317'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.365s'), (b'x-request-id', b'req_441bf831d39f4a1a891dd5087ad4c5e4'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb42c5d919607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:10,006 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:10,006 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:10,007 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:10,007 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:10,007 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:10,007 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:10 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '4058', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '4268', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25317', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.365s', 'x-request-id': 'req_441bf831d39f4a1a891dd5087ad4c5e4', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb42c5d919607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:10,007 [DEBUG] openai._base_client: request_id: req_441bf831d39f4a1a891dd5087ad4c5e4
2025-10-08 23:51:10,010 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-3c4060dc-cdda-494c-9c31-7b030a2afea7', 'json_data': {'messages': [{'content': "\n    You are an expert resume writer specializing in creating compelling project descriptions for technical resumes.\n    \n    Your task is to analyze the job posting and project details, then generate a LaTeX-formatted project entry that highlights the most relevant aspects for the target position.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    PROJECT DETAILS:\n    Title: PyFed\n    Description: PyFed is an open-source, lightweight federated learning framework that facilitates the implementation of federated learning algorithms using standard TensorFlow datasets or custom preprocessed data. The framework uses a client-server architecture with sockets, processes, and threads, and currently supports the FedAvg policy.\n    Tech Stack: Python, TensorFlow, scikit-learn, NumPy, Sockets, Threads, Processes, TensorBoard\n    \n    Detailed Documentation:\n    # PyFed\nPyFed is an open-source framework for federated learning. PyFed is fairly straightforward and brief in comparison to other federated learning frameworks. Furthermore, it allows running federated learning algorithms with any Tensorflow dataset on any preprocessed dataset. PyFed introduces several methods of federated learning implementation such as running multiple processes on a single machine and training on various systems. In addition, PyFed employs Tensorboard to demonstrate the history of training of each client and assess loss and accuracy of each client per round.\n</br>\nPyFed implements FL using sockets, processes, and threads. Simply put, each client will run its particular process and tries to establish a socket connection with the server, which also has its specific process. \nOnce initiated, each connection will be handled by one thread of the server's process. Each thread will communicate with its respective client to receive the trained weights per round. \nOnce they receive the result of one round, threads will return the weights to the server's process, which will arrive at a new model using the mentioned weights. The server will send the new model to the clients using newly initiated threads.\n</br>\nPyFed is mainly based on two classes:\n \n- __FL_Server__: which represents the server to which clients communicate in a federated learning problem. The __train()__ function of this class handles socket connections and the FL policy. </br>\n- __FL_Client__: which represents each client in a federated learning network. An object of this class handles training procedure of any global model on any local data.\n\nPyFed can run federated learning in 2 ways: \n\n1. Running FL only on one system and using separate processes.\n2. Running FL on multiple systems. \n\n</br>\nMore details are mentioned in the Usage section.\n</br>\nCurrently, PyFed is limited to FedAvg as its only federated learning policy; however, a broader range of configurations for FL experiments will be introduced in the future versions.\n\n# Features\nPyFed contains two critical classes: FL_Server and FL_Client, which are responsible for server and client actions in a federated learning problem, respectively. </br>\n* __FL_Server.train()__ establishes a socket connections with clients and handles weight averaging. In addition, at the end of all rounds a tensorboard session will be started to reveal the efficancy of each client.\n* __FL_Server.test()__ will test the final model on the given test data.\n* __FL_Client.train()__ will initiate a training session for the client who runs the command. Each client will train the received model on its local dataset.\n\n# Installation\n## MacOS\n    pip install pyfed-macos==0.0.34\n## Windows, Linux\n    pip install pyfed==0.0.34\n\n# Usage\nUtilizing PyFed is effortless and time efficient. Following are three approaches of running federated learning algorithms which can be implemented with PyFed. All of the examples below tackle the problem of classificati\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify key requirements and preferred technologies\n    2. From the project's tech stack, select 4-6 technologies that are MOST RELEVANT to the job requirements\n    3. Extract the most relevant project achievements and technical details\n    4. Be honest about the projects and do not hallucinate because something is need in the job description.\n    5. Generate a LaTeX project entry following this EXACT format:\n\n    \\resumeProjectHeading\n        {\\textbf{PyFed} $|$ \\emph{[Select 4-6 most job-relevant technologies from tech stack]} $|$ \\href{https://github.com/amirrezaskh/PyFed}{\\underline{Code}}} {}\n        \\resumeItemListStart\n            \\resumeItem{Key achievement/feature highlighting relevant technology with \\textbf{bold keywords}}\n            \\resumeItem{Technical implementation detail showing relevant skills with \\textbf{bold keywords}}\n            \\resumeItem{Impact/result with quantified metrics and \\textbf{bold keywords} where possible}\n        \\resumeItemListEnd\n\n    FORMATTING GUIDELINES:\n    - For the \\emph{} section, intelligently select 4-6 technologies from the project stack that are most relevant to the job posting\n    - Prioritize technologies explicitly mentioned in the job description\n    - Include complementary technologies that demonstrate full-stack or specialized capabilities relevant to the role\n    - Bold technical terms, frameworks, and methodologies mentioned in the job posting using \\textbf{}\n    - Focus on achievements and impact, not just features\n    - Include quantified results when available (percentages, time savings, performance improvements)\n    - IMPORTANT: Use proper LaTeX escaping - write \\% instead of percentage in text\n    - Use strong action verbs (Built, Developed, Implemented, Integrated, Designed, etc.)\n    - Tailor language to match job posting terminology\n    - Keep each \\resumeItem concise but impactful (1-2 lines max)\n    - Generate 3 \\resumeItem entries per project\n\n    Generate the LaTeX project entry:\n    ", 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:51:10,011 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:51:10,011 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:10,012 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:10,012 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:10,012 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:10,012 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:13,836 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'3656'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3722'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'25094'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'9.811s'), (b'x-request-id', b'req_8f7a923d734d481a8606b642cbe470bf'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb42e41c88607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:13,837 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:13,838 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:13,857 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:13,858 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:13,858 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:13,858 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:13 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '3656', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3722', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '25094', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '9.811s', 'x-request-id': 'req_8f7a923d734d481a8606b642cbe470bf', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb42e41c88607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:13,858 [DEBUG] openai._base_client: request_id: req_8f7a923d734d481a8606b642cbe470bf
2025-10-08 23:51:13,866 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d7566278-727b-4f9a-9066-7ba2e3d4a61e', 'json_data': {'messages': [{'content': '\n    You are an expert resume writer specializing in creating compelling "Highlight of Qualifications" sections that synthesize a candidate\'s experiences, skills, and projects into powerful qualification statements.\n    \n    Your task is to analyze the job posting and all provided resume content, then generate a LaTeX-formatted highlights section that positions the candidate as the ideal fit for the role.\n\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor\'s degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster\'s degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle\'s software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We\'re looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology\'s greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    CANDIDATE\'S EXPERIENCES:\n    \\resumeSubheading\n    {University of Manitoba}{September 2023 ‚Äì July 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed four innovative \\textbf{distributed AI architectures}, including PoCL and BSFL, addressing critical challenges in \\textbf{distributed learning}.}\n        \\resumeItem{Implemented \\textbf{blockchain integration} using \\textbf{Hyperledger Fabric} for enhanced model integrity and decentralized coordination.}\n        \\resumeItem{Optimized AI models with \\textbf{TensorFlow} and \\textbf{PyTorch}, achieving a reduction in communication overhead by \\textbf{85.2\\%} and improvement in fault tolerance by \\textbf{62.7\\%}.}\n        \\resumeItem{Authored research published in top-tier IEEE venues, demonstrating advanced \\textbf{technical communication} skills.}\n        \\resumeItem{Developed robust \\textbf{API} solutions using \\textbf{Express.js/Flask} for modular software development.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì August 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated product development through designing and implementing \\textbf{RESTful APIs} with \\textbf{Supabase} and \\textbf{PostgreSQL}, enhancing backend functionality.}\n        \\resumeItem{Automated data integration by developing a \\textbf{Python} script to convert CSV data into executable SQL, streamlining database population processes.}\n        \\resumeItem{Ensured seamless \\textbf{full-stack integration} by collaborating with front-end teams to align API specifications with user interface requirements.}\n        \\resumeItem{Contributed to \\textbf{agile workflow efficiency} using the Atlassian suite (\\textbf{Jira}, \\textbf{Confluence}) for task management and documentation.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {University of Manitoba}{September 2024 ‚Äì June 2025}\n    {Teaching Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Delivered academic and technical support to over 400 students in foundational Computer Science courses including \\textbf{Python}, \\textbf{Java}, and \\textbf{Data Structures \\& Algorithms}.}\n        \\resumeItem{Guided students through complex coding projects, emphasizing algorithm efficiency and \\textbf{system design} principles.}\n        \\resumeItem{Provided specialized technical assistance at the university\'s Help Centre, resolving programming issues and advising on effective coding practices.}\n    \\resumeItemListEnd\n\n    CANDIDATE\'S TECHNICAL SKILLS:\n    \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: TensorFlow, PyTorch, Transformers, Numpy, Pandas, Scikit-learn, Keras} \\\\\n    \\textbf{Languages}{: Python, C++, Java, JavaScript, TypeScript} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Linux, Git, GitHub, CI/CD, Kubernetes} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Redis, Db2} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React, Tailwind} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum, ClickUp}\n}}\n\\end{itemize}\n\n    CANDIDATE\'S PROJECTS:\n    \\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Hyperledger Fabric, Flask} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a \\textbf{Temporal Fusion Transformer (TFT)} for multi-variate time-series forecasting, achieving improved prediction accuracy for temperature, CO2, and humidity levels in \\textbf{smart buildings}.}\n        \\resumeItem{Developed a \\textbf{privacy-preserving} federated learning system by integrating \\textbf{Hyperledger Fabric} to ensure decentralized secure data sharing and trust, enhancing \\textbf{data security} and \\textbf{privacy} without centralized data storage.}\n        \\resumeItem{Enhanced system scalability and efficiency by orchestrating training rounds using \\textbf{Docker} containers and \\textbf{Flask} for smooth \\textbf{development} and \\textbf{deployment} processes, leading to \\textbf{50\\% faster} training procedure and lower \\textbf{computational cost}.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{Python, TensorFlow, Node.js, Express.js, Hyperledger Fabric, Docker} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a novel blockchain consensus mechanism called \\textbf{PoCL}, utilizing \\textbf{TensorFlow} for federated learning, transforming traditional mining to a more \\textbf{efficient} and \\textbf{productive} collaboration process.}\n        \\resumeItem{Implemented the system architecture using \\textbf{Hyperledger Fabric} for an \\textbf{immutable} blockchain ledger and \\textbf{Node.js} with \\textbf{Express.js} as an API gateway for process coordination, ensuring \\textbf{seamless} integration.}\n        \\resumeItem{Achieved a \\textbf{reduction} in energy consumption by replacing energy-intensive mining with a collaborative model training framework, leveraging \\textbf{Docker} for efficient deployment and scalability of the network.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{Python, PyTorch, Hyperledger Fabric, Node.js, JavaScript} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a decentralized framework integrating \\textbf{blockchain technology} with federated learning to ensure data privacy and secure transactions using \\textbf{Hyperledger Fabric}.}\n        \\resumeItem{Implemented \\textbf{personalized learning models} using \\textbf{PyTorch} and crafted a contribution-weighted aggregation method, addressing data heterogeneity across client nodes.}\n        \\resumeItem{Designed a \\textbf{token-based incentivization mechanism} to promote participation, resulting in a 30\\% improvement in model accuracy across datasets like MNIST and CIFAR-10.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{PyFed} $|$ \\emph{Python, TensorFlow, Sockets, Threads, Processes} $|$ \\href{https://github.com/amirrezaskh/PyFed}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a lightweight \\textbf{federated learning} framework using \\textbf{Python} and \\textbf{TensorFlow}, enabling seamless implementation of federated learning algorithms across standard datasets and custom preprocessed data.}\n        \\resumeItem{Implemented a \\textbf{client-server architecture} with \\textbf{Sockets} and \\textbf{Threads} to facilitate multi-process connections, improving the modularity and accessibility of federated learning implementations.}\n        \\resumeItem{Enhanced processing efficiency with \\textbf{Threads} and \\textbf{Processes}, achieving streamlined \\textbf{Federated Averaging (FedAvg)} policy execution and optimized resource utilization across systems.}\n    \\resumeItemListEnd\n\n    INSTRUCTIONS:\n    1. Analyze the job posting to identify the most critical qualifications and requirements\n    2. Review all the candidate\'s content (experiences, skills, projects) to extract relevant strengths\n    3. Synthesize this information into 5-7 compelling qualification highlights\n    4. Generate a LaTeX highlights section following this EXACT format:\n\n    \\resumeItem{\\textbf{Domain Area 1:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 2:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 3:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 4:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n    \\resumeItem{\\textbf{Domain Area 5:} Comprehensive statement showcasing relevant expertise with \\textbf{key technologies} and demonstrable outcomes.}\n\n    EXAMPLE OUTPUT:\n    \\resumeItem{\\textbf{Machine Learning \\& AI:} 5+ years developing \\textbf{deep learning} models using \\textbf{PyTorch} \\& \\textbf{TensorFlow} with 95\\% accuracy improvements.}\n\n    Note: Always use \\& instead of & for ampersands in LaTeX text.\n\n    FORMATTING GUIDELINES:\n    - Each highlight should start with a \\textbf{domain area} that matches job requirements\n    - Bold all technical skills, technologies, frameworks, and methodologies using \\textbf{}\n    - Include specific technologies and techniques mentioned in experiences and projects\n    - Quantify achievements where possible (percentages, scale, impact - use \\% for percentages in LaTeX)\n    - Use strong, confident language that demonstrates expertise\n    - Each highlight should be 1-2 lines maximum for readability\n    - Order highlights by importance to the job posting\n    - IMPORTANT: Use proper LaTeX escaping - write \\& instead of & for ampersands in text\n\n    Generate the LaTeX highlight of qualifications:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:51:13,871 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:51:13,871 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:13,872 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:13,872 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:13,872 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:13,872 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:28,350 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'14345'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'14377'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'23989'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'12.02s'), (b'x-request-id', b'req_cacf57277a464eb5bf49b08d6ccf753e'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb42fc3bc6607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:28,353 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:28,354 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:28,355 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:28,355 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:28,355 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:28,355 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '14345', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '14377', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '23989', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '12.02s', 'x-request-id': 'req_cacf57277a464eb5bf49b08d6ccf753e', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb42fc3bc6607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:28,355 [DEBUG] openai._base_client: request_id: req_cacf57277a464eb5bf49b08d6ccf753e
2025-10-08 23:51:29,724 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:51:29,730 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:51:29,730 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:51:29,730 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:51:29,730 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:51:29,730 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:51:29,732 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe5ec10>
2025-10-08 23:51:29,732 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,732 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:29,732 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,733 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:29,733 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,746 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:51:29 GMT')])
2025-10-08 23:51:29,746 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:51:29,746 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,746 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:29,746 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:29,746 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:29,747 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:51:29,754 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:51:29,754 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:51:29,754 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:51:29,754 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:51:29,754 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:51:29,755 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe5c210>
2025-10-08 23:51:29,755 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,755 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:29,755 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,756 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:29,756 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:51:29 GMT')])
2025-10-08 23:51:29,761 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:29,761 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:29,762 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:51:29 GMT')])
2025-10-08 23:51:29,762 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:51:29,762 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:29,762 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:29,762 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:29,763 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:29,765 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:29,765 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:29,765 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:29,766 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:29,766 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:29,775 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:51:29 GMT')])
2025-10-08 23:51:29,775 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:51:29,775 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:29,775 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:29,775 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:29,775 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:29,783 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-10065157-3afd-4ab4-ac1d-93cd9c1ab596', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe38ea0>, 'json_data': {'input': [[10714, 279, 2683, 198, 791, 3851, 3321, 690, 387, 1825, 3156, 520, 3325, 6664, 220, 1114, 11, 220, 2366, 20, 13, 1115, 6776, 690, 7293, 2930, 3196, 389, 2626, 3966, 902, 1253, 387, 1603, 477, 1306, 5300, 2457, 382, 9290, 25, 3296, 19486, 311, 420, 2361, 499, 690, 617, 459, 6776, 311, 4430, 701, 15236, 3318, 3813, 505, 279, 2768, 25, 77475, 11, 6328, 11, 7008, 26, 30613, 11, 43707, 11, 7008, 55068, 43784, 1473, 87831, 596, 8547, 304, 17863, 10170, 11, 264, 4528, 11156, 2115, 315, 4007, 11, 477, 13890, 15325, 3217, 627, 849, 716, 648, 2883, 25, 5195, 2361, 25, 4476, 25922, 11156, 7512, 25, 1144, 7413, 90, 1224, 553, 44489, 2414, 9113, 28, 15, 13, 868, 258, 11, 2440, 1185, 58420, 59, 9181, 36802, 1224, 517, 262, 1144, 1342, 13536, 90, 15836, 611, 13257, 21579, 15523, 25, 96086, 11, 5468, 51, 22312, 11, 81632, 11, 452, 6895, 11, 34606, 300, 11, 2522, 61503, 12, 12964, 11, 735, 9431, 92, 91255, 262, 1144, 1342, 13536, 90, 60386, 15523, 25, 13325, 11, 356, 23240, 8102, 11, 13210, 11, 88557, 92, 91255, 262, 1144, 1342, 13536, 90, 16440, 1144, 5, 6168, 40004, 15523, 25, 41649, 11, 14677, 11, 21804, 11, 33195, 11, 21351, 14, 6620, 11, 67474, 92, 91255, 262, 1144, 1342, 13536, 90, 35, 24760, 15523, 25, 74701, 11, 46428, 11, 27436, 11, 35258, 11, 12257, 17, 92, 91255, 262, 1144, 1342, 13536, 90, 6109, 24686, 82, 15523, 25, 1144, 336, 764, 90, 3792, 13368, 16487, 53704, 11, 29273, 11, 17855, 3587, 50, 13, 1144, 336, 764, 90, 24284, 13368, 16487, 3676, 11, 37179, 19703, 92, 91255, 262, 1144, 1342, 13536, 90, 16992, 1144, 5, 6872, 9268, 15523, 25, 622, 9008, 11, 1221, 41116, 11, 83284, 11, 2522, 10952, 11, 9369, 2378, 534, 11498, 59, 408, 90, 1224, 553, 92, 3217, 25, 1144, 42495, 3214, 11666, 198, 262, 314, 31272, 315, 64340, 15523, 30649, 220, 2366, 18, 1389, 5887, 220, 2366, 20, 534, 262, 314, 24956, 6426, 8483, 22103, 15523, 54, 6258, 47984, 11, 64340, 11, 7008, 534, 262, 1144, 42495, 83974, 3563, 198, 286, 1144, 42495, 1256, 90, 78233, 323, 8040, 3116, 18699, 1144, 1342, 13536, 90, 63475, 15592, 78335, 2186, 2737, 14128, 3218, 323, 28718, 6254, 11, 28118, 4742]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:51:29,783 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:51:29,784 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:51:29,809 [DEBUG] urllib3.connectionpool: Resetting dropped connection: us.i.posthog.com
2025-10-08 23:51:29,854 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe35590>
2025-10-08 23:51:29,854 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10fa63140> server_hostname='api.openai.com' timeout=None
2025-10-08 23:51:29,896 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe31a50>
2025-10-08 23:51:29,896 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:29,897 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:29,897 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:29,897 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:29,897 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:30,051 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:51:30,235 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'174'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-69d8474f6f-bt2tz'), (b'x-envoy-upstream-service-time', b'241'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999628'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'22ms'), (b'x-request-id', b'req_829baef41222477faf9bc40802deefc9'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Lw43UpVhbAVghvJ10wv8ARaPtVzhZffs8FJlIz.ZDPQ-1759985490-1.0.1.1-1UIqlYaoiy0K1hEWFrKPo5R7RxMUU5iBGfGoVW4dHD95ZOKMQ_nTxo4axFMNTMPF_Z5slXxYUKNLkzp2fHAX.Qh8YGPtE0pGhhUXepRDSlU; path=/; expires=Thu, 09-Oct-25 05:21:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=hfgVwnjtXjhIhlwGcBOGSU_JSIupi2IdHhZ8k81D9Fw-1759985490287-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb43606a9ae21f-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:30,236 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:51:30,237 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:30,240 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:30,241 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:30,241 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:30,241 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:51:30 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '174'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-69d8474f6f-bt2tz'), ('x-envoy-upstream-service-time', '241'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999628'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '22ms'), ('x-request-id', 'req_829baef41222477faf9bc40802deefc9'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Lw43UpVhbAVghvJ10wv8ARaPtVzhZffs8FJlIz.ZDPQ-1759985490-1.0.1.1-1UIqlYaoiy0K1hEWFrKPo5R7RxMUU5iBGfGoVW4dHD95ZOKMQ_nTxo4axFMNTMPF_Z5slXxYUKNLkzp2fHAX.Qh8YGPtE0pGhhUXepRDSlU; path=/; expires=Thu, 09-Oct-25 05:21:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=hfgVwnjtXjhIhlwGcBOGSU_JSIupi2IdHhZ8k81D9Fw-1759985490287-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb43606a9ae21f-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:51:30,241 [DEBUG] openai._base_client: request_id: req_829baef41222477faf9bc40802deefc9
2025-10-08 23:51:30,245 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:30,245 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:30,245 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:30,245 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:30,245 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:30,258 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'6503'), (b'date', b'Thu, 09 Oct 2025 04:51:29 GMT')])
2025-10-08 23:51:30,258 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-08 23:51:30,258 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:30,258 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:30,258 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:30,258 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:30,262 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'files': None, 'idempotency_key': 'stainless-python-retry-f0b6b952-7b9d-4a31-b3e1-25b73ce0a264', 'json_data': {'messages': [{'content': '\n    You are an expert cover letter writer specializing in creating compelling, personalized cover letters that effectively connect a candidate\'s background to specific job opportunities.\n\n    Your task is to analyze the job posting, personalized resume, and retrieved context to generate ONLY the cover letter content paragraphs in plain LaTeX format.\n\n    POSITION: Software Developer\n    COMPANY: Google\n    JOB POSTING:\n    About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor\'s degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster\'s degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle\'s software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We\'re looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology\'s greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.\n\n    PERSONALIZED RESUME CONTENT:\n    Highlights: \\resumeItem{\\textbf{Software Development:} Demonstrated expertise in \\textbf{Python}, \\textbf{C++}, \\textbf{Java}, and \\textbf{JavaScript} for developing scalable software systems, showcased by optimizing distributed AI architectures with significant communication overhead reduction by \\textbf{85.2\\%}.}\n\\resumeItem{\\textbf{Distributed Systems \\& Blockchain:} Innovatively designed and deployed \\textbf{distributed AI architectures} and integrated \\textbf{blockchain} solutions using \\textbf{Hyperledger Fabric}, ensuring decentralized data integrity and process coordination with seamless API integration.}\n\\resumeItem{\\textbf{AI \\& Machine Learning:} Specialized in \\textbf{TensorFlow} \\& \\textbf{PyTorch} for federated learning frameworks, achieving significant improvements in model accuracy and fault tolerance, including a \\textbf{62.7\\%} enhancement in resilience.}\n\\resumeItem{\\textbf{Data Structures \\& Algorithms:} Extensive experience providing academic support in \\textbf{Data Structures \\& Algorithms}, aiding over 400 students in developing efficient coding solutions and writing effective code in \\textbf{Python} and \\textbf{Java}.}\n\\resumeItem{\\textbf{Web \\& Mobile Development:} Advanced full-stack capabilities demonstrated by developing \\textbf{RESTful APIs} using \\textbf{Flask} \\& \\textbf{Express.js}, enhancing backend functionality and ensuring full-stack integration with robust frontend collaboration.}\n\\resumeItem{\\textbf{Collaboration \\& Agile Methodologies:} Strong proficiency in agile development; accelerated workflows through \\textbf{Jira}, \\textbf{Confluence}, and \\textbf{Scrum}, ensuring efficient project management and seamless teamwork across multidisciplinary environments.}\n    Experiences: \\resumeSubheading\n    {University of Manitoba}{September 2023 ‚Äì July 2025}\n    {Graduate Research Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Designed and developed four innovative \\textbf{distributed AI architectures}, including PoCL and BSFL, addressing critical challenges in \\textbf{distributed learning}.}\n        \\resumeItem{Implemented \\textbf{blockchain integration} using \\textbf{Hyperledger Fabric} for enhanced model integrity and decentralized coordination.}\n        \\resumeItem{Optimized AI models with \\textbf{TensorFlow} and \\textbf{PyTorch}, achieving a reduction in communication overhead by \\textbf{85.2\\%} and improvement in fault tolerance by \\textbf{62.7\\%}.}\n        \\resumeItem{Authored research published in top-tier IEEE venues, demonstrating advanced \\textbf{technical communication} skills.}\n        \\resumeItem{Developed robust \\textbf{API} solutions using \\textbf{Express.js/Flask} for modular software development.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {Bobo}{May 2024 ‚Äì August 2024}\n    {Full-stack Developer Intern}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Accelerated product development through designing and implementing \\textbf{RESTful APIs} with \\textbf{Supabase} and \\textbf{PostgreSQL}, enhancing backend functionality.}\n        \\resumeItem{Automated data integration by developing a \\textbf{Python} script to convert CSV data into executable SQL, streamlining database population processes.}\n        \\resumeItem{Ensured seamless \\textbf{full-stack integration} by collaborating with front-end teams to align API specifications with user interface requirements.}\n        \\resumeItem{Contributed to \\textbf{agile workflow efficiency} using the Atlassian suite (\\textbf{Jira}, \\textbf{Confluence}) for task management and documentation.}\n    \\resumeItemListEnd\n\n\\resumeSubheading\n    {University of Manitoba}{September 2024 ‚Äì June 2025}\n    {Teaching Assistant}{Winnipeg, Manitoba, Canada}\n    \\resumeItemListStart\n        \\resumeItem{Delivered academic and technical support to over 400 students in foundational Computer Science courses including \\textbf{Python}, \\textbf{Java}, and \\textbf{Data Structures \\& Algorithms}.}\n        \\resumeItem{Guided students through complex coding projects, emphasizing algorithm efficiency and \\textbf{system design} principles.}\n        \\resumeItem{Provided specialized technical assistance at the university\'s Help Centre, resolving programming issues and advising on effective coding practices.}\n    \\resumeItemListEnd\n    Skills: \\begin{itemize}[leftmargin=0.15in, label={}]\n\\small{\\item{\n    \\textbf{AI / Machine Learning}{: TensorFlow, PyTorch, Transformers, Numpy, Pandas, Scikit-learn, Keras} \\\\\n    \\textbf{Languages}{: Python, C++, Java, JavaScript, TypeScript} \\\\\n    \\textbf{Cloud \\& DevOps}{: Docker, Linux, Git, GitHub, CI/CD, Kubernetes} \\\\\n    \\textbf{Databases}{: PostgreSQL, MongoDB, MySQL, Redis, Db2} \\\\\n    \\textbf{Web Frameworks}{: \\emph{Back-end}: Django, Flask, Express.JS. \\emph{Front-end}: React, Tailwind} \\\\\n    \\textbf{Tools \\& Methodologies}{: Jira, Confluence, Agile, Scrum, ClickUp}\n}}\n\\end{itemize}\n    Projects: \\resumeProjectHeading\n    {\\textbf{Federated Learning enabled Digital Twin} $|$ \\emph{Python, PyTorch, Docker, Hyperledger Fabric, Flask} $|$ \\href{https://github.com/amirrezaskh/DigitalTwin}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Implemented a \\textbf{Temporal Fusion Transformer (TFT)} for multi-variate time-series forecasting, achieving improved prediction accuracy for temperature, CO2, and humidity levels in \\textbf{smart buildings}.}\n        \\resumeItem{Developed a \\textbf{privacy-preserving} federated learning system by integrating \\textbf{Hyperledger Fabric} to ensure decentralized secure data sharing and trust, enhancing \\textbf{data security} and \\textbf{privacy} without centralized data storage.}\n        \\resumeItem{Enhanced system scalability and efficiency by orchestrating training rounds using \\textbf{Docker} containers and \\textbf{Flask} for smooth \\textbf{development} and \\textbf{deployment} processes, leading to \\textbf{50\\% faster} training procedure and lower \\textbf{computational cost}.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Proof of Collaborative Learning (PoCL)} $|$ \\emph{Python, TensorFlow, Node.js, Express.js, Hyperledger Fabric, Docker} $|$ \\href{https://github.com/amirrezaskh/Proof-of-Collaborative-Learning}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a novel blockchain consensus mechanism called \\textbf{PoCL}, utilizing \\textbf{TensorFlow} for federated learning, transforming traditional mining to a more \\textbf{efficient} and \\textbf{productive} collaboration process.}\n        \\resumeItem{Implemented the system architecture using \\textbf{Hyperledger Fabric} for an \\textbf{immutable} blockchain ledger and \\textbf{Node.js} with \\textbf{Express.js} as an API gateway for process coordination, ensuring \\textbf{seamless} integration.}\n        \\resumeItem{Achieved a \\textbf{reduction} in energy consumption by replacing energy-intensive mining with a collaborative model training framework, leveraging \\textbf{Docker} for efficient deployment and scalability of the network.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{Blockchain-enabled Personalized Federated Learning (BPFL)} $|$ \\emph{Python, PyTorch, Hyperledger Fabric, Node.js, JavaScript} $|$ \\href{https://github.com/amirrezaskh/BPFL}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a decentralized framework integrating \\textbf{blockchain technology} with federated learning to ensure data privacy and secure transactions using \\textbf{Hyperledger Fabric}.}\n        \\resumeItem{Implemented \\textbf{personalized learning models} using \\textbf{PyTorch} and crafted a contribution-weighted aggregation method, addressing data heterogeneity across client nodes.}\n        \\resumeItem{Designed a \\textbf{token-based incentivization mechanism} to promote participation, resulting in a 30\\% improvement in model accuracy across datasets like MNIST and CIFAR-10.}\n    \\resumeItemListEnd\n\n\\resumeProjectHeading\n    {\\textbf{PyFed} $|$ \\emph{Python, TensorFlow, Sockets, Threads, Processes} $|$ \\href{https://github.com/amirrezaskh/PyFed}{\\underline{Code}}} {}\n    \\resumeItemListStart\n        \\resumeItem{Developed a lightweight \\textbf{federated learning} framework using \\textbf{Python} and \\textbf{TensorFlow}, enabling seamless implementation of federated learning algorithms across standard datasets and custom preprocessed data.}\n        \\resumeItem{Implemented a \\textbf{client-server architecture} with \\textbf{Sockets} and \\textbf{Threads} to facilitate multi-process connections, improving the modularity and accessibility of federated learning implementations.}\n        \\resumeItem{Enhanced processing efficiency with \\textbf{Threads} and \\textbf{Processes}, achieving streamlined \\textbf{Federated Averaging (FedAvg)} policy execution and optimized resource utilization across systems.}\n    \\resumeItemListEnd\n\n    RETRIEVED CONTEXT FROM KNOWLEDGE BASE:\n    [Document(id=\'a26731e0-1174-4ef3-9f78-7ffa6a7b7ee8\', metadata={\'start_index\': 0, \'company\': \'BMO\', \'position\': \'AI Developer\', \'source\': \'cover letter\'}, page_content=\'I am excited to apply for the AI Developer position at \\\\textbf{BMO}. With my background in \\\\textbf{AI/ML development} and extensive experience in \\\\textbf{cloud technologies}, I am eager to contribute to your team‚Äôs mission of advancing AI-driven business-to-consumer applications.\\n\\nIn my recent projects, I have built scalable applications using \\\\textbf{TensorFlow, PyTorch}, and various \\\\textbf{cloud services} that align with your requirements. For example, in the \\\\textbf{Federated Learning enabled Digital Twin} project, I developed innovative AI architectures to enhance data privacy using \\\\textbf{Hyperledger Fabric}, achieving a significant improvement in energy efficiency by 62.7\\\\%. My experience with \\\\textbf{AWS Sagemaker} and \\\\textbf{Azure AI services} has prepared me to deploy AI/ML models effectively, a key requirement in the job posting.\'), Document(id=\'b3b13048-0a08-4014-8bdb-713ff0facdec\', metadata={\'position\': \'AI Developer\', \'company\': \'BMO\', \'source\': \'cover letter\', \'start_index\': 0}, page_content="I am excited to apply for the AI Developer position at \\\\textbf{BMO}. With my background in \\\\textbf{AI/ML development} and extensive experience in \\\\textbf{cloud services}, I am eager to contribute to your team\'s mission of leveraging AI to enhance business-to-consumer applications. As a dedicated professional, I am thrilled at the opportunity to advance BMO\'s initiative to innovate within financial services through the use of cutting-edge AI technologies."), Document(id=\'645f87c1-7931-4aad-ae8a-145e118414c9\', metadata={\'company\': \'Ascendion\', \'start_index\': 507, \'source\': \'cover letter\', \'position\': \'Python Developer\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'8cd7ca4e-dc41-42e4-a9b8-c8b1250e6183\', metadata={\'position\': \'Python Developer\', \'company\': \'Ascendion\', \'start_index\': 507, \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For instance, during my tenure as a Graduate Research Assistant, I designed \\\\textbf{distributed AI architectures} and developed \\\\textbf{modular APIs} using \\\\textbf{Flask} that enhanced backend functionality by 25\\\\%. Additionally, in my role as a Full-stack Developer Intern at Bobo, I developed \\\\textbf{RESTful APIs} using \\\\textbf{PostgreSQL}, which improved data management and accelerated product development.\'), Document(id=\'a1bfdd36-5dad-4d9b-bb94-46b08067f5bf\', metadata={\'company\': \'Apple\', \'start_index\': 511, \'position\': \'Python Developer\', \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'4bd89d99-19d6-45a8-a6ff-7184a911e904\', metadata={\'company\': \'Apple\', \'start_index\': 513, \'position\': \'Python Developer\', \'source\': \'cover letter\'}, page_content=\'In my recent projects, I have developed scalable applications using \\\\textbf{Python} and \\\\textbf{AI} technologies that align with your requirements. For example, in the \\\\textbf{MarkMate} project, I created an AI-powered grading platform utilizing \\\\textbf{GPT-4} and a \\\\textbf{Django} REST API, significantly reducing manual grading time by automating assignment evaluations. Additionally, my role as a Graduate Research Assistant involved designing \\\\textbf{distributed AI architectures} and implementing novel \\\\textbf{AI model architectures} with \\\\textbf{TensorFlow} and \\\\textbf{PyTorch}, achieving an 85.2\\\\% reduction in communication overhead.\'), Document(id=\'115e47f4-6c38-4721-9fec-7112f646494c\', metadata={\'source\': \'cover letter\', \'position\': \'AI Developer\', \'company\': \'BMO\', \'start_index\': 1111}, page_content="I am particularly drawn to \\\\textbf{BMO}\'s forward-thinking approach to AI integration within Personal \\\\& Business Banking. My academic research, prominently presented in top-tier IEEE venues, has honed my problem-solving skills in the \\\\textbf{AI/ML} domain, and I am eager to apply this knowledge to inform business and data science stakeholders on AI capabilities. Moreover, my proficiency in \\\\textbf{AWS} and \\\\textbf{Azure} AI services‚Äîenhanced by cloud certifications‚Äîpositions me as a valuable asset for contributing to the operationalization of AI/ML models in your cloud initiatives.\\n\\nWith a proven track record of excellence in AI/ML model deployment and a passion for innovation, I am enthusiastic about the prospect of joining BMO and contributing to the development of transformative AI solutions. I am confident that my technical skills and dedication to continuous learning will significantly benefit your team."), Document(id=\'c426e5ad-f8b6-49cb-a846-e2c4561bf7bd\', metadata={\'company\': \'Apple\', \'source\': \'cover letter\', \'start_index\': 0, \'position\': \'Python Developer\'}, page_content=\'I am excited to apply for the Python Developer position at \\\\textbf{Apple}. With my extensive background in \\\\textbf{Python development} and my experience in frameworks such as \\\\textbf{Django}, \\\\textbf{Flask}, and \\\\textbf{FastAPI}, I am eager to contribute to your team‚Äôs mission of engineering captivating digital experiences. The prospect of being part of Apple‚Äôs culture, which emphasizes opportunity, inclusion, and innovation, motivates me to bring my skills in software engineering to your esteemed company.\')]\n                                                       \n    INSTRUCTIONS:\n    1. Analyze the job posting to identify:\n       - Company mission, values, and culture\n       - Key technical requirements and preferred technologies\n       - Required experience level and responsibilities\n       - Specific skills and qualifications sought\n\n    2. Use the personalized resume content to:\n       - Highlight the most relevant experiences and achievements\n       - Showcase technical skills that match job requirements\n       - Reference specific projects that demonstrate required capabilities\n       - Quantify accomplishments where possible\n\n    3. Leverage the retrieved context to:\n       - Add depth and authenticity to your statements\n       - Reference relevant research, projects, or experiences not in the resume\n       - Demonstrate domain knowledge and passion for the field\n       - Show progression and learning from past experiences\n\n    4. Follow the template structure but adapt content to:\n       - Match the company\'s tone and industry language\n       - Create a narrative that connects past experiences to future contributions\n       - Show genuine enthusiasm for the specific role and company\n       - Address any potential gaps or explain career transitions\n\n\n    CRITICAL OUTPUT REQUIREMENTS:\n    - Generate ONLY the cover letter content paragraphs\n    - DO NOT include any LaTeX document structure (\\documentclass, \\begin{document}, \\begin{letter}, \\end{letter}, \\end{document})\n    - DO NOT include any explanations, introductions, or meta-commentary\n    - DO NOT include salutations like "Dear Hiring Manager" or closings like "Sincerely"\n    - Start directly with the first paragraph content\n    - End with the last paragraph content\n    - Use clean paragraph breaks (double newlines) between paragraphs\n\n    CONTENT STRUCTURE:\n    Write exactly 3-4 paragraphs of cover letter content:\n\n    Paragraph 1: Express enthusiasm for the specific role and company, briefly state your relevant background and why you\'re applying.\n\n    Paragraph 2: Highlight your most relevant technical experiences and projects with specific examples that match the job requirements.\n\n    Paragraph 3: Demonstrate alignment with the company\'s mission and technology stack, referencing retrieved context to show deeper knowledge and genuine interest.\n\n    Paragraph 4 (optional): Reiterate enthusiasm and mention specific contributions you can make to the team and company.\n\n                                                       \n    COVER LETTER STRUCTURE:\n    1. Opening paragraph: Express enthusiasm for the specific role and company, briefly state your relevant background\n    2. Body paragraph 1: Highlight relevant technical experience and projects with specific examples\n    3. Body paragraph 2: Demonstrate alignment with company mission/technology stack, reference retrieved context for depth\n    4. Closing paragraph: Reiterate enthusiasm, mention specific contributions you can make, professional closing\n\n    TONE AND STYLE:\n    - Professional yet enthusiastic\n    - Confident but not boastful\n    - Specific and detailed, not generic\n    - Forward-looking and solution-oriented\n    - Authentic and genuine\n    LATEX FORMATTING REQUIREMENTS:\n    - Use proper LaTeX escaping: \\& instead of & for ampersands\n    - Use \\% for percentages in text  \n    - Use \\textbf{} for emphasizing key technologies, company names, and achievements\n    - Write in clean, readable paragraph format\n    - Use standard paragraph separation (double newlines)\n\n    CONTENT GUIDELINES:\n    - Reference specific technologies, frameworks, and methodologies from the job posting\n    - Use metrics and quantified achievements where available\n    - Show understanding of the company\'s challenges and how you can help\n    - Demonstrate continuous learning and adaptability\n    - Connect past experiences to future potential contributions\n    - Use retrieved context to add unique insights or relevant background\n    - Be professional yet enthusiastic, confident but not boastful\n    - Make it specific and detailed, not generic\n\n    OUTPUT FORMAT EXAMPLE:\n    I am excited to apply for the {position} position at \\textbf{{company name}}. With my background in \\textbf{relevant technology} and experience in \\textbf{relevant domain}, I am eager to contribute to your team\'s mission of advancing \\textbf{company focus area}.\n\n    In my recent projects, I have built scalable applications using \\textbf{specific technologies} that align with your requirements. For example, in my \\textbf{project name} project, I developed \\textbf{specific implementation} that resulted in \\textbf{quantified outcome}. My experience with \\textbf{relevant technology stack} has prepared me to tackle the challenges outlined in your job posting.\n\n    I am particularly drawn to \\textbf{{company name}}\'s commitment to \\textbf{company values/mission}. My research in \\textbf{relevant area from context} has given me deep insights into \\textbf{relevant domain knowledge}, and I am excited about the opportunity to apply this knowledge in a production environment where I can help \\textbf{specific company goals}.\n\n    Generate the cover letter content paragraphs following this exact format:\n    ', 'role': 'user'}], 'model': 'gpt-4o', 'stream': False}}
2025-10-08 23:51:30,266 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-10-08 23:51:30,266 [DEBUG] httpcore.connection: close.started
2025-10-08 23:51:30,267 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:51:30,269 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:30,269 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:30,269 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:30,269 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:30,269 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:40,283 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'9870'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'9900'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'23789'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'12.422s'), (b'x-request-id', b'req_b63ecd50a2e24cfe995273169b25776c'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb4362bd61607d-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:40,285 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-08 23:51:40,285 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:40,286 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:40,286 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:40,286 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:40,286 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '9870', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '9900', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '23789', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '12.422s', 'x-request-id': 'req_b63ecd50a2e24cfe995273169b25776c', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb4362bd61607d-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:40,286 [DEBUG] openai._base_client: request_id: req_b63ecd50a2e24cfe995273169b25776c
2025-10-08 23:51:42,064 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:51:42,070 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:51:42,070 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:51:42,070 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:51:42,070 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:51:42,071 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:51:42,072 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe49b50>
2025-10-08 23:51:42,072 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,072 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,072 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,072 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,072 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,073 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'73'), (b'date', b'Thu, 09 Oct 2025 04:51:42 GMT')])
2025-10-08 23:51:42,073 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-08 23:51:42,073 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,073 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,073 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,073 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,074 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-08 23:51:42,080 [DEBUG] chromadb.config: Starting component System
2025-10-08 23:51:42,080 [DEBUG] chromadb.config: Starting component Posthog
2025-10-08 23:51:42,080 [DEBUG] chromadb.config: Starting component OpenTelemetryClient
2025-10-08 23:51:42,080 [DEBUG] chromadb.config: Starting component FastAPI
2025-10-08 23:51:42,081 [DEBUG] httpcore.connection: connect_tcp.started host='localhost' port=8000 local_address=None timeout=None socket_options=None
2025-10-08 23:51:42,081 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe36cd0>
2025-10-08 23:51:42,082 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,082 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,082 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,082 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,082 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'46'), (b'date', b'Thu, 09 Oct 2025 04:51:42 GMT')])
2025-10-08 23:51:42,083 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,083 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'97'), (b'date', b'Thu, 09 Oct 2025 04:51:42 GMT')])
2025-10-08 23:51:42,084 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,084 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,086 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:42,086 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,086 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:42,086 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,086 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:42,087 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'363'), (b'date', b'Thu, 09 Oct 2025 04:51:42 GMT')])
2025-10-08 23:51:42,087 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-08 23:51:42,087 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:42,087 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,087 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,087 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,090 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-5d1d18ca-7760-41e6-88ab-f2151cd9f43e', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe3a700>, 'json_data': {'input': [[40, 1097, 12304, 311, 3881, 369, 279, 4476, 25922, 2361, 520, 1144, 1342, 13536, 90, 14783, 7966, 3161, 856, 16781, 4092, 304, 1144, 1342, 13536, 90, 37751, 4500, 92, 323, 3217, 304, 30829, 1144, 1342, 13536, 90, 63475, 15592, 78335, 2186, 358, 1097, 24450, 311, 17210, 311, 701, 2128, 596, 9131, 315, 11469, 18699, 14645, 430, 4667, 323, 26285, 3932, 15603, 382, 644, 856, 3293, 7224, 11, 358, 617, 34716, 1144, 1342, 13536, 90, 31380, 11, 356, 23240, 8102, 2186, 323, 1144, 1342, 13536, 90, 30575, 92, 311, 2274, 69311, 3241, 6067, 430, 5398, 449, 701, 8670, 13, 1789, 3187, 11, 2391, 856, 40061, 439, 264, 44825, 8483, 22103, 11, 358, 34440, 4332, 15592, 78335, 555, 18189, 10758, 32115, 555, 1144, 1342, 13536, 90, 5313, 13, 17, 59, 4, 7966, 3092, 3217, 449, 1144, 1342, 13536, 90, 39407, 1285, 34456, 2186, 1144, 1342, 13536, 90, 3968, 1091, 2186, 323, 1144, 1342, 13536, 90, 8672, 2927, 92, 72849, 44993, 323, 11297, 19713, 4500, 11, 18899, 2539, 57190, 18052, 13], [40, 1097, 8104, 15107, 311, 1144, 1342, 13536, 90, 14783, 11923, 82, 15507, 311, 19297, 323, 11850, 2038, 520, 264, 11191, 5569, 13, 3092, 990, 389, 1144, 1342, 13536, 90, 4677, 8995, 18052, 92, 1701, 1144, 1342, 13536, 90, 75046, 51804, 37407, 92, 706, 3984, 757, 449, 5655, 26793, 1139, 49063, 828, 17025, 11, 323, 358, 1097, 12304, 922, 279, 6776, 311, 12178, 5195, 596, 9131, 555, 19486, 856, 19248, 304, 4332, 6067, 323, 15592, 311, 11886, 6485, 5435, 304, 1972, 31184, 8522, 382, 40, 1097, 42702, 922, 18667, 5195, 596, 8915, 2128, 11, 1405, 358, 649, 4546, 13544, 856, 7512, 304, 1144, 1342, 13536, 90, 15836, 92, 323, 1144, 1342, 13536, 90, 33156, 6975, 92, 311, 17210, 311, 6968, 87435, 14645, 13, 3161, 856, 17033, 3839, 3335, 304, 77582, 11084, 14645, 11, 358, 1097, 16913, 304, 856, 5845, 311, 1304, 23222, 19564, 311, 701, 18699, 7224, 13]], 'model': 'text-embedding-3-large', 'encoding_format': 'base64'}}
2025-10-08 23:51:42,091 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:51:42,092 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-10-08 23:51:42,125 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe4cf10>
2025-10-08 23:51:42,125 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x10fa62570> server_hostname='api.openai.com' timeout=None
2025-10-08 23:51:42,161 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe43790>
2025-10-08 23:51:42,161 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:42,162 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,162 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:42,162 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,162 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:42,280 [DEBUG] urllib3.connectionpool: https://us.i.posthog.com:443 "POST /batch/ HTTP/1.1" 200 15
2025-10-08 23:51:42,946 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-large'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'80'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-76f88f4767-n68nt'), (b'x-envoy-upstream-service-time', b'265'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999684'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'18ms'), (b'x-request-id', b'req_f3631773e4a4459b9c90a48c933d0946'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yGmBarLTinBpkURDR20TzRgyGQ0TBsC5BnYHK.LMva0-1759985502-1.0.1.1-KKAlV29bW3GY.VbaCq0.7HML3kHqH9jvIyu9Q_EIavt4FjYpktV3Cvy05Jr9BgFV3gyZhBeOkzrab0Z.HL5WK0hoOvIMJcpiZWXHrEHRqlM; path=/; expires=Thu, 09-Oct-25 05:21:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=iRp.52O2o5vhRy81f.wZiaGrOvqdgtd7QoHR24YWZlU-1759985502937-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb43ad0c4561fc-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:42,946 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:51:42,946 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:42,947 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,947 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,947 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,947 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers([('date', 'Thu, 09 Oct 2025 04:51:42 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-large'), ('openai-organization', 'personal-moaon6'), ('openai-processing-ms', '80'), ('openai-project', 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-76f88f4767-n68nt'), ('x-envoy-upstream-service-time', '265'), ('x-ratelimit-limit-requests', '3000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '2999'), ('x-ratelimit-remaining-tokens', '999684'), ('x-ratelimit-reset-requests', '20ms'), ('x-ratelimit-reset-tokens', '18ms'), ('x-request-id', 'req_f3631773e4a4459b9c90a48c933d0946'), ('x-openai-proxy-wasm', 'v0.1'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=yGmBarLTinBpkURDR20TzRgyGQ0TBsC5BnYHK.LMva0-1759985502-1.0.1.1-KKAlV29bW3GY.VbaCq0.7HML3kHqH9jvIyu9Q_EIavt4FjYpktV3Cvy05Jr9BgFV3gyZhBeOkzrab0Z.HL5WK0hoOvIMJcpiZWXHrEHRqlM; path=/; expires=Thu, 09-Oct-25 05:21:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=iRp.52O2o5vhRy81f.wZiaGrOvqdgtd7QoHR24YWZlU-1759985502937-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98bb43ad0c4561fc-ORD'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-10-08 23:51:42,947 [DEBUG] openai._base_client: request_id: req_f3631773e4a4459b9c90a48c933d0946
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'GET']>
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'55'), (b'date', b'Thu, 09 Oct 2025 04:51:42 GMT')])
2025-10-08 23:51:42,949 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'GET']>
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:42,949 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:42,952 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:42,952 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:42,953 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:42,953 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:42,953 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:43,002 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'content-type', b'application/json'), (b'content-length', b'2'), (b'date', b'Thu, 09 Oct 2025 04:51:43 GMT')])
2025-10-08 23:51:43,003 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-08 23:51:43,003 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:43,003 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:43,003 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:43,003 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:43,022 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-a8d66c73-348c-429a-b113-1263cf8a3103', 'post_parser': <function Embeddings.create.<locals>.parser at 0x10fe39080>, 'json_data': {'input': "Company: Google. Position: Software Developer. Description: About the job\nThe application window will be open until at least October 17, 2025. This opportunity will remain online based on business needs which may be before or after specified date.\n\nNote: By applying to this position you will have an opportunity to share your preferred working location from the following: Waterloo, ON, Canada; Montreal, QC, Canada.Minimum qualifications:\n\nBachelor's degree in Computer Science, a similar technical field of study, or equivalent practical experience.\nExperience working with data structures or algorithms during coursework/projects, research, internships, or practical experience in school or work (e.g. open-source coding).\nExperience with software development in one or more programming languages (e.g., Python, C, C++, Java, JavaScript).\n\nPreferred qualifications:\n\nMaster's degree in Computer Science or a related technical field.\nExperience working with one or more of the following: web or mobile application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, or security software development.\nExperience developing accessible technologies.\nAbility to learn coding languages as needed.\n\nAbout The Job\n\nGoogle's software developers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for software developers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software developer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our software developers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.\n\nGoogle aspires to be an organization that reflects the global audience that our products and technology serve. We believe that in addition to hiring talent, a variety of perspectives, ideas, and cultures leads to the creation of better products and services. You will be considered for all potential Software Development opportunities. Roles may vary by location and will be discussed in more detail with your recruiter after completion of onsite interviews.\n\nGoogle is a software development company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, software developers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google software developers are changing the world one technological achievement after another.\n\nResponsibilities\n\nDesign, develop, test, deploy, maintain, and improve software.\nManage individual project priorities, deadlines, and deliverables.\nTake on tasks as requested, following through to completion despite roadblocks or distractions.", 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}
2025-10-08 23:51:43,023 [DEBUG] openai._base_client: Sending HTTP Request: POST https://api.openai.com/v1/embeddings
2025-10-08 23:51:43,023 [DEBUG] httpcore.connection: close.started
2025-10-08 23:51:43,023 [DEBUG] httpcore.connection: close.complete
2025-10-08 23:51:43,023 [DEBUG] httpcore.connection: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-08 23:51:43,057 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe5f510>
2025-10-08 23:51:43,057 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x103fd32f0> server_hostname='api.openai.com' timeout=5.0
2025-10-08 23:51:43,094 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10fe456d0>
2025-10-08 23:51:43,094 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-10-08 23:51:43,094 [DEBUG] httpcore.http11: send_request_headers.complete
2025-10-08 23:51:43,094 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-10-08 23:51:43,095 [DEBUG] httpcore.http11: send_request_body.complete
2025-10-08 23:51:43,095 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-10-08 23:51:43,936 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 09 Oct 2025 04:51:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-3-small'), (b'openai-organization', b'personal-moaon6'), (b'openai-processing-ms', b'71'), (b'openai-project', b'proj_xpqpZ6mXeDEwLbfYGvXUcwd5'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-5c84678cfc-7dqgh'), (b'x-envoy-upstream-service-time', b'252'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999103'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'53ms'), (b'x-request-id', b'req_174fa214c3684552ac5e560208d82455'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98bb43b2efc8233e-ORD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-10-08 23:51:43,937 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-08 23:51:43,938 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-10-08 23:51:43,939 [DEBUG] httpcore.http11: receive_response_body.complete
2025-10-08 23:51:43,939 [DEBUG] httpcore.http11: response_closed.started
2025-10-08 23:51:43,939 [DEBUG] httpcore.http11: response_closed.complete
2025-10-08 23:51:43,939 [DEBUG] openai._base_client: HTTP Response: POST https://api.openai.com/v1/embeddings "200 OK" Headers({'date': 'Thu, 09 Oct 2025 04:51:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-3-small', 'openai-organization': 'personal-moaon6', 'openai-processing-ms': '71', 'openai-project': 'proj_xpqpZ6mXeDEwLbfYGvXUcwd5', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-5c84678cfc-7dqgh', 'x-envoy-upstream-service-time': '252', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999103', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '53ms', 'x-request-id': 'req_174fa214c3684552ac5e560208d82455', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '98bb43b2efc8233e-ORD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-10-08 23:51:43,940 [DEBUG] openai._base_client: request_id: req_174fa214c3684552ac5e560208d82455
2025-10-08 23:51:43,966 [INFO] src.api.app: ‚úÖ POST /api/generate/ - 200 - 70.206s
2025-10-08 23:51:43,966 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 70.206s
2025-10-08 23:51:43,966 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:51:43] "POST /api/generate/ HTTP/1.1" 200 -
2025-10-08 23:51:50,647 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-08 23:51:50,648 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.002s
2025-10-08 23:51:50,649 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:51:50] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-08 23:51:53,142 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-08 23:51:53,146 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.004s
2025-10-08 23:51:53,148 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:51:53] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-08 23:55:57,842 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-08 23:55:57,846 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.004s
2025-10-08 23:55:57,850 [INFO] werkzeug: 127.0.0.1 - - [08/Oct/2025 23:55:57] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:03:04,890 [INFO] werkzeug:  * Detected change in '/Users/amirrezasokhankhosh/Documents/Workstation/Aria/agent/src/api/middleware/logging.py', reloading
2025-10-09 00:03:05,245 [DEBUG] httpcore.connection: close.started
2025-10-09 00:03:05,246 [DEBUG] httpcore.connection: close.complete
2025-10-09 00:03:05,246 [DEBUG] httpcore.connection: close.started
2025-10-09 00:03:05,247 [DEBUG] httpcore.connection: close.complete
2025-10-09 00:03:05,247 [DEBUG] httpcore.connection: close.started
2025-10-09 00:03:05,247 [DEBUG] httpcore.connection: close.complete
2025-10-09 00:03:05,248 [DEBUG] httpcore.connection: close.started
2025-10-09 00:03:05,248 [DEBUG] httpcore.connection: close.complete
2025-10-09 00:03:05,333 [DEBUG] httpcore.connection: close.started
2025-10-09 00:03:05,333 [DEBUG] httpcore.connection: close.complete
2025-10-09 00:03:05,558 [INFO] werkzeug:  * Restarting with stat
2025-10-09 00:03:09,751 [WARNING] werkzeug:  * Debugger is active!
2025-10-09 00:03:09,761 [INFO] werkzeug:  * Debugger PIN: 246-503-739
2025-10-09 00:04:46,823 [INFO] werkzeug:  * Detected change in '/Users/amirrezasokhankhosh/Documents/Workstation/Aria/agent/src/api/app.py', reloading
2025-10-09 00:04:47,031 [INFO] werkzeug:  * Restarting with stat
2025-10-09 00:05:04,917 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-09 00:05:04,917 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-09 00:05:49,733 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-09 00:05:49,736 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.003s
2025-10-09 00:05:49,738 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:05:49] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:05:49,741 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-09 00:05:50,696 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.955s
2025-10-09 00:05:50,697 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:05:50] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:05:50,705 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-09 00:05:50,705 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-09 00:05:50,705 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:05:50] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:05:50,707 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-09 00:06:41,616 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:06:41,648 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:06:42,091 [INFO] src.api.app: ‚ùå POST /api/generate/ - 500 - 51.383s
2025-10-09 00:06:42,091 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 51.383s
2025-10-09 00:06:42,091 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:06:42] "[35m[1mPOST /api/generate/ HTTP/1.1[0m" 500 -
2025-10-09 00:08:05,441 [INFO] src.api.app: üåê OPTIONS /api/generate/cover-letter/ - 127.0.0.1
2025-10-09 00:08:05,446 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/cover-letter/ - 200 - 0.005s
2025-10-09 00:08:05,447 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:08:05] "OPTIONS /api/generate/cover-letter/ HTTP/1.1" 200 -
2025-10-09 00:08:05,450 [INFO] src.api.app: üåê POST /api/generate/cover-letter/ - 127.0.0.1
2025-10-09 00:08:06,198 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:08:06,207 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:08:06,583 [INFO] src.api.app: ‚ùå POST /api/generate/cover-letter/ - 500 - 1.133s
2025-10-09 00:08:06,584 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:08:06] "[35m[1mPOST /api/generate/cover-letter/ HTTP/1.1[0m" 500 -
2025-10-09 00:08:09,442 [INFO] src.api.app: üåê POST /api/generate/cover-letter/ - 127.0.0.1
2025-10-09 00:08:09,886 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:08:09,893 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:08:10,879 [INFO] src.api.app: ‚ùå POST /api/generate/cover-letter/ - 500 - 1.438s
2025-10-09 00:08:10,880 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:08:10] "[35m[1mPOST /api/generate/cover-letter/ HTTP/1.1[0m" 500 -
2025-10-09 00:17:55,497 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-09 00:17:55,501 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.004s
2025-10-09 00:17:55,503 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:17:55] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:17:55,507 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-09 00:17:56,080 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.573s
2025-10-09 00:17:56,080 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:17:56] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:18:10,075 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-09 00:18:10,076 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-09 00:18:10,076 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:18:10] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:18:10,077 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-09 00:19:07,271 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:19:07,300 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:19:25,731 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:19:25,742 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:19:26,590 [INFO] src.api.app: ‚úÖ POST /api/generate/ - 200 - 76.512s
2025-10-09 00:19:26,590 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 76.512s
2025-10-09 00:19:26,591 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:19:26] "POST /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:19:31,643 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:19:31,643 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.001s
2025-10-09 00:19:31,644 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:19:31] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:19:35,744 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:19:35,755 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.011s
2025-10-09 00:19:35,757 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:19:35] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:23:07,328 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-09 00:23:07,328 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-09 00:30:56,491 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-09 00:30:56,493 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.001s
2025-10-09 00:30:56,495 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:30:56] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:30:56,500 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-09 00:30:57,376 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:30:57,400 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 0.901s
2025-10-09 00:30:57,401 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:30:57] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:30:59,963 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-09 00:30:59,964 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-09 00:30:59,964 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:30:59] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:30:59,969 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-09 00:31:00,848 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:31:15,705 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:19,548 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:20,633 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:24,265 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:27,672 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:31,009 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:34,734 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:45,391 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:31:46,901 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:31:46,952 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-09 00:31:46,953 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:31:46,969 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-09 00:31:46,971 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-09 00:31:46,983 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-09 00:31:47,509 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:31:47,561 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-09 00:32:00,285 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:32:02,874 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:32:02,891 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-09 00:32:02,892 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:32:02,902 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-09 00:32:02,904 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-09 00:32:02,907 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-09 00:32:03,606 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:32:03,608 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-09 00:32:03,631 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-09 00:32:04,341 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:32:04,359 [INFO] src.api.app: ‚úÖ POST /api/generate/ - 200 - 64.390s
2025-10-09 00:32:04,359 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 64.390s
2025-10-09 00:32:04,360 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:32:04] "POST /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:32:08,026 [INFO] src.api.app: üåê GET /api/resumes/generated/Google/Software Developer.pdf - 127.0.0.1
2025-10-09 00:32:08,030 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/Google/Software Developer.pdf - 200 - 0.004s
2025-10-09 00:32:08,032 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:32:08] "GET /api/resumes/generated/Google%2FSoftware%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:32:11,675 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:32:11,677 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.003s
2025-10-09 00:32:11,678 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:32:11] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:35:12,367 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:35:12,369 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.002s
2025-10-09 00:35:12,372 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:35:12] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:35:19,140 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:35:19,140 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.000s
2025-10-09 00:35:19,140 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:35:19] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:35:20,407 [INFO] src.api.app: üåê GET /api/resumes/generated/Software Developer.pdf - 127.0.0.1
2025-10-09 00:35:20,408 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/Software Developer.pdf - 404 - 0.001s
2025-10-09 00:35:20,413 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:35:20] "[33mGET /api/resumes/generated/Software%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:38:25,170 [INFO] src.api.app: üåê GET /api/resumes/generated/test.pdf - 127.0.0.1
2025-10-09 00:38:25,172 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/test.pdf - 404 - 0.003s
2025-10-09 00:38:25,173 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:38:25] "[33mGET /api/resumes/generated/test.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:39:18,463 [INFO] src.api.app: üåê GET /api/resumes/generated/CompanyName/Position.pdf - 127.0.0.1
2025-10-09 00:39:18,464 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/CompanyName/Position.pdf - 404 - 0.002s
2025-10-09 00:39:18,465 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:39:18] "[33mGET /api/resumes/generated/CompanyName/Position.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:40:39,560 [INFO] src.api.app: üåê GET /api/resumes/generated/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:40:39,561 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/Apple/Python Developer.pdf - 200 - 0.001s
2025-10-09 00:40:39,562 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:40:39] "GET /api/resumes/generated/Apple/Python%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:41:23,074 [INFO] src.api.app: üåê GET /api/resumes/generated/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:41:23,076 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/Apple/Python Developer.pdf - 200 - 0.002s
2025-10-09 00:41:23,076 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:41:23] "GET /api/resumes/generated/Apple/Python%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:41:47,418 [INFO] src.api.app: üåê GET /api/resumes/generated/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:41:47,419 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/Apple/Python Developer.pdf - 200 - 0.001s
2025-10-09 00:41:47,419 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:41:47] "GET /api/resumes/generated/Apple/Python%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:45:39,081 [INFO] src.api.app: üåê GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:45:39,082 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 404 - 0.002s
2025-10-09 00:45:39,083 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:45:39] "[33mGET /api/resumes/generated/cover_letters/Apple/Python%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:46:43,112 [INFO] src.api.app: üåê GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:46:43,113 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 404 - 0.001s
2025-10-09 00:46:43,114 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:46:43] "[33mGET /api/resumes/generated/cover_letters/Apple/Python%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:49:46,512 [INFO] src.api.app: üåê GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:49:46,513 [INFO] src.api.app: ‚ùå GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 404 - 0.001s
2025-10-09 00:49:46,514 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:49:46] "[33mGET /api/resumes/generated/cover_letters/Apple/Python%20Developer.pdf HTTP/1.1[0m" 404 -
2025-10-09 00:50:46,216 [INFO] src.api.app: üåê GET /api/resumes/preview/generated - 127.0.0.1
2025-10-09 00:50:46,217 [INFO] src.api.app: ‚ùå GET /api/resumes/preview/generated - 404 - 0.001s
2025-10-09 00:50:46,217 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:50:46] "[33mGET /api/resumes/preview/generated HTTP/1.1[0m" 404 -
2025-10-09 00:51:37,281 [INFO] werkzeug: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://localhost:8080
2025-10-09 00:51:37,281 [INFO] werkzeug: [33mPress CTRL+C to quit[0m
2025-10-09 00:52:02,962 [INFO] src.api.app: üåê GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:52:02,964 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/cover_letters/Apple/Python Developer.pdf - 200 - 0.002s
2025-10-09 00:52:02,966 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:52:02] "GET /api/resumes/generated/cover_letters/Apple/Python%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:52:32,932 [INFO] src.api.app: üåê GET /api/resumes/generated/resumes/Apple/Python Developer.pdf - 127.0.0.1
2025-10-09 00:52:32,934 [INFO] src.api.app: ‚úÖ GET /api/resumes/generated/resumes/Apple/Python Developer.pdf - 200 - 0.002s
2025-10-09 00:52:32,935 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:52:32] "GET /api/resumes/generated/resumes/Apple/Python%20Developer.pdf HTTP/1.1" 200 -
2025-10-09 00:55:56,324 [INFO] src.api.app: üåê OPTIONS /api/jobs/similar - 127.0.0.1
2025-10-09 00:55:56,326 [INFO] src.api.app: ‚úÖ OPTIONS /api/jobs/similar - 200 - 0.002s
2025-10-09 00:55:56,327 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:55:56] "OPTIONS /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:55:56,329 [INFO] src.api.app: üåê POST /api/jobs/similar - 127.0.0.1
2025-10-09 00:55:57,335 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:55:57,345 [INFO] src.api.app: ‚úÖ POST /api/jobs/similar - 200 - 1.016s
2025-10-09 00:55:57,345 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:55:57] "POST /api/jobs/similar HTTP/1.1" 200 -
2025-10-09 00:55:57,349 [INFO] src.api.app: üåê OPTIONS /api/generate/ - 127.0.0.1
2025-10-09 00:55:57,349 [INFO] src.api.app: ‚úÖ OPTIONS /api/generate/ - 200 - 0.000s
2025-10-09 00:55:57,350 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:55:57] "OPTIONS /api/generate/ HTTP/1.1" 200 -
2025-10-09 00:55:57,351 [INFO] src.api.app: üåê POST /api/generate/ - 127.0.0.1
2025-10-09 00:55:57,734 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:56:27,023 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:28,968 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:30,092 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:38,135 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:41,453 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:44,760 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:48,008 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:56:59,560 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:57:00,314 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:57:00,343 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-09 00:57:00,343 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:57:00,355 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-09 00:57:00,356 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-09 00:57:00,363 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-09 00:57:00,831 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:57:00,846 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/query "HTTP/1.1 200 OK"
2025-10-09 00:57:10,283 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-09 00:57:11,724 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:57:11,735 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/auth/identity "HTTP/1.1 200 OK"
2025-10-09 00:57:11,736 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-10-09 00:57:11,744 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant "HTTP/1.1 200 OK"
2025-10-09 00:57:11,745 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database "HTTP/1.1 200 OK"
2025-10-09 00:57:11,747 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections "HTTP/1.1 200 OK"
2025-10-09 00:57:12,752 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:57:12,757 [INFO] httpx: HTTP Request: GET http://localhost:8000/api/v2/pre-flight-checks "HTTP/1.1 200 OK"
2025-10-09 00:57:12,785 [INFO] httpx: HTTP Request: POST http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections/87760c6d-637c-45d3-bbc4-b592e9ab0264/upsert "HTTP/1.1 200 OK"
2025-10-09 00:57:13,305 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-10-09 00:57:13,322 [INFO] src.api.app: ‚ùå POST /api/generate/ - 500 - 75.970s
2025-10-09 00:57:13,322 [WARNING] src.api.app: üêå Slow request: POST /api/generate/ took 75.970s
2025-10-09 00:57:13,322 [INFO] werkzeug: 127.0.0.1 - - [09/Oct/2025 00:57:13] "[35m[1mPOST /api/generate/ HTTP/1.1[0m" 500 -
