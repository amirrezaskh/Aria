{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bd0db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "059900fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4ace62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    numDocs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00773342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aria:\n",
    "    def __init__(self):\n",
    "        self.llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        self.vector_store = InMemoryVectorStore(self.embeddings)\n",
    "\n",
    "        self.chunk_size = 1000\n",
    "        self.chunk_overlap = 200\n",
    "        self.add_start_index = True\n",
    "\n",
    "        self.prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        graph_builder = StateGraph(State).add_sequence(\n",
    "            [self.retrieve, self.generate])\n",
    "        graph_builder.add_edge(START, \"retrieve\")\n",
    "        self.graph = graph_builder.compile()\n",
    "\n",
    "    def embed_pdf(self, path):\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            add_start_index=self.add_start_index,\n",
    "        )\n",
    "\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        _ = self.vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "    def retrieve(self, state: State):\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            state[\"question\"], k=state[\"numDocs\"])\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "    def generate(self, state: State):\n",
    "        docs_content = \"\\n\\n\".join(\n",
    "            doc.page_content for doc in state[\"context\"])\n",
    "        messages = self.prompt.invoke(\n",
    "            {\"question\": state[\"question\"], \"context\": docs_content})\n",
    "        response = self.llm.invoke(messages)\n",
    "        return {\"answer\": response.content}\n",
    "\n",
    "    def invoke(self, question, numDocs):\n",
    "        result = self.graph.invoke({\"question\": question, \"numDocs\": numDocs})\n",
    "\n",
    "        print(f\"Context: {result['context']}\\n\\n\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "\n",
    "    def summarize_papers(self):\n",
    "        prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "        You are an expert resume writer tasked with summarizing academic papers in LaTeX resume format.\n",
    "        \n",
    "        Based on the provided papers, create a summary for each paper following this EXACT LaTeX format:\n",
    "\n",
    "        \\\\resumeSubheading\n",
    "            {{Paper Title}}{{\\\\emph{{Conference/Journal Year}} }}\n",
    "            {{Author Name(s)}}{{\\\\href{{GitHub Link}}{{\\\\underline{{Code}}}} $|$ \\\\href{{Paper Link}}{{\\\\underline{{Paper}}}}}}\n",
    "            \\\\resumeItemListStart\n",
    "                \\\\resumeItem{{First key contribution or achievement with \\\\textbf{{bold}} keywords}}\n",
    "                \\\\resumeItem{{Second key contribution or achievement with \\\\textbf{{bold}} keywords}}\n",
    "                \\\\resumeItem{{Third key contribution or achievement with \\\\textbf{{bold}} keywords}}\n",
    "                \\\\resumeItem{{Fourth key contribution or achievement with \\\\textbf{{bold}} keywords}}\n",
    "            \\\\resumeItemListEnd\n",
    "\n",
    "        IMPORTANT GUIDELINES:\n",
    "        1. Extract the paper title, conference/journal, and year from the documents\n",
    "        2. Include \"Amirreza Sokhankhosh\" as the primary author and list co-authors\n",
    "        3. For GitHub links, use placeholder \"https://github.com/amirrezaskh/[repo-name]\" if not found\n",
    "        4. For paper links, use placeholder \"https://paperlink.com\" if not found\n",
    "        5. Each resumeItem should highlight a specific contribution, methodology, or achievement\n",
    "        6. Use \\\\textbf{{}} to bold important technical terms, methodologies, or key achievements\n",
    "        7. Keep each resumeItem concise but informative (1-2 lines max)\n",
    "        8. Focus on technical contributions, novel approaches, and measurable results\n",
    "        9. Use action verbs like \"Proposed\", \"Developed\", \"Designed\", \"Demonstrated\", \"Achieved\"\n",
    "\n",
    "        Papers to summarize:\n",
    "        {context}\n",
    "\n",
    "        Generate the LaTeX resume format for each paper found in the context:\n",
    "        \"\"\")\n",
    "\n",
    "        # Get all documents from vector store for comprehensive analysis\n",
    "        all_docs_query = \"summarize all papers research contributions methodology results\"\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            all_docs_query, k=20)\n",
    "\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        messages = prompt_template.invoke({\"context\": docs_content})\n",
    "        response = self.llm.invoke(messages)\n",
    "\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d32dda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "aria = Aria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0362df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Below are the LaTeX formatted summaries for each specified paper based on the provided guidelines:\\n\\n```latex\\n\\\\resumeSubheading\\n    {Innovative Approaches to Algorithm Design}{\\\\emph{International Conference on Algorithms 2023} }\\n    {Amirreza Sokhankhosh, Jane Doe, John Smith}{\\\\href{https://github.com/amirrezaskh/algorithm-design}{\\\\underline{Code}} $|$ \\\\href{https://paperlink.com}{\\\\underline{Paper}}}\\n    \\\\resumeItemListStart\\n        \\\\resumeItem{Developed a novel \\\\textbf{algorithmic framework} enhancing computational efficiency by \\\\textbf{30\\\\%} over previous methods}\\n        \\\\resumeItem{Demonstrated the effectiveness of \\\\textbf{adaptive heuristics} in reducing time complexity in large-scale data processing}\\n        \\\\resumeItem{Introduced an innovative \\\\textbf{multi-threading approach} that significantly improved performance in parallel computing scenarios}\\n        \\\\resumeItem{Achieved \\\\textbf>state-of-the-art results} in multiple benchmark tests, validating the robustness of the proposed solutions}\\n    \\\\resumeItemListEnd\\n\\n\\\\resumeSubheading\\n    {Deep Learning for Image Processing: Challenges and Solutions}{\\\\emph{Journal of Machine Learning Applications 2023} }\\n    {Amirreza Sokhankhosh, Alice White, Michael Lee}{\\\\href{https://github.com/amirrezaskh/image-processing}{\\\\underline{Code}} $|$ \\\\href{https://paperlink.com}{\\\\underline{Paper}}}\\n    \\\\resumeItemListStart\\n        \\\\resumeItem{Proposed a \\\\textbf>novel convolutional neural network architecture} that improves image classification accuracy by \\\\textbf{15\\\\%} on standard datasets}\\n        \\\\resumeItem{Analyzed challenges in \\\\textbf{data augmentation techniques} for enhancing model robustness and generalization}\\n        \\\\resumeItem{Developed a \\\\textbf>dynamic learning rate schedule} that adaptively adjusts to improve convergence during training phase}\\n        \\\\resumeItem{Achieved significant improvements in \\\\textbf>image segmentation tasks}, setting a new benchmark for future studies}\\n    \\\\resumeItemListEnd\\n\\n\\\\resumeSubheading\\n    {Blockchain Technology in Supply Chain Management}{\\\\emph{Global Conference on Blockchain 2023} }\\n    {Amirreza Sokhankhosh, Sarah Johnson, Mark Brown}{\\\\href{https://github.com/amirrezaskh/blockchain-supplychain}{\\\\underline{Code}} $|$ \\\\href{https://paperlink.com}{\\\\underline{Paper}}}\\n    \\\\resumeItemListStart\\n        \\\\resumeItem{Developed a \\\\textbf>hybrid blockchain framework} integrating private and public chains to enhance supply chain transparency}\\n        \\\\resumeItem{Demonstrated a \\\\textbf>real-time tracking system} that utilizes blockchain for secure and immutable tracking of goods}\\n        \\\\resumeItem{Quantified the impact of \\\\textbf>decreased fraud incidents} leading to a \\\\textbf{25\\\\%} reduction in supply chain costs}\\n        \\\\resumeItem{Achieved scalability in blockchain applications by \\\\textbf>optimizing data handling mechanisms} within the proposed framework}\\n    \\\\resumeItemListEnd\\n```\\n\\nIn this format, each paper's contributions are clearly stated with key technical terms highlighted, in line with your requirements. Let me know if you need any further assistance!\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aria.summarize_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the summarize_papers method\n",
    "paper_summaries = aria.summarize_papers()\n",
    "print(paper_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca000720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47af6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4366f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1728ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./papers/PoCL.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc2d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split my paper post into 52 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split my paper post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed47f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379cf00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "076f21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0caa5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d56ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3befac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='b9122ada-e399-43af-ae9a-e450b34c9d5c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-06-21T00:32:10+00:00', 'moddate': '2024-06-21T00:32:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'source': './papers/PoCL.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='Proof-of-Collaborative-Learning: A Multi-winner\\nFederated Learning Consensus Algorithm\\nAmirreza Sokhankhosh\\nUniversity of Manitoba, Winnipeg, Canada\\nsokhanka@myumanitoba.ca\\nSara Rouhani\\nUniversity of Manitoba, Winnipeg, Canada\\nsara.rouhani@umanitoba.ca\\nAbstract—Regardless of their variations, blockchains require\\na consensus mechanism to validate transactions, supervise added\\nblocks, maintain network security, synchronize the network\\nstate, and distribute incentives. Proof-of-Work (PoW), one of\\nthe most influential implementations of consensus mechanisms,\\nconsumes an extraordinary amount of energy for a task that\\nlacks direct productive output. In this paper, we propose Proof-of-\\nCollaborative-Learning (PoCL), a multi-winner federated learn-\\ning validated consensus mechanism that redirects the computa-\\ntion power of blockchains to train federated learning models. In\\naddition, we present a novel evaluation mechanism to ensure the'), Document(id='2fe7d5dd-f31f-461a-a86d-26c7d8bba021', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-06-21T00:32:10+00:00', 'moddate': '2024-06-21T00:32:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'source': './papers/PoCL.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'start_index': 3929}, page_content='to this approach: i) FL is only achieved in pools with a\\nlimited number of miners, thereby undermining the efficiency\\nof models trained, and ii) the framework lacks fairness because\\nthe winning global model is not shared with other pools. This\\nleads to an unfair advantage for pools that win in the initial\\nrounds. This disparity significantly affects miners who join the\\ncompetition in the middle or final rounds.\\nAccordingly, this paper presents Proof-of-Collaborative-\\nLearning (PoCL), a novel decentralized multi-winner FL con-\\nsensus mechanism that improves model evaluation by using\\na distributed network of miners. In our framework, miners\\ndistribute unlabeled test records to evaluate the trained lo-\\ncal models of other miners, who predict these records and\\nreport their results. These predictions are evaluated based\\non accuracy (loss value) and timeliness (prediction time)\\nparameters. Through the implemented smart contacts, top K'), Document(id='53fd727a-08ad-4645-81f2-a505a08d0610', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-06-21T00:32:10+00:00', 'moddate': '2024-06-21T00:32:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'source': './papers/PoCL.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'start_index': 2396}, page_content='a short time. Winner miners are rewarded according to the\\nsignificance of their contribution to the global models.\\n(iv) Aggregator: An off-chain program that supervises the\\naggregation of the winner models. The aggregator can shift\\nbetween different variations of FL according to the requester’s\\npreference. Furthermore, the program computes the contribu-\\ntion of each winning miner and reports it to the blockchain to\\nreward them accordingly.\\n(v) Users: Similar to any user in a blockchain network, they\\nsubmit transactions to be mined and added to the blockchain.\\nAll submitted transactions are stored in a transaction pool,\\nfrom which miners select transactions to mine.\\nFig. 1. Proof-of-Collaborative-Learning (PoCL) Design.\\nFurthermore, we assume that the peers within the blockchain\\nnetwork possess the capability to execute and store smart\\ncontracts. Given these assumptions, we propose a series of\\nactions to be undertaken in each round to achieve a consensus'), Document(id='96fa41ea-6f3d-4c59-b084-e0358c2305cd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-06-21T00:32:10+00:00', 'moddate': '2024-06-21T00:32:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'source': './papers/PoCL.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'start_index': 1644}, page_content='that our framework promotes fair competition in each round\\nand ensures an equitable reward distribution among winning\\nminers across all rounds.\\nMoreover, we conducted a similar experiment using the\\nsame hyperparameters to assess the impact of the KNN\\nattacks on the system. In this experiment, we introduced two\\nadversaries, miners one and six, with the latter possessing\\nfour times more data records than the former. The results of\\nthis setting are demonstrated in Figure 6, showing that neither\\nadversary won any competition round, regardless of data size.\\nVII. C ONCLUSION\\nIn this paper, we proposed Proof-of-Collaborative-Learning\\n(PoCL), a multi-winner FL validated consensus mechanism\\nthat recycles the energy of the original proof of work. This\\nframework trains the requested models globally using the\\ncomputation power of the contributing miners. To evaluate\\nminers’ performance and select winners, we proposed a novel\\nevaluation step that relies on the predictions miners make')]\n",
      "\n",
      "\n",
      "Answer: Proof of Collaborative Learning (PoCL) is a multi-winner federated learning consensus mechanism that utilizes the computational power of blockchain miners to train federated learning models. It aims to enhance model evaluation by allowing miners to evaluate each other's predictions and receive rewards based on their contributions. PoCL promotes fair competition and equitable reward distribution among miners throughout the consensus process.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Proof of Collaborative Learning?\"})\n",
    "\n",
    "print(f\"Context: {result['context']}\\n\\n\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57bea080",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    111\u001b[39m load_dotenv()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m aria = \u001b[43mAria\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mAria.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.add_start_index = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.prompt = hub.pull(\u001b[33m\"\u001b[39m\u001b[33mrlm/rag-prompt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mAria.build_graph\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# graph_builder.set_finish(\"generate_cover_letter\")\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.graph = graph_builder.compile()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m display(Image(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/data-science/lib/python3.11/site-packages/langchain_core/runnables/graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/data-science/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:310\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    304\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    305\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    306\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    307\u001b[39m         )\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/data-science/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    459\u001b[39m     msg = (\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from glob import glob\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    job: str\n",
    "    resume: str\n",
    "    prompt: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class Aria:\n",
    "    def __init__(self):\n",
    "        self.llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        self.vector_store = InMemoryVectorStore(self.embeddings)\n",
    "\n",
    "        self.chunk_size = 1000\n",
    "        self.chunk_overlap = 200\n",
    "        self.add_start_index = True\n",
    "\n",
    "        self.prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "    def embed_pdf(self, path):\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            add_start_index=self.add_start_index,\n",
    "        )\n",
    "\n",
    "        all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        _ = self.vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "\n",
    "    def retrieve(self, state: State):\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            state[\"job\"])\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "    def generate(self, state: State):\n",
    "        docs_content = \"\\n\\n\".join(\n",
    "            doc.page_content for doc in state[\"context\"])\n",
    "        messages = self.prompt.invoke(\n",
    "            {\"question\": state[\"question\"], \"context\": docs_content})\n",
    "        response = self.llm.invoke(messages)\n",
    "        return {\"answer\": response.content}\n",
    "    \n",
    "\n",
    "    def build_graph(self):\n",
    "        graph_builder = StateGraph(State)\n",
    "\n",
    "        graph_builder.add_node(\"retrieve\", self.retrieve)\n",
    "        graph_builder.add_node(\"summarize_experiences\", self.summarize_experiences)\n",
    "        graph_builder.add_node(\"summarize_technical_skills\", self.summarize_technical_skills)\n",
    "        graph_builder.add_node(\"select_projects\", self.select_projects)\n",
    "        graph_builder.add_node(\"summarize_projects\", self.summarize_projects)\n",
    "        graph_builder.add_node(\"generate_highlights\", self.generate_highlights)\n",
    "        graph_builder.add_node(\"generate_cover_letter\", self.generate_cover_letter)\n",
    "\n",
    "        graph_builder.add_edge(START, \"summarize_experiences\")\n",
    "        graph_builder.add_edge(\"summarize_experiences\", \"summarize_technical_skills\")\n",
    "        graph_builder.add_edge(\"summarize_technical_skills\", \"select_projects\")\n",
    "        graph_builder.add_edge(\"select_projects\", \"summarize_projects\")\n",
    "        graph_builder.add_edge(\"summarize_projects\", \"generate_highlights\")\n",
    "        graph_builder.add_edge(\"retrieve\", \"generate_cover_letter\")\n",
    "        graph_builder.add_edge(\"generate_highlights\", \"generate_cover_letter\")\n",
    "\n",
    "        # graph_builder.set_finish(\"generate_cover_letter\")\n",
    "\n",
    "        self.graph = graph_builder.compile()\n",
    "\n",
    "        display(Image(self.graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n",
    "    def summarize_experiences(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def summarize_technical_skills(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def select_projects(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def summarize_projects(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def generate_highlights(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def generate_cover_letter(self, state: State):\n",
    "        pass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aria = Aria()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
